{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f53520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_6852\\451422013.py:14: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\anaconda\\envs\\dl-new\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from util import *\n",
    "import cifar10\n",
    "\n",
    "\n",
    "\n",
    "LR_DECAY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c81b2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "\n",
    "    # read config json and update the sysarg\n",
    "    with open(\"config.json\", \"r\") as read_file:\n",
    "        config = json.load(read_file)\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b48588e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 24\n",
    "\n",
    "def train_transform(reshaped_image):\n",
    "    # copied from cifar10_input.py / distorted_input()\n",
    "\n",
    "    height = IMAGE_SIZE\n",
    "    width = IMAGE_SIZE\n",
    "\n",
    "    # Image processing for training the network. Note the many random\n",
    "    # distortions applied to the image.\n",
    "\n",
    "    # Randomly crop a [height, width] section of the image.\n",
    "    distorted_image = tf.random_crop(reshaped_image, [tf.shape(reshaped_image)[0], height, width, 3])\n",
    "    # tf shape gives dynamic shape\n",
    "\n",
    "    # Randomly flip the image horizontally.\n",
    "    distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "\n",
    "    # Because these operations are not commutative, consider randomizing\n",
    "    # the order their operation.\n",
    "    distorted_image = tf.image.random_brightness(distorted_image,\n",
    "                                               max_delta=63)\n",
    "    distorted_image = tf.image.random_contrast(distorted_image,\n",
    "                                             lower=0.2, upper=1.8)\n",
    "\n",
    "    # Subtract off the mean and divide by the variance of the pixels.\n",
    "    float_image = tf.image.per_image_standardization(distorted_image)\n",
    "\n",
    "    return float_image\n",
    "\n",
    "def test_transform(reshaped_image):\n",
    "    # copied from cifar10_input.py / input()\n",
    "\n",
    "    height = IMAGE_SIZE\n",
    "    width = IMAGE_SIZE\n",
    "\n",
    "    # Image processing for evaluation.\n",
    "    # Crop the central [height, width] of the image.\n",
    "    resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image,\n",
    "                                                         width, height)\n",
    "\n",
    "    # Subtract off the mean and divide by the variance of the pixels.\n",
    "    float_image = tf.image.per_image_standardization(resized_image)\n",
    "\n",
    "    return float_image\n",
    "\n",
    "\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "\n",
    "def create_batches(pmt, batch_size):\n",
    "    batch_indices = []\n",
    "    ct = 0\n",
    "    for b_i in range(int(np.ceil( len(pmt) / batch_size))):\n",
    "        if ct + batch_size > len(pmt):\n",
    "            batch = pmt[ct : len(pmt)]\n",
    "            ct = len(pmt)\n",
    "        else:\n",
    "            batch = pmt[ct : ct + batch_size]\n",
    "            ct += batch_size\n",
    "        batch_indices.append(batch)\n",
    "\n",
    "    return batch_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "490bb8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainCIFARCluster(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        assert self.config['m'] % self.config['p'] == 0\n",
    "\n",
    "    def setup(self):\n",
    "\n",
    "        os.makedirs(self.config['project_dir'], exist_ok = True)\n",
    "\n",
    "        self.result_fname = os.path.join(self.config['project_dir'], 'results')\n",
    "        self.checkpoint_fname = os.path.join(self.config['project_dir'], 'checkpoint')\n",
    "\n",
    "        set_random_seed(self.config['data_seed'])\n",
    "        self.setup_datasets()\n",
    "        self.setup_model()\n",
    "\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        with tf.device('/gpu:0'):\n",
    "            self.sess = tf.Session(config=config)\n",
    "\n",
    "\n",
    "        set_random_seed(self.config['data_seed']+self.config['train_seed'])\n",
    "        self.initialize_models()\n",
    "        self.initialize_assign_ops()\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        self.epoch = None\n",
    "        self.lr = None\n",
    "\n",
    "\n",
    "    def setup_datasets(self):\n",
    "        # tf.enable_eager_execution()\n",
    "\n",
    "        # generate indices for each dataset\n",
    "        # also write cluster info\n",
    "\n",
    "        CIFAR10_TRAINSET_DATA_SIZE = 50000\n",
    "        CIFAR10_TESTSET_DATA_SIZE = 10000\n",
    "\n",
    "        cfg = self.config\n",
    "\n",
    "        self.dataset = {}\n",
    "\n",
    "        dataset = {}\n",
    "        dataset['data_indices'], dataset['cluster_assign'] = \\\n",
    "            self._setup_dataset(CIFAR10_TRAINSET_DATA_SIZE, cfg['p'], cfg['m'], cfg['n'])\n",
    "        dl = self._load_CIFAR(train=True)\n",
    "        dataset['data_loader'] = dl\n",
    "        self.dataset['train'] = dataset\n",
    "\n",
    "        dataset = {}\n",
    "        dataset['data_indices'], dataset['cluster_assign'] = \\\n",
    "            self._setup_dataset(CIFAR10_TESTSET_DATA_SIZE, cfg['p'], cfg['m_test'], cfg['n'], random=False)\n",
    "        dl = self._load_CIFAR(train=False)\n",
    "        dataset['data_loader'] = dl\n",
    "        self.dataset['test'] = dataset\n",
    "\n",
    "        # tf.disable_eager_execution()\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "\n",
    "    def _setup_dataset(self, num_data, p, m, n, random = True):\n",
    "\n",
    "        assert (m // p) * n == num_data\n",
    "\n",
    "        dataset = {}\n",
    "\n",
    "        cfg = self.config\n",
    "\n",
    "        data_indices = []\n",
    "        cluster_assign = []\n",
    "\n",
    "        m_per_cluster = m // p\n",
    "\n",
    "        for p_i in range(p):\n",
    "\n",
    "            if random:\n",
    "                ll = list(np.random.permutation(num_data))\n",
    "            else:\n",
    "                ll = list(range(num_data))\n",
    "\n",
    "            ll2 = chunkify(ll, m_per_cluster) # splits ll into m lists with size n\n",
    "            data_indices += ll2\n",
    "\n",
    "            cluster_assign += [p_i for _ in range(m_per_cluster)]\n",
    "\n",
    "        data_indices = np.array(data_indices)\n",
    "        cluster_assign = np.array(cluster_assign)\n",
    "        assert data_indices.shape[0] == cluster_assign.shape[0]\n",
    "        assert data_indices.shape[0] == m\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        return data_indices, cluster_assign\n",
    "\n",
    "\n",
    "    def _load_CIFAR(self, train=True):\n",
    "        # gives dataloader that gives (X,y) based on asked index\n",
    "\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "        # (50000, 32,32, 3) [0~1] , (50000, 1)\n",
    "\n",
    "        if train:\n",
    "            X = x_train / 255.0\n",
    "            y = y_train.reshape(-1)\n",
    "        else:\n",
    "            X = x_test / 255.0\n",
    "            y = y_test.reshape(-1)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    def setup_model(self):\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "\n",
    "        # setup tensorflow model structure\n",
    "\n",
    "        self.x_pl = tf.placeholder(tf.float32, shape=(None, 24, 24, 3), name='input_x')\n",
    "        self.y_pl = tf.placeholder(tf.int32, shape=(None, ), name='output_y')\n",
    "        self.lr_pl = tf.placeholder(tf.float32, shape=(), name='learning_rate')\n",
    "\n",
    "        self.y_logits = cifar10.inference(self.x_pl) # construct model\n",
    "        self.loss = cifar10.loss(self.y_logits, self.y_pl)\n",
    "\n",
    "        self.y_pred = tf.cast(tf.argmax(self.y_logits, 1), tf.int32)\n",
    "        self.correct_prediction = tf.equal(self.y_pred, self.y_pl) # used for accuracy\n",
    "        self.num_correct = tf.reduce_sum(tf.cast(self.correct_prediction, tf.int64))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.lr_pl)\n",
    "        self.train_op = self.optimizer.minimize(self.loss)\n",
    "        self.opt_reset_op = tf.variables_initializer(self.optimizer.variables())\n",
    "\n",
    "        # import ipdb; ipdb.set_trace() # check self.optimizer.variables()\n",
    "\n",
    "        self.metrics = { # used by self.eval()\n",
    "            'loss':self.loss,\n",
    "            'correct': self.num_correct,\n",
    "            # and add more...\n",
    "        }\n",
    "\n",
    "\n",
    "        # transform ops\n",
    "        self.x_tr_pl = tf.placeholder(tf.float32, shape=(None, 32, 32, 3))\n",
    "        # with tf.device('/cpu:0'):\n",
    "        self.train_transform_op = train_transform(self.x_tr_pl)\n",
    "        self.test_transform_op = test_transform(self.x_tr_pl)\n",
    "\n",
    "\n",
    "    def initialize_models(self):\n",
    "\n",
    "        p = self.config['p']\n",
    "        m = self.config['m']\n",
    "\n",
    "        # initialize p times, to get p different sets of weights.\n",
    "\n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "        self.model_weights = []\n",
    "        models = []\n",
    "        for p_i in range(p):\n",
    "            self.sess.run(self.init_op)\n",
    "            weights = self.get_model_weights()\n",
    "            models.append(weights)\n",
    "\n",
    "        for m_i in range(m):\n",
    "            self.model_weights.append(models)\n",
    "\n",
    "    def get_model_weights(self):\n",
    "        self.collection = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "        names = [var.name for var in self.collection]\n",
    "        weights_arrays = self.sess.run(self.collection)\n",
    "\n",
    "        weights = dict(zip(names, weights_arrays))\n",
    "        # {'conv1/weights:0': np.array, ...}\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def initialize_assign_ops(self):\n",
    "        self.collection = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "        assign_ops = {}\n",
    "        assign_pls = {}\n",
    "        for var in self.collection:\n",
    "            # import ipdb; ipdb.set_trace()\n",
    "            pl = tf.placeholder(tf.float32, shape=var.shape)\n",
    "            assign_pls[var.name] = pl\n",
    "\n",
    "            op = tf.compat.v1.assign(var, pl)\n",
    "            assign_ops[var.name] = op\n",
    "\n",
    "\n",
    "        self.assign_ops = assign_ops\n",
    "        self.assign_pls = assign_pls\n",
    "\n",
    "    def put_model_weights(self, weights):\n",
    "\n",
    "        assign_ops = []\n",
    "\n",
    "        fd = {}\n",
    "        for var_name in self.assign_pls:\n",
    "            # assign_op = tf.assign(var, weights[var.name])\n",
    "            pl = self.assign_pls[var_name]\n",
    "            fd[pl] = weights[var_name]\n",
    "\n",
    "        self.sess.run(self.opt_reset_op) # reset the optimizer state ?\n",
    "        self.sess.run(list(self.assign_ops.values()), feed_dict = fd)\n",
    "\n",
    "    def average_model_weights(self, weights_list):\n",
    "\n",
    "        w2 = {}\n",
    "\n",
    "        for key in weights_list[0].keys():\n",
    "\n",
    "            w2[key] = np.mean([w[key] for w in weights_list], axis=0)\n",
    "\n",
    "        return w2\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        TRAIN_INFER_FULL_NODES = 0\n",
    "\n",
    "        num_epochs = self.config['num_epochs']\n",
    "        lr = self.config['lr']\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # epoch -1\n",
    "        self.epoch = -1\n",
    "\n",
    "        self.find_good_initializer()\n",
    "\n",
    "\n",
    "        result = {}\n",
    "        result['epoch'] = -1\n",
    "\n",
    "        t0 = time.time()\n",
    "        self.set_participating_nodes()\n",
    "        res = self.test(train=True, force_full_nodes =TRAIN_INFER_FULL_NODES)\n",
    "        # res = self.test(train=True)\n",
    "        t1 = time.time()\n",
    "        res['infer_time'] = t1-t0\n",
    "        result['train'] = res\n",
    "\n",
    "        self.print_epoch_stats(res)\n",
    "\n",
    "        t0 = time.time()\n",
    "        res = self.test(train=False)\n",
    "        t1 = time.time()\n",
    "        res['infer_time'] = t1-t0\n",
    "        result['test'] = res\n",
    "        self.print_epoch_stats(res)\n",
    "        results.append(result)\n",
    "\n",
    "        # this will be used in next epoch\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            result = {}\n",
    "            result['epoch'] = epoch\n",
    "\n",
    "            lr = self.lr_schedule(epoch)\n",
    "            result['lr'] = lr\n",
    "\n",
    "            t0 = time.time()\n",
    "            result['train'] = self.train(lr = lr)\n",
    "            t1 = time.time()\n",
    "            train_time = t1-t0\n",
    "\n",
    "            t0 = time.time()\n",
    "            self.set_participating_nodes()\n",
    "            # res = self.test(train=True)\n",
    "            res = self.test(train=True, force_full_nodes =TRAIN_INFER_FULL_NODES)\n",
    "            t1 = time.time()\n",
    "            res['infer_time'] = t1-t0\n",
    "            res['train_time'] = train_time\n",
    "            res['lr'] = lr\n",
    "            result['train'] = res\n",
    "\n",
    "            self.print_epoch_stats(res)\n",
    "\n",
    "            t0 = time.time()\n",
    "            res = self.test(train=False)\n",
    "            t1 = time.time()\n",
    "            res['infer_time'] = t1-t0\n",
    "            result['test'] = res\n",
    "            self.print_epoch_stats(res)\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "            if epoch % 10 == 0 or epoch == num_epochs - 1 :\n",
    "                with open(self.result_fname+\".pickle\", 'wb') as outfile:\n",
    "                    pickle.dump(results, outfile)\n",
    "                    print(f'result written at {self.result_fname+\".pickle\"}')\n",
    "                # self.save_checkpoint()\n",
    "                # print(f'checkpoint written at {self.checkpoint_fname}')\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "\n",
    "    def find_good_initializer(self):\n",
    "        print(\"finding good initializer from train data\")\n",
    "\n",
    "        cfg = self.config\n",
    "\n",
    "        if cfg['p'] == 4:\n",
    "            th = 0.1\n",
    "        elif cfg['p'] == 2:\n",
    "            th = 0.35\n",
    "        elif cfg['p'] == 1:\n",
    "            th = 0.0\n",
    "        else:\n",
    "            raise NotImplementedError(\"only p=1,2,4 supported\")\n",
    "\n",
    "        is_not_good = True\n",
    "        while is_not_good:\n",
    "            self.initialize_models()\n",
    "            t0 = time.time()\n",
    "            self.set_participating_nodes()\n",
    "            # res = self.test(train=True, force_full_nodes = True)\n",
    "            res = self.test(train=True)\n",
    "            t1 = time.time()\n",
    "            res['infer_time'] = t1-t0\n",
    "            self.print_epoch_stats(res)\n",
    "\n",
    "            cl_ct = res['cl_ct']\n",
    "\n",
    "            num_nodes = np.sum(cl_ct)\n",
    "            is_not_good = False\n",
    "            for ct in cl_ct:\n",
    "                if ct / num_nodes < th:\n",
    "                    is_not_good = True\n",
    "\n",
    "        print(\"found good initializer\")\n",
    "\n",
    "\n",
    "\n",
    "    def set_participating_nodes(self):\n",
    "        cfg = self.config\n",
    "        m = cfg['m']\n",
    "        p = cfg['p']\n",
    "        p_rate = cfg['participation_rate']\n",
    "\n",
    "        self.participating_nodes = np.random.choice(m, int(m * p_rate), replace = False)\n",
    "\n",
    "        return self.participating_nodes\n",
    "\n",
    "    def lr_schedule(self, epoch):\n",
    "        if self.lr is None:\n",
    "            self.lr = self.config['lr']\n",
    "\n",
    "        if epoch != 0 and LR_DECAY:\n",
    "            self.lr = self.lr * 0.99\n",
    "\n",
    "        return self.lr\n",
    "\n",
    "\n",
    "    def print_epoch_stats(self, res):\n",
    "        if res['is_train']:\n",
    "            data_str = 'tr'\n",
    "        else:\n",
    "            data_str = 'tst'\n",
    "\n",
    "        if 'train_time' in res:\n",
    "            time_str = f\"{res['train_time']:.3f}sec(train) {res['infer_time']:.3f}sec(infer)\"\n",
    "        else:\n",
    "            time_str = f\"{res['infer_time']:.3f}sec\"\n",
    "\n",
    "        if 'lr' in res:\n",
    "            lr_str = f\" lr {res['lr']:4f}\"\n",
    "        else:\n",
    "            lr_str = \"\"\n",
    "\n",
    "        if 'cl_ct' in res:\n",
    "            cl_str = f\" clct{res['cl_ct']} ans{res['cl_ct_ans']}\"\n",
    "        else:\n",
    "            cl_str = \"\"\n",
    "\n",
    "        str0 = f\"Epoch {self.epoch} {data_str}: l {res['loss']:.3f} a {res['acc']:.3f} {cl_str}{lr_str} {time_str}\"\n",
    "\n",
    "        print(str0)\n",
    "\n",
    "    def train(self, lr):\n",
    "\n",
    "        VERBOSE = 0\n",
    "\n",
    "        cfg = self.config\n",
    "        m = cfg['m']\n",
    "        p = cfg['p']\n",
    "        tau = cfg['tau']\n",
    "        n = cfg['n']\n",
    "        batch_size = cfg['batch_size']\n",
    "\n",
    "        participating_nodes = self.participating_nodes\n",
    "        cluster_assign = self.cluster_assign\n",
    "\n",
    "        t_put_weight = 0\n",
    "        t_get_weight = 0\n",
    "        time_load_data = 0\n",
    "        time_train = 0\n",
    "\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        updated_local_weights = []\n",
    "        for m_i2, m_i in enumerate(participating_nodes):\n",
    "            p_i = cluster_assign[m_i]\n",
    "\n",
    "            # if VERBOSE and m_i2 % 1 == 0: print(f'Local update m_i2 {m_i2}/{len(participating_nodes)} processing \\r', end ='')\n",
    "            if VERBOSE and m_i2 % 1 == 0: print(f'Local update m_i2 {m_i2}/{len(participating_nodes)} processing')\n",
    "\n",
    "            # Local Update process\n",
    "\n",
    "            t_p = time.time()\n",
    "            self.put_model_weights(self.model_weights[m_i][p_i])\n",
    "            t_p1 = time.time()\n",
    "            t_put_weight += t_p1-t_p\n",
    "\n",
    "            for l_epoch in range(tau): # local epochs\n",
    "\n",
    "                pmt = np.random.permutation(n)\n",
    "                local_indices_list = create_batches(pmt, batch_size = batch_size)\n",
    "                node_data_indices = self.dataset['train']['data_indices'][m_i]\n",
    "\n",
    "                for b_i, local_indices in enumerate(local_indices_list):\n",
    "                    t00 = time.time()\n",
    "\n",
    "                    current_batch_indices = node_data_indices[local_indices]\n",
    "\n",
    "                    (X_b, y_b) = self.load_data_by_index(current_batch_indices, m_i)\n",
    "\n",
    "                    t01 = time.time()\n",
    "\n",
    "                    fd0 = {\n",
    "                        self.x_pl:X_b,\n",
    "                        self.y_pl:y_b,\n",
    "                        self.lr_pl:self.lr\n",
    "                    }\n",
    "                    self.sess.run([self.train_op], feed_dict= fd0)\n",
    "\n",
    "                    t02 = time.time()\n",
    "\n",
    "                    time_load_data += t01 - t00\n",
    "                    time_train += t02 - t01\n",
    "\n",
    "\n",
    "            t_p = time.time()\n",
    "            updated_local_weight = self.get_model_weights()\n",
    "            t_p1 = time.time()\n",
    "            t_get_weight += t_p1-t_p\n",
    "\n",
    "            self.model_weights[m_i][p_i] = copy.deepcopy(updated_local_weight)\n",
    "            # updated_local_weights.append(updated_local_weight)\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        # averaging\n",
    "\n",
    "        counts = {i: 0 for i in range(p)}\n",
    "        for m_i2, m_i in enumerate(participating_nodes):\n",
    "            counts[cluster_assign[m_i]] += 1\n",
    "\n",
    "        for m_i2, m_i in enumerate(participating_nodes):\n",
    "            p_i = cluster_assign[m_i]\n",
    "            num_clients = len(participating_nodes)\n",
    "            \n",
    "            num_cluster_i = counts[cluster_assign[m_i]]\n",
    "            num_cluster_rest = num_clients - num_cluster_i\n",
    "\n",
    "            th_j = min(num_cluster_rest, 100)\n",
    "            th_i = min(num_cluster_i, 100)\n",
    "            th = min(th_i, th_j)\n",
    "\n",
    "            # threshold_j = min(num_cluster_rest, int(np.floor(e/2)))\n",
    "            # threshold_i = min(num_cluster_i, int(np.floor(e/2))) - 1\n",
    "\n",
    "            if th <= 1:\n",
    "                continue\n",
    "\n",
    "            selected_clients = random.sample([i for _, i in enumerate(participating_nodes) if i != m_i], int(np.random.randint(min(5, th-1), th, (1,)).item()))\n",
    "            # selected_clients =  random.sample([i for _, i in enumerate(participating_nodes) if i != m_i], min(threshold_i,threshold_j))\n",
    "            m_i_cluster = cluster_assign[m_i]\n",
    "\n",
    "            for m_j in selected_clients:\n",
    "                m_j_cluster = cluster_assign[m_j]\n",
    "                \n",
    "                self.model_weights[m_i][m_j_cluster] = self.average_model_weights([self.model_weights[m_i][m_j_cluster], self.model_weights[m_j][m_j_cluster]])\n",
    "\n",
    "\n",
    "        # for p_i in range(p):\n",
    "        #     if len(local_weights_cluster[p_i]) > 0:\n",
    "        #         self.model_weights[p_i] = self.average_model_weights(local_weights_cluster[p_i])\n",
    "\n",
    "        t2 = time.time()\n",
    "\n",
    "        if VERBOSE: print(f\"train_whole {t1-t0:.3f} t_gd {time_train:.3f} t load data {time_load_data:.3f} t put model {t_put_weight:.3f} t get mdoel {t_get_weight:.3f}  averaging {t2-t1:.3f}\")\n",
    "\n",
    "    def get_cluster_accuracy(self, actual, pred):\n",
    "        \n",
    "        cm = confusion_matrix(actual, pred)\n",
    "\n",
    "        row_ind, col_ind = linear_sum_assignment(-cm)\n",
    "        matching = dict(zip(col_ind, row_ind))\n",
    "\n",
    "        remapped_preds = [matching[p] for p in pred]\n",
    "\n",
    "        cl_acc = np.mean(np.array(remapped_preds) == np.array(actual))\n",
    "\n",
    "        return cl_acc\n",
    "\n",
    "\n",
    "    def test(self, train = True, force_full_nodes = False):\n",
    "\n",
    "        VERBOSE = 0\n",
    "\n",
    "        cfg = self.config\n",
    "        p = cfg['p']\n",
    "        p_rate = cfg['participation_rate']\n",
    "\n",
    "\n",
    "        if train:\n",
    "            m = cfg['m']\n",
    "            dataset = self.dataset['train']\n",
    "            if force_full_nodes:\n",
    "                participating_nodes = list(range(m))\n",
    "            else:\n",
    "                participating_nodes = self.participating_nodes\n",
    "        else:\n",
    "            m = cfg['m_test']\n",
    "            dataset = self.dataset['test']\n",
    "            participating_nodes = list(range(m))\n",
    "\n",
    "            # DEBUGGING\n",
    "            # print(\"DEBUGGING MODEe\")\n",
    "            # participating_nodes = np.random.choice(m, int(m * p_rate), replace = False)\n",
    "\n",
    "\n",
    "        # get loss and correct from all data\n",
    "\n",
    "\n",
    "        t_load_model = 0\n",
    "        t_load_data = 0\n",
    "        t_infer = 0\n",
    "\n",
    "        losses = {}\n",
    "        corrects = {}\n",
    "        num_data = 0\n",
    "        for m_i in participating_nodes:\n",
    "            for p_i in range(p):\n",
    "\n",
    "                tp0= time.time()\n",
    "                self.put_model_weights(self.model_weights[m_i][p_i])\n",
    "                tp1= time.time()\n",
    "                t_load_model += tp1-tp0\n",
    "\n",
    "                t00= time.time()\n",
    "                (X, y) = self.load_node_data(m_i, train=train) # load batch data rotated\n",
    "                t01= time.time()\n",
    "                t_load_data += t01-t00\n",
    "\n",
    "                ti0= time.time()\n",
    "                (loss, correct) = self.sess.run([self.loss, self.num_correct], feed_dict = {self.x_pl:X, self.y_pl:y})\n",
    "                ti1= time.time()\n",
    "                t_infer += ti1-ti0\n",
    "\n",
    "\n",
    "                losses[(m_i,p_i)] = loss\n",
    "                corrects[(m_i,p_i)] = correct\n",
    "\n",
    "            num_data += X.shape[0]\n",
    "\n",
    "\n",
    "        if VERBOSE: print(f\"loadmodel {t_load_model:.3f}, load data {t_load_data:.3f}, infer {t_infer:.3f}\")\n",
    "\n",
    "\n",
    "        # calculate loss and cluster the machines\n",
    "        cluster_assign = [-1 for _ in range(m)]\n",
    "        for m_i in participating_nodes:\n",
    "            machine_losses = [ losses[(m_i,p_i)] for p_i in range(p) ]\n",
    "            min_p_i = np.argmin(machine_losses)\n",
    "            cluster_assign[m_i] = min_p_i\n",
    "\n",
    "        # calculate optimal model's loss, acc over all models\n",
    "\n",
    "        min_corrects = []\n",
    "        min_losses = []\n",
    "        for m_i in participating_nodes:\n",
    "            p_i = cluster_assign[m_i]\n",
    "\n",
    "            min_loss = losses[(m_i,p_i)]\n",
    "            min_losses.append(min_loss)\n",
    "\n",
    "            min_correct = corrects[(m_i,p_i)]\n",
    "            min_corrects.append(min_correct)\n",
    "\n",
    "        # if train:\n",
    "        #     loss = np.mean(min_losses)\n",
    "        #     acc = np.sum(min_corrects) / num_data\n",
    "\n",
    "        # else:\n",
    "        #     loss, acc = self.test_all()\n",
    "\n",
    "        loss = np.mean(min_losses)\n",
    "        acc = np.sum(min_corrects) / num_data\n",
    "\n",
    "        # check cluster assignment acc\n",
    "        # cl_acc = self.get_cluster_accuracy(dataset['cluster_assign'], cluster_assign)\n",
    "        cl_ct = [np.sum(np.array(cluster_assign) == p_i ) for p_i in range(p)]\n",
    "\n",
    "\n",
    "        cluster_assign_ans = dataset['cluster_assign']\n",
    "        cluster_assign_ans_part = np.array(cluster_assign_ans)[participating_nodes]\n",
    "        cl_ct_ans = [np.sum(np.array(cluster_assign_ans_part) == p_i ) for p_i in range(p)]\n",
    "\n",
    "        res = {} # results\n",
    "        # res['losses'] = losses\n",
    "        # res['corrects'] = corrects\n",
    "        # res['cluster_assign'] = cluster_assign\n",
    "        res['loss'] = loss\n",
    "        res['acc'] = acc\n",
    "        # res['cl_acc'] = cl_acc\n",
    "        res['cl_ct'] = cl_ct\n",
    "        res['cl_ct_ans'] = cl_ct_ans\n",
    "        res['is_train'] = train\n",
    "\n",
    "        if train:\n",
    "            self.cluster_assign = cluster_assign\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    def load_node_data(self, m_i, train=True):\n",
    "        if train:\n",
    "            dataset = self.dataset['train']\n",
    "        else:\n",
    "            dataset = self.dataset['test']\n",
    "\n",
    "        indices = dataset['data_indices'][m_i]\n",
    "\n",
    "        return self.load_data_by_index(indices, m_i, train)\n",
    "\n",
    "    def load_data_by_index(self, indices, m_i, train=True):\n",
    "\n",
    "        # transform\n",
    "        # maybe improve speed by tf.data.Dataset.apply?\n",
    "        # or just run pool map to this...\n",
    "        # maybe not needed\n",
    "\n",
    "        cfg = self.config\n",
    "\n",
    "        if train:\n",
    "            dataset = self.dataset['train']\n",
    "            transform_op = self.train_transform_op\n",
    "            # transform_op = self.test_transform_op\n",
    "        else:\n",
    "            dataset = self.dataset['test']\n",
    "            transform_op = self.test_transform_op\n",
    "\n",
    "\n",
    "        X_b = dataset['data_loader'][0][indices]\n",
    "        y_b = dataset['data_loader'][1][indices]\n",
    "\n",
    "        p_i = dataset['cluster_assign'][m_i]\n",
    "\n",
    "        if cfg['p'] == 4:\n",
    "            k = p_i\n",
    "        elif cfg['p'] == 2:\n",
    "            k = (p_i % 2) * 2\n",
    "        elif cfg['p'] == 1:\n",
    "            k = 0\n",
    "        else:\n",
    "            raise NotImplementedError(\"only p=1,2,4 supported\")\n",
    "\n",
    "        X_b2 = np.rot90(X_b, k=k, axes = (1,2)) # X_b: (bs, 32, 32, 3)\n",
    "\n",
    "        X_b3 = self.sess.run(transform_op, feed_dict = { self.x_tr_pl : X_b2 } )\n",
    "\n",
    "        return (X_b3, y_b)\n",
    "\n",
    "\n",
    "    # def save_checkpoint(self):\n",
    "    #     models_to_save = [model.state_dict() for model in self.models]\n",
    "    #     torch.save({'models':models_to_save}, self.checkpoint_fname)\n",
    "\n",
    "    #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab84d2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: {'m': 200, 'm_test': 40, 'p': 2, 'n': 500, 'participation_rate': 0.1, 'num_epochs': 600, 'batch_size': 50, 'tau': 5, 'lr': 0.25, 'data_seed': 0, 'train_seed': 0, 'project_dir': 'output'}\n",
      "WARNING:tensorflow:From C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_6852\\3279985567.py:118: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\GIT Repos\\decentralized-ifca\\cifar_tf\\cifar10.py:143: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From d:\\GIT Repos\\decentralized-ifca\\cifar_tf\\cifar10.py:231: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_6852\\3279985567.py:134: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_6852\\2967275566.py:39: The name tf.image.resize_image_with_crop_or_pad is deprecated. Please use tf.image.resize_with_crop_or_pad instead.\n",
      "\n",
      "finding good initializer from train data\n",
      "Epoch -1 tr: l 4.680 a 0.088  clct[np.int64(3), np.int64(17)] ans[np.int64(10), np.int64(10)] 2.910sec\n",
      "Epoch -1 tr: l 4.674 a 0.098  clct[np.int64(20), np.int64(0)] ans[np.int64(9), np.int64(11)] 2.652sec\n",
      "Epoch -1 tr: l 4.675 a 0.098  clct[np.int64(6), np.int64(14)] ans[np.int64(8), np.int64(12)] 2.595sec\n",
      "Epoch -1 tr: l 4.676 a 0.090  clct[np.int64(20), np.int64(0)] ans[np.int64(9), np.int64(11)] 2.596sec\n",
      "Epoch -1 tr: l 4.675 a 0.101  clct[np.int64(0), np.int64(20)] ans[np.int64(11), np.int64(9)] 2.686sec\n",
      "Epoch -1 tr: l 4.675 a 0.106  clct[np.int64(17), np.int64(3)] ans[np.int64(7), np.int64(13)] 2.611sec\n",
      "Epoch -1 tr: l 4.676 a 0.107  clct[np.int64(9), np.int64(11)] ans[np.int64(6), np.int64(14)] 2.585sec\n",
      "found good initializer\n",
      "Epoch -1 tr: l 4.676 a 0.112  clct[np.int64(10), np.int64(10)] ans[np.int64(11), np.int64(9)] 2.554sec\n",
      "Epoch -1 tst: l 4.676 a 0.106  clct[np.int64(24), np.int64(16)] ans[np.int64(20), np.int64(20)] 5.069sec\n",
      "Epoch 0 tr: l 2.932 a 0.266  clct[np.int64(12), np.int64(8)] ans[np.int64(11), np.int64(9)] lr 0.250000 14.542sec(train) 2.560sec(infer)\n",
      "Epoch 0 tst: l 2.889 a 0.278  clct[np.int64(21), np.int64(19)] ans[np.int64(20), np.int64(20)] 5.034sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 1 tr: l 2.090 a 0.408  clct[np.int64(7), np.int64(13)] ans[np.int64(7), np.int64(13)] lr 0.247500 14.556sec(train) 2.683sec(infer)\n",
      "Epoch 1 tst: l 1.947 a 0.456  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.811sec\n",
      "Epoch 2 tr: l 1.868 a 0.448  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.245025 15.325sec(train) 2.907sec(infer)\n",
      "Epoch 2 tst: l 1.786 a 0.479  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.723sec\n",
      "Epoch 3 tr: l 1.780 a 0.486  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.242575 19.068sec(train) 3.070sec(infer)\n",
      "Epoch 3 tst: l 1.644 a 0.533  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.656sec\n",
      "Epoch 4 tr: l 1.772 a 0.499  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.240149 15.370sec(train) 2.917sec(infer)\n",
      "Epoch 4 tst: l 1.652 a 0.541  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.705sec\n",
      "Epoch 5 tr: l 1.752 a 0.522  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.237748 15.223sec(train) 2.922sec(infer)\n",
      "Epoch 5 tst: l 1.666 a 0.548  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.590sec\n",
      "Epoch 6 tr: l 1.754 a 0.520  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.235370 15.331sec(train) 2.940sec(infer)\n",
      "Epoch 6 tst: l 1.640 a 0.563  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.592sec\n",
      "Epoch 7 tr: l 1.717 a 0.567  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.233016 15.772sec(train) 2.940sec(infer)\n",
      "Epoch 7 tst: l 1.613 a 0.600  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.700sec\n",
      "Epoch 8 tr: l 1.682 a 0.562  clct[np.int64(13), np.int64(7)] ans[np.int64(13), np.int64(7)] lr 0.230686 15.549sec(train) 2.907sec(infer)\n",
      "Epoch 8 tst: l 1.533 a 0.614  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.700sec\n",
      "Epoch 9 tr: l 1.700 a 0.579  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.228379 15.111sec(train) 2.936sec(infer)\n",
      "Epoch 9 tst: l 1.610 a 0.609  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.617sec\n",
      "Epoch 10 tr: l 1.609 a 0.615  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.226096 15.314sec(train) 2.904sec(infer)\n",
      "Epoch 10 tst: l 1.521 a 0.642  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.639sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 11 tr: l 1.681 a 0.595  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.223835 15.444sec(train) 2.894sec(infer)\n",
      "Epoch 11 tst: l 1.603 a 0.617  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.589sec\n",
      "Epoch 12 tr: l 1.601 a 0.611  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.221596 15.255sec(train) 2.832sec(infer)\n",
      "Epoch 12 tst: l 1.509 a 0.642  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.689sec\n",
      "Epoch 13 tr: l 1.749 a 0.592  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.219380 15.299sec(train) 2.909sec(infer)\n",
      "Epoch 13 tst: l 1.670 a 0.619  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.689sec\n",
      "Epoch 14 tr: l 1.616 a 0.610  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.217186 15.211sec(train) 2.965sec(infer)\n",
      "Epoch 14 tst: l 1.543 a 0.636  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.911sec\n",
      "Epoch 15 tr: l 1.682 a 0.593  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.215015 15.547sec(train) 2.931sec(infer)\n",
      "Epoch 15 tst: l 1.556 a 0.636  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.608sec\n",
      "Epoch 16 tr: l 1.648 a 0.599  clct[np.int64(14), np.int64(6)] ans[np.int64(14), np.int64(6)] lr 0.212864 15.141sec(train) 2.932sec(infer)\n",
      "Epoch 16 tst: l 1.566 a 0.624  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.677sec\n",
      "Epoch 17 tr: l 1.544 a 0.637  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.210736 15.152sec(train) 2.927sec(infer)\n",
      "Epoch 17 tst: l 1.458 a 0.666  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.668sec\n",
      "Epoch 18 tr: l 1.605 a 0.617  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.208628 15.580sec(train) 2.938sec(infer)\n",
      "Epoch 18 tst: l 1.531 a 0.645  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.694sec\n",
      "Epoch 19 tr: l 1.616 a 0.618  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.206542 15.501sec(train) 3.149sec(infer)\n",
      "Epoch 19 tst: l 1.530 a 0.652  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.657sec\n",
      "Epoch 20 tr: l 1.631 a 0.607  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.204477 15.303sec(train) 2.828sec(infer)\n",
      "Epoch 20 tst: l 1.569 a 0.633  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.729sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 21 tr: l 1.512 a 0.653  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.202432 15.242sec(train) 2.958sec(infer)\n",
      "Epoch 21 tst: l 1.460 a 0.667  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.714sec\n",
      "Epoch 22 tr: l 1.526 a 0.636  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.200408 15.210sec(train) 3.042sec(infer)\n",
      "Epoch 22 tst: l 1.479 a 0.656  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.721sec\n",
      "Epoch 23 tr: l 1.559 a 0.639  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.198404 15.159sec(train) 2.933sec(infer)\n",
      "Epoch 23 tst: l 1.456 a 0.673  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.723sec\n",
      "Epoch 24 tr: l 1.783 a 0.612  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.196420 15.300sec(train) 3.173sec(infer)\n",
      "Epoch 24 tst: l 1.698 a 0.637  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.797sec\n",
      "Epoch 25 tr: l 1.513 a 0.655  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.194455 15.207sec(train) 2.962sec(infer)\n",
      "Epoch 25 tst: l 1.442 a 0.678  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.730sec\n",
      "Epoch 26 tr: l 1.591 a 0.637  clct[np.int64(5), np.int64(15)] ans[np.int64(5), np.int64(15)] lr 0.192511 15.151sec(train) 2.945sec(infer)\n",
      "Epoch 26 tst: l 1.452 a 0.678  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.670sec\n",
      "Epoch 27 tr: l 1.473 a 0.649  clct[np.int64(14), np.int64(6)] ans[np.int64(14), np.int64(6)] lr 0.190586 14.969sec(train) 2.876sec(infer)\n",
      "Epoch 27 tst: l 1.400 a 0.682  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.705sec\n",
      "Epoch 28 tr: l 1.567 a 0.637  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.188680 16.103sec(train) 3.001sec(infer)\n",
      "Epoch 28 tst: l 1.469 a 0.675  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.657sec\n",
      "Epoch 29 tr: l 1.585 a 0.628  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.186793 15.179sec(train) 3.026sec(infer)\n",
      "Epoch 29 tst: l 1.524 a 0.654  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.789sec\n",
      "Epoch 30 tr: l 1.467 a 0.663  clct[np.int64(5), np.int64(15)] ans[np.int64(5), np.int64(15)] lr 0.184925 19.606sec(train) 3.333sec(infer)\n",
      "Epoch 30 tst: l 1.426 a 0.679  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.409sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 31 tr: l 1.566 a 0.637  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.183076 17.291sec(train) 3.293sec(infer)\n",
      "Epoch 31 tst: l 1.413 a 0.682  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.411sec\n",
      "Epoch 32 tr: l 1.432 a 0.662  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.181245 17.609sec(train) 3.405sec(infer)\n",
      "Epoch 32 tst: l 1.375 a 0.694  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.469sec\n",
      "Epoch 33 tr: l 1.440 a 0.674  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.179433 17.656sec(train) 3.260sec(infer)\n",
      "Epoch 33 tst: l 1.421 a 0.689  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.599sec\n",
      "Epoch 34 tr: l 1.471 a 0.661  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.177638 18.303sec(train) 3.377sec(infer)\n",
      "Epoch 34 tst: l 1.425 a 0.681  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.745sec\n",
      "Epoch 35 tr: l 1.535 a 0.650  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.175862 17.568sec(train) 3.283sec(infer)\n",
      "Epoch 35 tst: l 1.400 a 0.689  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.416sec\n",
      "Epoch 36 tr: l 1.514 a 0.647  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.174103 17.889sec(train) 3.310sec(infer)\n",
      "Epoch 36 tst: l 1.455 a 0.673  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.300sec\n",
      "Epoch 37 tr: l 1.560 a 0.639  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.172362 16.887sec(train) 3.187sec(infer)\n",
      "Epoch 37 tst: l 1.488 a 0.662  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.271sec\n",
      "Epoch 38 tr: l 1.508 a 0.657  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.170639 16.873sec(train) 3.243sec(infer)\n",
      "Epoch 38 tst: l 1.434 a 0.683  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.283sec\n",
      "Epoch 39 tr: l 1.456 a 0.652  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.168932 16.927sec(train) 3.214sec(infer)\n",
      "Epoch 39 tst: l 1.397 a 0.684  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.340sec\n",
      "Epoch 40 tr: l 1.391 a 0.672  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.167243 17.247sec(train) 3.435sec(infer)\n",
      "Epoch 40 tst: l 1.326 a 0.702  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.957sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 41 tr: l 1.429 a 0.671  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.165571 17.649sec(train) 3.189sec(infer)\n",
      "Epoch 41 tst: l 1.371 a 0.692  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.296sec\n",
      "Epoch 42 tr: l 1.382 a 0.687  clct[np.int64(5), np.int64(15)] ans[np.int64(5), np.int64(15)] lr 0.163915 17.050sec(train) 3.197sec(infer)\n",
      "Epoch 42 tst: l 1.334 a 0.700  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.233sec\n",
      "Epoch 43 tr: l 1.401 a 0.686  clct[np.int64(7), np.int64(13)] ans[np.int64(7), np.int64(13)] lr 0.162276 16.643sec(train) 3.209sec(infer)\n",
      "Epoch 43 tst: l 1.322 a 0.715  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.178sec\n",
      "Epoch 44 tr: l 1.473 a 0.645  clct[np.int64(13), np.int64(7)] ans[np.int64(13), np.int64(7)] lr 0.160653 16.843sec(train) 3.233sec(infer)\n",
      "Epoch 44 tst: l 1.334 a 0.692  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.248sec\n",
      "Epoch 45 tr: l 1.440 a 0.663  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.159046 17.191sec(train) 3.209sec(infer)\n",
      "Epoch 45 tst: l 1.352 a 0.692  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.266sec\n",
      "Epoch 46 tr: l 1.348 a 0.693  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.157456 16.768sec(train) 3.186sec(infer)\n",
      "Epoch 46 tst: l 1.294 a 0.711  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.278sec\n",
      "Epoch 47 tr: l 1.619 a 0.641  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.155881 16.816sec(train) 3.198sec(infer)\n",
      "Epoch 47 tst: l 1.549 a 0.662  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.306sec\n",
      "Epoch 48 tr: l 1.344 a 0.685  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.154323 16.818sec(train) 3.174sec(infer)\n",
      "Epoch 48 tst: l 1.291 a 0.706  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.217sec\n",
      "Epoch 49 tr: l 1.370 a 0.683  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.152779 16.753sec(train) 3.198sec(infer)\n",
      "Epoch 49 tst: l 1.341 a 0.696  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.233sec\n",
      "Epoch 50 tr: l 1.464 a 0.675  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.151252 17.056sec(train) 3.236sec(infer)\n",
      "Epoch 50 tst: l 1.356 a 0.703  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.269sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 51 tr: l 1.460 a 0.661  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.149739 17.523sec(train) 3.160sec(infer)\n",
      "Epoch 51 tst: l 1.363 a 0.696  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.303sec\n",
      "Epoch 52 tr: l 1.345 a 0.688  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.148242 16.822sec(train) 3.229sec(infer)\n",
      "Epoch 52 tst: l 1.316 a 0.705  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.396sec\n",
      "Epoch 53 tr: l 1.381 a 0.681  clct[np.int64(13), np.int64(7)] ans[np.int64(13), np.int64(7)] lr 0.146759 17.308sec(train) 3.253sec(infer)\n",
      "Epoch 53 tst: l 1.293 a 0.711  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.233sec\n",
      "Epoch 54 tr: l 1.371 a 0.685  clct[np.int64(7), np.int64(13)] ans[np.int64(7), np.int64(13)] lr 0.145292 16.989sec(train) 3.284sec(infer)\n",
      "Epoch 54 tst: l 1.334 a 0.698  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.335sec\n",
      "Epoch 55 tr: l 1.356 a 0.682  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.143839 16.692sec(train) 3.231sec(infer)\n",
      "Epoch 55 tst: l 1.318 a 0.709  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.330sec\n",
      "Epoch 56 tr: l 1.280 a 0.709  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.142400 16.831sec(train) 3.192sec(infer)\n",
      "Epoch 56 tst: l 1.278 a 0.713  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.342sec\n",
      "Epoch 57 tr: l 1.358 a 0.697  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.140976 16.813sec(train) 3.245sec(infer)\n",
      "Epoch 57 tst: l 1.317 a 0.716  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.156sec\n",
      "Epoch 58 tr: l 1.323 a 0.697  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.139567 16.865sec(train) 3.380sec(infer)\n",
      "Epoch 58 tst: l 1.270 a 0.719  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.674sec\n",
      "Epoch 59 tr: l 1.454 a 0.673  clct[np.int64(5), np.int64(15)] ans[np.int64(5), np.int64(15)] lr 0.138171 16.711sec(train) 3.213sec(infer)\n",
      "Epoch 59 tst: l 1.296 a 0.714  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.288sec\n",
      "Epoch 60 tr: l 1.380 a 0.683  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.136789 16.651sec(train) 3.233sec(infer)\n",
      "Epoch 60 tst: l 1.300 a 0.710  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.327sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 61 tr: l 1.315 a 0.696  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.135421 16.945sec(train) 3.165sec(infer)\n",
      "Epoch 61 tst: l 1.263 a 0.727  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.344sec\n",
      "Epoch 62 tr: l 1.257 a 0.709  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.134067 16.799sec(train) 3.185sec(infer)\n",
      "Epoch 62 tst: l 1.222 a 0.724  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.359sec\n",
      "Epoch 63 tr: l 1.355 a 0.690  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.132726 17.220sec(train) 3.270sec(infer)\n",
      "Epoch 63 tst: l 1.290 a 0.712  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.491sec\n",
      "Epoch 64 tr: l 1.272 a 0.707  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.131399 19.027sec(train) 3.652sec(infer)\n",
      "Epoch 64 tst: l 1.232 a 0.727  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.663sec\n",
      "Epoch 65 tr: l 1.197 a 0.727  clct[np.int64(6), np.int64(14)] ans[np.int64(6), np.int64(14)] lr 0.130085 16.312sec(train) 3.228sec(infer)\n",
      "Epoch 65 tst: l 1.189 a 0.729  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.289sec\n",
      "Epoch 66 tr: l 1.304 a 0.704  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.128784 16.330sec(train) 3.186sec(infer)\n",
      "Epoch 66 tst: l 1.219 a 0.729  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.271sec\n",
      "Epoch 67 tr: l 1.276 a 0.704  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.127496 16.614sec(train) 3.281sec(infer)\n",
      "Epoch 67 tst: l 1.211 a 0.727  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.284sec\n",
      "Epoch 68 tr: l 1.224 a 0.721  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.126221 16.537sec(train) 3.637sec(infer)\n",
      "Epoch 68 tst: l 1.212 a 0.727  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.766sec\n",
      "Epoch 69 tr: l 1.324 a 0.697  clct[np.int64(13), np.int64(7)] ans[np.int64(13), np.int64(7)] lr 0.124959 19.211sec(train) 3.581sec(infer)\n",
      "Epoch 69 tst: l 1.261 a 0.715  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 7.068sec\n",
      "Epoch 70 tr: l 1.306 a 0.702  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.123710 18.940sec(train) 3.610sec(infer)\n",
      "Epoch 70 tst: l 1.239 a 0.723  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 7.222sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 71 tr: l 1.200 a 0.733  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.122473 17.694sec(train) 3.412sec(infer)\n",
      "Epoch 71 tst: l 1.253 a 0.729  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.848sec\n",
      "Epoch 72 tr: l 1.248 a 0.707  clct[np.int64(5), np.int64(15)] ans[np.int64(5), np.int64(15)] lr 0.121248 18.319sec(train) 3.602sec(infer)\n",
      "Epoch 72 tst: l 1.192 a 0.731  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.973sec\n",
      "Epoch 73 tr: l 1.325 a 0.697  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.120035 18.792sec(train) 4.064sec(infer)\n",
      "Epoch 73 tst: l 1.265 a 0.722  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 7.332sec\n",
      "Epoch 74 tr: l 1.246 a 0.710  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.118835 17.884sec(train) 3.645sec(infer)\n",
      "Epoch 74 tst: l 1.242 a 0.718  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.450sec\n",
      "Epoch 75 tr: l 1.216 a 0.728  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.117647 17.769sec(train) 3.553sec(infer)\n",
      "Epoch 75 tst: l 1.212 a 0.736  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 7.377sec\n",
      "Epoch 76 tr: l 1.255 a 0.709  clct[np.int64(13), np.int64(7)] ans[np.int64(13), np.int64(7)] lr 0.116470 18.354sec(train) 3.263sec(infer)\n",
      "Epoch 76 tst: l 1.216 a 0.725  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.645sec\n",
      "Epoch 77 tr: l 1.198 a 0.727  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.115305 16.770sec(train) 3.337sec(infer)\n",
      "Epoch 77 tst: l 1.166 a 0.740  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.588sec\n",
      "Epoch 78 tr: l 1.292 a 0.709  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.114152 17.628sec(train) 3.330sec(infer)\n",
      "Epoch 78 tst: l 1.231 a 0.727  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.638sec\n",
      "Epoch 79 tr: l 1.168 a 0.727  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.113011 17.092sec(train) 3.234sec(infer)\n",
      "Epoch 79 tst: l 1.142 a 0.738  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.358sec\n",
      "Epoch 80 tr: l 1.183 a 0.727  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.111881 16.782sec(train) 3.273sec(infer)\n",
      "Epoch 80 tst: l 1.153 a 0.747  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.359sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 81 tr: l 1.217 a 0.730  clct[np.int64(7), np.int64(13)] ans[np.int64(7), np.int64(13)] lr 0.110762 17.815sec(train) 3.600sec(infer)\n",
      "Epoch 81 tst: l 1.160 a 0.744  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 7.088sec\n",
      "Epoch 82 tr: l 1.214 a 0.718  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.109654 18.241sec(train) 3.431sec(infer)\n",
      "Epoch 82 tst: l 1.164 a 0.740  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 7.086sec\n",
      "Epoch 83 tr: l 1.162 a 0.736  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.108558 18.919sec(train) 3.593sec(infer)\n",
      "Epoch 83 tst: l 1.153 a 0.737  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.974sec\n",
      "Epoch 84 tr: l 1.159 a 0.733  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.107472 18.678sec(train) 4.147sec(infer)\n",
      "Epoch 84 tst: l 1.144 a 0.742  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 8.584sec\n",
      "Epoch 85 tr: l 1.202 a 0.720  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.106398 17.545sec(train) 3.519sec(infer)\n",
      "Epoch 85 tst: l 1.113 a 0.750  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.788sec\n",
      "Epoch 86 tr: l 1.188 a 0.723  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.105334 18.103sec(train) 3.350sec(infer)\n",
      "Epoch 86 tst: l 1.126 a 0.742  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.446sec\n",
      "Epoch 87 tr: l 1.162 a 0.723  clct[np.int64(6), np.int64(14)] ans[np.int64(6), np.int64(14)] lr 0.104280 15.897sec(train) 3.061sec(infer)\n",
      "Epoch 87 tst: l 1.108 a 0.742  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.104sec\n",
      "Epoch 88 tr: l 1.165 a 0.728  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.103237 15.830sec(train) 3.114sec(infer)\n",
      "Epoch 88 tst: l 1.145 a 0.741  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.040sec\n",
      "Epoch 89 tr: l 1.123 a 0.738  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.102205 16.084sec(train) 3.164sec(infer)\n",
      "Epoch 89 tst: l 1.098 a 0.749  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.059sec\n",
      "Epoch 90 tr: l 1.117 a 0.737  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.101183 15.887sec(train) 3.106sec(infer)\n",
      "Epoch 90 tst: l 1.109 a 0.746  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.992sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 91 tr: l 1.188 a 0.724  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.100171 16.327sec(train) 3.078sec(infer)\n",
      "Epoch 91 tst: l 1.157 a 0.742  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.040sec\n",
      "Epoch 92 tr: l 1.137 a 0.731  clct[np.int64(14), np.int64(6)] ans[np.int64(14), np.int64(6)] lr 0.099169 17.050sec(train) 3.437sec(infer)\n",
      "Epoch 92 tst: l 1.108 a 0.746  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 7.027sec\n",
      "Epoch 93 tr: l 1.168 a 0.731  clct[np.int64(7), np.int64(13)] ans[np.int64(7), np.int64(13)] lr 0.098178 17.660sec(train) 3.215sec(infer)\n",
      "Epoch 93 tst: l 1.134 a 0.749  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.234sec\n",
      "Epoch 94 tr: l 1.159 a 0.723  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.097196 16.028sec(train) 3.190sec(infer)\n",
      "Epoch 94 tst: l 1.114 a 0.741  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.194sec\n",
      "Epoch 95 tr: l 1.113 a 0.746  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.096224 16.564sec(train) 3.177sec(infer)\n",
      "Epoch 95 tst: l 1.124 a 0.745  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.533sec\n",
      "Epoch 96 tr: l 1.122 a 0.731  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.095262 14.972sec(train) 2.961sec(infer)\n",
      "Epoch 96 tst: l 1.089 a 0.750  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.731sec\n",
      "Epoch 97 tr: l 1.084 a 0.741  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.094309 14.991sec(train) 2.937sec(infer)\n",
      "Epoch 97 tst: l 1.105 a 0.748  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.730sec\n",
      "Epoch 98 tr: l 1.141 a 0.734  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.093366 15.092sec(train) 3.579sec(infer)\n",
      "Epoch 98 tst: l 1.080 a 0.754  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.991sec\n",
      "Epoch 99 tr: l 1.095 a 0.738  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.092432 16.182sec(train) 2.968sec(infer)\n",
      "Epoch 99 tst: l 1.088 a 0.746  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.832sec\n",
      "Epoch 100 tr: l 1.097 a 0.739  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.091508 14.899sec(train) 3.349sec(infer)\n",
      "Epoch 100 tst: l 1.064 a 0.755  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.309sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 101 tr: l 1.091 a 0.745  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.090593 16.415sec(train) 3.150sec(infer)\n",
      "Epoch 101 tst: l 1.106 a 0.749  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.202sec\n",
      "Epoch 102 tr: l 1.160 a 0.724  clct[np.int64(13), np.int64(7)] ans[np.int64(13), np.int64(7)] lr 0.089687 16.453sec(train) 2.958sec(infer)\n",
      "Epoch 102 tst: l 1.135 a 0.746  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.116sec\n",
      "Epoch 103 tr: l 1.139 a 0.731  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.088790 15.972sec(train) 2.940sec(infer)\n",
      "Epoch 103 tst: l 1.099 a 0.745  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.425sec\n",
      "Epoch 104 tr: l 0.991 a 0.770  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.087902 14.944sec(train) 2.924sec(infer)\n",
      "Epoch 104 tst: l 1.035 a 0.760  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.702sec\n",
      "Epoch 105 tr: l 1.159 a 0.723  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.087023 16.103sec(train) 3.087sec(infer)\n",
      "Epoch 105 tst: l 1.145 a 0.742  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.040sec\n",
      "Epoch 106 tr: l 1.091 a 0.738  clct[np.int64(13), np.int64(7)] ans[np.int64(13), np.int64(7)] lr 0.086153 16.596sec(train) 3.142sec(infer)\n",
      "Epoch 106 tst: l 1.082 a 0.746  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.891sec\n",
      "Epoch 107 tr: l 1.067 a 0.746  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.085292 17.453sec(train) 3.423sec(infer)\n",
      "Epoch 107 tst: l 1.099 a 0.750  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.793sec\n",
      "Epoch 108 tr: l 1.145 a 0.727  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.084439 17.185sec(train) 3.172sec(infer)\n",
      "Epoch 108 tst: l 1.052 a 0.759  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.688sec\n",
      "Epoch 109 tr: l 1.051 a 0.759  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.083594 16.354sec(train) 3.229sec(infer)\n",
      "Epoch 109 tst: l 1.040 a 0.763  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.294sec\n",
      "Epoch 110 tr: l 1.097 a 0.739  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.082758 16.893sec(train) 3.209sec(infer)\n",
      "Epoch 110 tst: l 1.048 a 0.758  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.994sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 111 tr: l 1.050 a 0.749  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.081931 14.981sec(train) 3.173sec(infer)\n",
      "Epoch 111 tst: l 1.016 a 0.765  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.427sec\n",
      "Epoch 112 tr: l 1.066 a 0.747  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.081111 15.879sec(train) 3.294sec(infer)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m exp \u001b[38;5;241m=\u001b[39m TrainCIFARCluster(config)\n\u001b[0;32m      6\u001b[0m exp\u001b[38;5;241m.\u001b[39msetup()\n\u001b[1;32m----> 7\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 291\u001b[0m, in \u001b[0;36mTrainCIFARCluster.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_epoch_stats(res)\n\u001b[0;32m    290\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 291\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    293\u001b[0m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfer_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m t1\u001b[38;5;241m-\u001b[39mt0\n",
      "Cell \u001b[1;32mIn[4], line 565\u001b[0m, in \u001b[0;36mTrainCIFARCluster.test\u001b[1;34m(self, train, force_full_nodes)\u001b[0m\n\u001b[0;32m    562\u001b[0m t_load_model \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tp1\u001b[38;5;241m-\u001b[39mtp0\n\u001b[0;32m    564\u001b[0m t00\u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 565\u001b[0m (X, y) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_node_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# load batch data rotated\u001b[39;00m\n\u001b[0;32m    566\u001b[0m t01\u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    567\u001b[0m t_load_data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m t01\u001b[38;5;241m-\u001b[39mt00\n",
      "Cell \u001b[1;32mIn[4], line 650\u001b[0m, in \u001b[0;36mTrainCIFARCluster.load_node_data\u001b[1;34m(self, m_i, train)\u001b[0m\n\u001b[0;32m    646\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    648\u001b[0m indices \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_indices\u001b[39m\u001b[38;5;124m'\u001b[39m][m_i]\n\u001b[1;32m--> 650\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data_by_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 675\u001b[0m, in \u001b[0;36mTrainCIFARCluster.load_data_by_index\u001b[1;34m(self, indices, m_i, train)\u001b[0m\n\u001b[0;32m    671\u001b[0m y_b \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_loader\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m][indices]\n\u001b[0;32m    673\u001b[0m p_i \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster_assign\u001b[39m\u001b[38;5;124m'\u001b[39m][m_i]\n\u001b[1;32m--> 675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m:\n\u001b[0;32m    676\u001b[0m     k \u001b[38;5;241m=\u001b[39m p_i\n\u001b[0;32m    677\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "config['train_seed'] = config['data_seed']\n",
    "print(\"config:\",config)\n",
    "\n",
    "exp = TrainCIFARCluster(config)\n",
    "exp.setup()\n",
    "exp.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
