{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f53520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_12488\\451422013.py:14: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\anaconda\\envs\\dl-new\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from util import *\n",
    "import cifar10\n",
    "\n",
    "\n",
    "\n",
    "LR_DECAY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c81b2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "\n",
    "    # read config json and update the sysarg\n",
    "    with open(\"config.json\", \"r\") as read_file:\n",
    "        config = json.load(read_file)\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b48588e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 24\n",
    "\n",
    "def train_transform(reshaped_image):\n",
    "    # copied from cifar10_input.py / distorted_input()\n",
    "\n",
    "    height = IMAGE_SIZE\n",
    "    width = IMAGE_SIZE\n",
    "\n",
    "    # Image processing for training the network. Note the many random\n",
    "    # distortions applied to the image.\n",
    "\n",
    "    # Randomly crop a [height, width] section of the image.\n",
    "    distorted_image = tf.random_crop(reshaped_image, [tf.shape(reshaped_image)[0], height, width, 3])\n",
    "    # tf shape gives dynamic shape\n",
    "\n",
    "    # Randomly flip the image horizontally.\n",
    "    distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "\n",
    "    # Because these operations are not commutative, consider randomizing\n",
    "    # the order their operation.\n",
    "    distorted_image = tf.image.random_brightness(distorted_image,\n",
    "                                               max_delta=63)\n",
    "    distorted_image = tf.image.random_contrast(distorted_image,\n",
    "                                             lower=0.2, upper=1.8)\n",
    "\n",
    "    # Subtract off the mean and divide by the variance of the pixels.\n",
    "    float_image = tf.image.per_image_standardization(distorted_image)\n",
    "\n",
    "    return float_image\n",
    "\n",
    "def test_transform(reshaped_image):\n",
    "    # copied from cifar10_input.py / input()\n",
    "\n",
    "    height = IMAGE_SIZE\n",
    "    width = IMAGE_SIZE\n",
    "\n",
    "    # Image processing for evaluation.\n",
    "    # Crop the central [height, width] of the image.\n",
    "    resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image,\n",
    "                                                         width, height)\n",
    "\n",
    "    # Subtract off the mean and divide by the variance of the pixels.\n",
    "    float_image = tf.image.per_image_standardization(resized_image)\n",
    "\n",
    "    return float_image\n",
    "\n",
    "\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "\n",
    "def create_batches(pmt, batch_size):\n",
    "    batch_indices = []\n",
    "    ct = 0\n",
    "    for b_i in range(int(np.ceil( len(pmt) / batch_size))):\n",
    "        if ct + batch_size > len(pmt):\n",
    "            batch = pmt[ct : len(pmt)]\n",
    "            ct = len(pmt)\n",
    "        else:\n",
    "            batch = pmt[ct : ct + batch_size]\n",
    "            ct += batch_size\n",
    "        batch_indices.append(batch)\n",
    "\n",
    "    return batch_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "490bb8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainCIFARCluster(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        assert self.config['m'] % self.config['p'] == 0\n",
    "\n",
    "    def setup(self):\n",
    "\n",
    "        os.makedirs(self.config['project_dir'], exist_ok = True)\n",
    "\n",
    "        self.result_fname = os.path.join(self.config['project_dir'], 'results')\n",
    "        self.checkpoint_fname = os.path.join(self.config['project_dir'], 'checkpoint')\n",
    "\n",
    "        set_random_seed(self.config['data_seed'])\n",
    "        self.setup_datasets()\n",
    "        self.setup_model()\n",
    "\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config)\n",
    "\n",
    "\n",
    "        set_random_seed(self.config['data_seed']+self.config['train_seed'])\n",
    "        self.initialize_models()\n",
    "        self.initialize_assign_ops()\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        self.epoch = None\n",
    "        self.lr = None\n",
    "\n",
    "\n",
    "    def setup_datasets(self):\n",
    "        # tf.enable_eager_execution()\n",
    "\n",
    "        # generate indices for each dataset\n",
    "        # also write cluster info\n",
    "\n",
    "        CIFAR10_TRAINSET_DATA_SIZE = 50000\n",
    "        CIFAR10_TESTSET_DATA_SIZE = 10000\n",
    "\n",
    "        cfg = self.config\n",
    "\n",
    "        self.dataset = {}\n",
    "\n",
    "        dataset = {}\n",
    "        dataset['data_indices'], dataset['cluster_assign'] = \\\n",
    "            self._setup_dataset(CIFAR10_TRAINSET_DATA_SIZE, cfg['p'], cfg['m'], cfg['n'])\n",
    "        dl = self._load_CIFAR(train=True)\n",
    "        dataset['data_loader'] = dl\n",
    "        self.dataset['train'] = dataset\n",
    "\n",
    "        dataset = {}\n",
    "        dataset['data_indices'], dataset['cluster_assign'] = \\\n",
    "            self._setup_dataset(CIFAR10_TESTSET_DATA_SIZE, cfg['p'], cfg['m_test'], cfg['n'], random=False)\n",
    "        dl = self._load_CIFAR(train=False)\n",
    "        dataset['data_loader'] = dl\n",
    "        self.dataset['test'] = dataset\n",
    "\n",
    "        # tf.disable_eager_execution()\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "\n",
    "    def _setup_dataset(self, num_data, p, m, n, random = True):\n",
    "\n",
    "        assert (m // p) * n == num_data\n",
    "\n",
    "        dataset = {}\n",
    "\n",
    "        cfg = self.config\n",
    "\n",
    "        data_indices = []\n",
    "        cluster_assign = []\n",
    "\n",
    "        m_per_cluster = m // p\n",
    "\n",
    "        for p_i in range(p):\n",
    "\n",
    "            if random:\n",
    "                ll = list(np.random.permutation(num_data))\n",
    "            else:\n",
    "                ll = list(range(num_data))\n",
    "\n",
    "            ll2 = chunkify(ll, m_per_cluster) # splits ll into m lists with size n\n",
    "            data_indices += ll2\n",
    "\n",
    "            cluster_assign += [p_i for _ in range(m_per_cluster)]\n",
    "\n",
    "        data_indices = np.array(data_indices)\n",
    "        cluster_assign = np.array(cluster_assign)\n",
    "        assert data_indices.shape[0] == cluster_assign.shape[0]\n",
    "        assert data_indices.shape[0] == m\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        return data_indices, cluster_assign\n",
    "\n",
    "\n",
    "    def _load_CIFAR(self, train=True):\n",
    "        # gives dataloader that gives (X,y) based on asked index\n",
    "\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "        # (50000, 32,32, 3) [0~1] , (50000, 1)\n",
    "\n",
    "        if train:\n",
    "            X = x_train / 255.0\n",
    "            y = y_train.reshape(-1)\n",
    "        else:\n",
    "            X = x_test / 255.0\n",
    "            y = y_test.reshape(-1)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    def setup_model(self):\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "\n",
    "        # setup tensorflow model structure\n",
    "\n",
    "        self.x_pl = tf.placeholder(tf.float32, shape=(None, 24, 24, 3), name='input_x')\n",
    "        self.y_pl = tf.placeholder(tf.int32, shape=(None, ), name='output_y')\n",
    "        self.lr_pl = tf.placeholder(tf.float32, shape=(), name='learning_rate')\n",
    "\n",
    "        self.y_logits = cifar10.inference(self.x_pl) # construct model\n",
    "        self.loss = cifar10.loss(self.y_logits, self.y_pl)\n",
    "\n",
    "        self.y_pred = tf.cast(tf.argmax(self.y_logits, 1), tf.int32)\n",
    "        self.correct_prediction = tf.equal(self.y_pred, self.y_pl) # used for accuracy\n",
    "        self.num_correct = tf.reduce_sum(tf.cast(self.correct_prediction, tf.int64))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.lr_pl)\n",
    "        self.train_op = self.optimizer.minimize(self.loss)\n",
    "        self.opt_reset_op = tf.variables_initializer(self.optimizer.variables())\n",
    "\n",
    "        # import ipdb; ipdb.set_trace() # check self.optimizer.variables()\n",
    "\n",
    "        self.metrics = { # used by self.eval()\n",
    "            'loss':self.loss,\n",
    "            'correct': self.num_correct,\n",
    "            # and add more...\n",
    "        }\n",
    "\n",
    "\n",
    "        # transform ops\n",
    "        self.x_tr_pl = tf.placeholder(tf.float32, shape=(None, 32, 32, 3))\n",
    "        # with tf.device('/cpu:0'):\n",
    "        self.train_transform_op = train_transform(self.x_tr_pl)\n",
    "        self.test_transform_op = test_transform(self.x_tr_pl)\n",
    "\n",
    "\n",
    "    def initialize_models(self):\n",
    "\n",
    "        p = self.config['p']\n",
    "        m = self.config['m']\n",
    "\n",
    "        # initialize p times, to get p different sets of weights.\n",
    "\n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "        self.model_weights = []\n",
    "        models = []\n",
    "        for p_i in range(p):\n",
    "            self.sess.run(self.init_op)\n",
    "            weights = self.get_model_weights()\n",
    "            models.append(weights)\n",
    "\n",
    "        for m_i in range(m):\n",
    "            self.model_weights.append(copy.deepcopy(models))\n",
    "\n",
    "    def get_model_weights(self):\n",
    "        self.collection = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "        names = [var.name for var in self.collection]\n",
    "        weights_arrays = self.sess.run(self.collection)\n",
    "\n",
    "        weights = dict(zip(names, weights_arrays))\n",
    "        # {'conv1/weights:0': np.array, ...}\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def initialize_assign_ops(self):\n",
    "        self.collection = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "        assign_ops = {}\n",
    "        assign_pls = {}\n",
    "        for var in self.collection:\n",
    "            # import ipdb; ipdb.set_trace()\n",
    "            pl = tf.placeholder(tf.float32, shape=var.shape)\n",
    "            assign_pls[var.name] = pl\n",
    "\n",
    "            op = tf.compat.v1.assign(var, pl)\n",
    "            assign_ops[var.name] = op\n",
    "\n",
    "\n",
    "        self.assign_ops = assign_ops\n",
    "        self.assign_pls = assign_pls\n",
    "\n",
    "    def put_model_weights(self, weights):\n",
    "\n",
    "        assign_ops = []\n",
    "\n",
    "        fd = {}\n",
    "        for var_name in self.assign_pls:\n",
    "            # assign_op = tf.assign(var, weights[var.name])\n",
    "            pl = self.assign_pls[var_name]\n",
    "            fd[pl] = weights[var_name]\n",
    "\n",
    "        self.sess.run(self.opt_reset_op) # reset the optimizer state ?\n",
    "        self.sess.run(list(self.assign_ops.values()), feed_dict = fd)\n",
    "\n",
    "    def average_model_weights(self, weights_list):\n",
    "\n",
    "        w2 = {}\n",
    "\n",
    "        for key in weights_list[0].keys():\n",
    "\n",
    "            w2[key] = np.mean([w[key] for w in weights_list], axis=0)\n",
    "\n",
    "        return w2\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        TRAIN_INFER_FULL_NODES = 0\n",
    "\n",
    "        num_epochs = self.config['num_epochs']\n",
    "        lr = self.config['lr']\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # epoch -1\n",
    "        self.epoch = -1\n",
    "\n",
    "        self.find_good_initializer()\n",
    "\n",
    "\n",
    "        result = {}\n",
    "        result['epoch'] = -1\n",
    "\n",
    "        t0 = time.time()\n",
    "        self.set_participating_nodes()\n",
    "        res = self.test(train=True, force_full_nodes =TRAIN_INFER_FULL_NODES)\n",
    "        # res = self.test(train=True)\n",
    "        t1 = time.time()\n",
    "        res['infer_time'] = t1-t0\n",
    "        result['train'] = res\n",
    "\n",
    "        self.print_epoch_stats(res)\n",
    "\n",
    "        t0 = time.time()\n",
    "        res = self.test(train=False)\n",
    "        t1 = time.time()\n",
    "        res['infer_time'] = t1-t0\n",
    "        result['test'] = res\n",
    "        self.print_epoch_stats(res)\n",
    "        results.append(result)\n",
    "\n",
    "        # this will be used in next epoch\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            result = {}\n",
    "            result['epoch'] = epoch\n",
    "\n",
    "            lr = self.lr_schedule(epoch)\n",
    "            result['lr'] = lr\n",
    "\n",
    "            t0 = time.time()\n",
    "            result['train'] = self.train(lr = lr)\n",
    "            t1 = time.time()\n",
    "            train_time = t1-t0\n",
    "\n",
    "            t0 = time.time()\n",
    "            self.set_participating_nodes()\n",
    "            # res = self.test(train=True)\n",
    "            res = self.test(train=True, force_full_nodes =TRAIN_INFER_FULL_NODES)\n",
    "            t1 = time.time()\n",
    "            res['infer_time'] = t1-t0\n",
    "            res['train_time'] = train_time\n",
    "            res['lr'] = lr\n",
    "            result['train'] = res\n",
    "\n",
    "            self.print_epoch_stats(res)\n",
    "\n",
    "            t0 = time.time()\n",
    "            res = self.test(train=False)\n",
    "            t1 = time.time()\n",
    "            res['infer_time'] = t1-t0\n",
    "            result['test'] = res\n",
    "            self.print_epoch_stats(res)\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "            if epoch % 10 == 0 or epoch == num_epochs - 1 :\n",
    "                with open(self.result_fname+\".pickle\", 'wb') as outfile:\n",
    "                    pickle.dump(results, outfile)\n",
    "                    print(f'result written at {self.result_fname+\".pickle\"}')\n",
    "                # self.save_checkpoint()\n",
    "                # print(f'checkpoint written at {self.checkpoint_fname}')\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "\n",
    "    def find_good_initializer(self):\n",
    "        print(\"finding good initializer from train data\")\n",
    "\n",
    "        cfg = self.config\n",
    "\n",
    "        if cfg['p'] == 4:\n",
    "            th = 0.1\n",
    "        elif cfg['p'] == 2:\n",
    "            th = 0.35\n",
    "        elif cfg['p'] == 1:\n",
    "            th = 0.0\n",
    "        else:\n",
    "            raise NotImplementedError(\"only p=1,2,4 supported\")\n",
    "\n",
    "        is_not_good = True\n",
    "        while is_not_good:\n",
    "            self.initialize_models()\n",
    "            t0 = time.time()\n",
    "            self.set_participating_nodes()\n",
    "            # res = self.test(train=True, force_full_nodes = True)\n",
    "            res = self.test(train=True)\n",
    "            t1 = time.time()\n",
    "            res['infer_time'] = t1-t0\n",
    "            self.print_epoch_stats(res)\n",
    "\n",
    "            cl_ct = res['cl_ct']\n",
    "\n",
    "            num_nodes = np.sum(cl_ct)\n",
    "            is_not_good = False\n",
    "            for ct in cl_ct:\n",
    "                if ct / num_nodes < th:\n",
    "                    is_not_good = True\n",
    "\n",
    "        print(\"found good initializer\")\n",
    "\n",
    "\n",
    "\n",
    "    def set_participating_nodes(self):\n",
    "        cfg = self.config\n",
    "        m = cfg['m']\n",
    "        p = cfg['p']\n",
    "        p_rate = cfg['participation_rate']\n",
    "\n",
    "        self.participating_nodes = np.random.choice(m, int(m * p_rate), replace = False)\n",
    "\n",
    "        return self.participating_nodes\n",
    "\n",
    "    def lr_schedule(self, epoch):\n",
    "        if self.lr is None:\n",
    "            self.lr = self.config['lr']\n",
    "\n",
    "        if epoch != 0 and LR_DECAY:\n",
    "            self.lr = self.lr * 0.99\n",
    "\n",
    "        return self.lr\n",
    "\n",
    "\n",
    "    def print_epoch_stats(self, res):\n",
    "        if res['is_train']:\n",
    "            data_str = 'tr'\n",
    "        else:\n",
    "            data_str = 'tst'\n",
    "\n",
    "        if 'train_time' in res:\n",
    "            time_str = f\"{res['train_time']:.3f}sec(train) {res['infer_time']:.3f}sec(infer)\"\n",
    "        else:\n",
    "            time_str = f\"{res['infer_time']:.3f}sec\"\n",
    "\n",
    "        if 'lr' in res:\n",
    "            lr_str = f\" lr {res['lr']:4f}\"\n",
    "        else:\n",
    "            lr_str = \"\"\n",
    "\n",
    "        if 'cl_ct' in res:\n",
    "            cl_str = f\" clct{res['cl_ct']} ans{res['cl_ct_ans']}\"\n",
    "        else:\n",
    "            cl_str = \"\"\n",
    "\n",
    "        str0 = f\"Epoch {self.epoch} {data_str}: l {res['loss']:.3f} a {res['acc']:.3f} {cl_str}{lr_str} {time_str}\"\n",
    "\n",
    "        print(str0)\n",
    "\n",
    "    def train(self, lr):\n",
    "\n",
    "        VERBOSE = 0\n",
    "\n",
    "        cfg = self.config\n",
    "        m = cfg['m']\n",
    "        p = cfg['p']\n",
    "        tau = cfg['tau']\n",
    "        n = cfg['n']\n",
    "        batch_size = cfg['batch_size']\n",
    "\n",
    "        participating_nodes = self.participating_nodes\n",
    "        cluster_assign = self.cluster_assign\n",
    "\n",
    "        t_put_weight = 0\n",
    "        t_get_weight = 0\n",
    "        time_load_data = 0\n",
    "        time_train = 0\n",
    "\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        updated_local_weights = []\n",
    "        for m_i2, m_i in enumerate(participating_nodes):\n",
    "            p_i = cluster_assign[m_i]\n",
    "\n",
    "            # if VERBOSE and m_i2 % 1 == 0: print(f'Local update m_i2 {m_i2}/{len(participating_nodes)} processing \\r', end ='')\n",
    "            if VERBOSE and m_i2 % 1 == 0: print(f'Local update m_i2 {m_i2}/{len(participating_nodes)} processing')\n",
    "\n",
    "            # Local Update process\n",
    "\n",
    "            t_p = time.time()\n",
    "            self.put_model_weights(self.model_weights[m_i][p_i])\n",
    "            t_p1 = time.time()\n",
    "            t_put_weight += t_p1-t_p\n",
    "\n",
    "            for l_epoch in range(tau): # local epochs\n",
    "\n",
    "                pmt = np.random.permutation(n)\n",
    "                local_indices_list = create_batches(pmt, batch_size = batch_size)\n",
    "                node_data_indices = self.dataset['train']['data_indices'][m_i]\n",
    "\n",
    "                for b_i, local_indices in enumerate(local_indices_list):\n",
    "                    t00 = time.time()\n",
    "\n",
    "                    current_batch_indices = node_data_indices[local_indices]\n",
    "\n",
    "                    (X_b, y_b) = self.load_data_by_index(current_batch_indices, m_i)\n",
    "\n",
    "                    t01 = time.time()\n",
    "\n",
    "                    fd0 = {\n",
    "                        self.x_pl:X_b,\n",
    "                        self.y_pl:y_b,\n",
    "                        self.lr_pl:self.lr\n",
    "                    }\n",
    "                    self.sess.run([self.train_op], feed_dict= fd0)\n",
    "\n",
    "                    t02 = time.time()\n",
    "\n",
    "                    time_load_data += t01 - t00\n",
    "                    time_train += t02 - t01\n",
    "\n",
    "\n",
    "            t_p = time.time()\n",
    "            updated_local_weight = self.get_model_weights()\n",
    "            t_p1 = time.time()\n",
    "            t_get_weight += t_p1-t_p\n",
    "\n",
    "            self.model_weights[m_i][p_i] = copy.deepcopy(updated_local_weight)\n",
    "            # updated_local_weights.append(updated_local_weight)\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        # averaging\n",
    "\n",
    "        counts = {i: 0 for i in range(p)}\n",
    "        for m_i2, m_i in enumerate(participating_nodes):\n",
    "            counts[cluster_assign[m_i]] += 1\n",
    "\n",
    "        for m_i2, m_i in enumerate(participating_nodes):\n",
    "            p_i = cluster_assign[m_i]\n",
    "            num_clients = len(participating_nodes)\n",
    "            \n",
    "            num_cluster_i = counts[cluster_assign[m_i]]\n",
    "            num_cluster_rest = num_clients - num_cluster_i\n",
    "\n",
    "            threshold_j = min(num_cluster_rest, 100)\n",
    "            threshold_i = min(num_cluster_i, 100)\n",
    "\n",
    "            if threshold_i <=1 or threshold_j <= 1:\n",
    "                continue\n",
    "\n",
    "            # selected_clients = random.sample([i for _, i in enumerate(participating_nodes) if i != m_i], int(np.random.randint(1, min(threshold_i,threshold_j), (1,))))\n",
    "            selected_clients =  random.sample([i for _, i in enumerate(participating_nodes) if i != m_i], min(threshold_i,threshold_j))\n",
    "            m_i_cluster = cluster_assign[m_i]\n",
    "\n",
    "            for m_j in selected_clients:\n",
    "                m_j_cluster = cluster_assign[m_j]\n",
    "                \n",
    "                self.model_weights[m_i][m_j_cluster] = self.average_model_weights([self.model_weights[m_i][m_j_cluster], self.model_weights[m_j][m_j_cluster]])\n",
    "                self.model_weights[m_j][m_i_cluster] = self.average_model_weights([self.model_weights[m_i][m_i_cluster], self.model_weights[m_j][m_i_cluster]])\n",
    "\n",
    "\n",
    "        # for p_i in range(p):\n",
    "        #     if len(local_weights_cluster[p_i]) > 0:\n",
    "        #         self.model_weights[p_i] = self.average_model_weights(local_weights_cluster[p_i])\n",
    "\n",
    "        t2 = time.time()\n",
    "\n",
    "        if VERBOSE: print(f\"train_whole {t1-t0:.3f} t_gd {time_train:.3f} t load data {time_load_data:.3f} t put model {t_put_weight:.3f} t get mdoel {t_get_weight:.3f}  averaging {t2-t1:.3f}\")\n",
    "\n",
    "    def get_cluster_accuracy(self, actual, pred):\n",
    "        \n",
    "        cm = confusion_matrix(actual, pred)\n",
    "\n",
    "        row_ind, col_ind = linear_sum_assignment(-cm)\n",
    "        matching = dict(zip(col_ind, row_ind))\n",
    "\n",
    "        remapped_preds = [matching[p] for p in pred]\n",
    "\n",
    "        cl_acc = np.mean(np.array(remapped_preds) == np.array(actual))\n",
    "\n",
    "        return cl_acc\n",
    "\n",
    "\n",
    "    def test(self, train = True, force_full_nodes = False):\n",
    "\n",
    "        VERBOSE = 0\n",
    "\n",
    "        cfg = self.config\n",
    "        p = cfg['p']\n",
    "        p_rate = cfg['participation_rate']\n",
    "\n",
    "\n",
    "        if train:\n",
    "            m = cfg['m']\n",
    "            dataset = self.dataset['train']\n",
    "            if force_full_nodes:\n",
    "                participating_nodes = list(range(m))\n",
    "            else:\n",
    "                participating_nodes = self.participating_nodes\n",
    "        else:\n",
    "            m = cfg['m_test']\n",
    "            dataset = self.dataset['test']\n",
    "            participating_nodes = list(range(m))\n",
    "\n",
    "            # DEBUGGING\n",
    "            # print(\"DEBUGGING MODEe\")\n",
    "            # participating_nodes = np.random.choice(m, int(m * p_rate), replace = False)\n",
    "\n",
    "\n",
    "        # get loss and correct from all data\n",
    "\n",
    "\n",
    "        t_load_model = 0\n",
    "        t_load_data = 0\n",
    "        t_infer = 0\n",
    "\n",
    "        losses = {}\n",
    "        corrects = {}\n",
    "        num_data = 0\n",
    "        for m_i in participating_nodes:\n",
    "            for p_i in range(p):\n",
    "\n",
    "                tp0= time.time()\n",
    "                self.put_model_weights(self.model_weights[m_i][p_i])\n",
    "                tp1= time.time()\n",
    "                t_load_model += tp1-tp0\n",
    "\n",
    "                t00= time.time()\n",
    "                (X, y) = self.load_node_data(m_i, train=train) # load batch data rotated\n",
    "                t01= time.time()\n",
    "                t_load_data += t01-t00\n",
    "\n",
    "                ti0= time.time()\n",
    "                (loss, correct) = self.sess.run([self.loss, self.num_correct], feed_dict = {self.x_pl:X, self.y_pl:y})\n",
    "                ti1= time.time()\n",
    "                t_infer += ti1-ti0\n",
    "\n",
    "\n",
    "                losses[(m_i,p_i)] = loss\n",
    "                corrects[(m_i,p_i)] = correct\n",
    "\n",
    "            num_data += X.shape[0]\n",
    "\n",
    "\n",
    "        if VERBOSE: print(f\"loadmodel {t_load_model:.3f}, load data {t_load_data:.3f}, infer {t_infer:.3f}\")\n",
    "\n",
    "\n",
    "        # calculate loss and cluster the machines\n",
    "        cluster_assign = [-1 for _ in range(m)]\n",
    "        for m_i in participating_nodes:\n",
    "            machine_losses = [ losses[(m_i,p_i)] for p_i in range(p) ]\n",
    "            min_p_i = np.argmin(machine_losses)\n",
    "            cluster_assign[m_i] = min_p_i\n",
    "\n",
    "        # calculate optimal model's loss, acc over all models\n",
    "\n",
    "        min_corrects = []\n",
    "        min_losses = []\n",
    "        for m_i in participating_nodes:\n",
    "            p_i = cluster_assign[m_i]\n",
    "\n",
    "            min_loss = losses[(m_i,p_i)]\n",
    "            min_losses.append(min_loss)\n",
    "\n",
    "            min_correct = corrects[(m_i,p_i)]\n",
    "            min_corrects.append(min_correct)\n",
    "\n",
    "        # if train:\n",
    "        #     loss = np.mean(min_losses)\n",
    "        #     acc = np.sum(min_corrects) / num_data\n",
    "\n",
    "        # else:\n",
    "        #     loss, acc = self.test_all()\n",
    "\n",
    "        loss = np.mean(min_losses)\n",
    "        acc = np.sum(min_corrects) / num_data\n",
    "\n",
    "        # check cluster assignment acc\n",
    "        # cl_acc = self.get_cluster_accuracy(dataset['cluster_assign'], cluster_assign)\n",
    "        cl_ct = [np.sum(np.array(cluster_assign) == p_i ) for p_i in range(p)]\n",
    "\n",
    "\n",
    "        cluster_assign_ans = dataset['cluster_assign']\n",
    "        cluster_assign_ans_part = np.array(cluster_assign_ans)[participating_nodes]\n",
    "        cl_ct_ans = [np.sum(np.array(cluster_assign_ans_part) == p_i ) for p_i in range(p)]\n",
    "\n",
    "        res = {} # results\n",
    "        # res['losses'] = losses\n",
    "        # res['corrects'] = corrects\n",
    "        # res['cluster_assign'] = cluster_assign\n",
    "        res['loss'] = loss\n",
    "        res['acc'] = acc\n",
    "        # res['cl_acc'] = cl_acc\n",
    "        res['cl_ct'] = cl_ct\n",
    "        res['cl_ct_ans'] = cl_ct_ans\n",
    "        res['is_train'] = train\n",
    "\n",
    "        if train:\n",
    "            self.cluster_assign = cluster_assign\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    def load_node_data(self, m_i, train=True):\n",
    "        if train:\n",
    "            dataset = self.dataset['train']\n",
    "        else:\n",
    "            dataset = self.dataset['test']\n",
    "\n",
    "        indices = dataset['data_indices'][m_i]\n",
    "\n",
    "        return self.load_data_by_index(indices, m_i, train)\n",
    "\n",
    "    def load_data_by_index(self, indices, m_i, train=True):\n",
    "\n",
    "        # transform\n",
    "        # maybe improve speed by tf.data.Dataset.apply?\n",
    "        # or just run pool map to this...\n",
    "        # maybe not needed\n",
    "\n",
    "        cfg = self.config\n",
    "\n",
    "        if train:\n",
    "            dataset = self.dataset['train']\n",
    "            transform_op = self.train_transform_op\n",
    "            # transform_op = self.test_transform_op\n",
    "        else:\n",
    "            dataset = self.dataset['test']\n",
    "            transform_op = self.test_transform_op\n",
    "\n",
    "\n",
    "        X_b = dataset['data_loader'][0][indices]\n",
    "        y_b = dataset['data_loader'][1][indices]\n",
    "\n",
    "        p_i = dataset['cluster_assign'][m_i]\n",
    "\n",
    "        if cfg['p'] == 4:\n",
    "            k = p_i\n",
    "        elif cfg['p'] == 2:\n",
    "            k = (p_i % 2) * 2\n",
    "        elif cfg['p'] == 1:\n",
    "            k = 0\n",
    "        else:\n",
    "            raise NotImplementedError(\"only p=1,2,4 supported\")\n",
    "\n",
    "        X_b2 = np.rot90(X_b, k=k, axes = (1,2)) # X_b: (bs, 32, 32, 3)\n",
    "\n",
    "        X_b3 = self.sess.run(transform_op, feed_dict = { self.x_tr_pl : X_b2 } )\n",
    "\n",
    "        return (X_b3, y_b)\n",
    "\n",
    "\n",
    "    # def save_checkpoint(self):\n",
    "    #     models_to_save = [model.state_dict() for model in self.models]\n",
    "    #     torch.save({'models':models_to_save}, self.checkpoint_fname)\n",
    "\n",
    "    #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab84d2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: {'m': 200, 'm_test': 40, 'p': 2, 'n': 500, 'participation_rate': 0.1, 'num_epochs': 600, 'batch_size': 50, 'tau': 5, 'lr': 0.25, 'data_seed': 0, 'train_seed': 0, 'project_dir': 'output'}\n",
      "WARNING:tensorflow:From C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_12488\\1716388567.py:117: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\GIT Repos\\decentralized-ifca\\cifar_tf\\cifar10.py:143: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From d:\\GIT Repos\\decentralized-ifca\\cifar_tf\\cifar10.py:231: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_12488\\1716388567.py:133: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_12488\\2967275566.py:39: The name tf.image.resize_image_with_crop_or_pad is deprecated. Please use tf.image.resize_with_crop_or_pad instead.\n",
      "\n",
      "finding good initializer from train data\n",
      "Epoch -1 tr: l 4.673 a 0.102  clct[np.int64(20), np.int64(0)] ans[np.int64(10), np.int64(10)] 3.328sec\n",
      "Epoch -1 tr: l 4.674 a 0.105  clct[np.int64(6), np.int64(14)] ans[np.int64(9), np.int64(11)] 3.247sec\n",
      "Epoch -1 tr: l 4.673 a 0.090  clct[np.int64(20), np.int64(0)] ans[np.int64(8), np.int64(12)] 3.202sec\n",
      "Epoch -1 tr: l 4.672 a 0.103  clct[np.int64(20), np.int64(0)] ans[np.int64(9), np.int64(11)] 3.261sec\n",
      "Epoch -1 tr: l 4.673 a 0.140  clct[np.int64(0), np.int64(20)] ans[np.int64(11), np.int64(9)] 3.302sec\n",
      "Epoch -1 tr: l 4.673 a 0.091  clct[np.int64(14), np.int64(6)] ans[np.int64(7), np.int64(13)] 3.283sec\n",
      "Epoch -1 tr: l 4.673 a 0.105  clct[np.int64(0), np.int64(20)] ans[np.int64(6), np.int64(14)] 3.094sec\n",
      "Epoch -1 tr: l 4.675 a 0.108  clct[np.int64(9), np.int64(11)] ans[np.int64(11), np.int64(9)] 3.141sec\n",
      "found good initializer\n",
      "Epoch -1 tr: l 4.675 a 0.101  clct[np.int64(10), np.int64(10)] ans[np.int64(8), np.int64(12)] 3.190sec\n",
      "Epoch -1 tst: l 4.675 a 0.104  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.405sec\n",
      "Epoch 0 tr: l 4.631 a 0.113  clct[np.int64(11), np.int64(9)] ans[np.int64(10), np.int64(10)] lr 0.250000 18.748sec(train) 3.328sec(infer)\n",
      "Epoch 0 tst: l 4.639 a 0.118  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.691sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 1 tr: l 4.656 a 0.114  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.247500 17.437sec(train) 3.214sec(infer)\n",
      "Epoch 1 tst: l 4.630 a 0.121  clct[np.int64(21), np.int64(19)] ans[np.int64(20), np.int64(20)] 5.945sec\n",
      "Epoch 2 tr: l 4.575 a 0.141  clct[np.int64(13), np.int64(7)] ans[np.int64(7), np.int64(13)] lr 0.245025 17.690sec(train) 3.159sec(infer)\n",
      "Epoch 2 tst: l 4.608 a 0.127  clct[np.int64(23), np.int64(17)] ans[np.int64(20), np.int64(20)] 6.218sec\n",
      "Epoch 3 tr: l 4.565 a 0.138  clct[np.int64(10), np.int64(10)] ans[np.int64(12), np.int64(8)] lr 0.242575 16.218sec(train) 3.020sec(infer)\n",
      "Epoch 3 tst: l 4.580 a 0.135  clct[np.int64(25), np.int64(15)] ans[np.int64(20), np.int64(20)] 5.925sec\n",
      "Epoch 4 tr: l 4.526 a 0.150  clct[np.int64(11), np.int64(9)] ans[np.int64(13), np.int64(7)] lr 0.240149 16.682sec(train) 3.033sec(infer)\n",
      "Epoch 4 tst: l 4.557 a 0.140  clct[np.int64(25), np.int64(15)] ans[np.int64(20), np.int64(20)] 6.011sec\n",
      "Epoch 5 tr: l 4.520 a 0.167  clct[np.int64(16), np.int64(4)] ans[np.int64(8), np.int64(12)] lr 0.237748 16.736sec(train) 3.017sec(infer)\n",
      "Epoch 5 tst: l 4.529 a 0.145  clct[np.int64(26), np.int64(14)] ans[np.int64(20), np.int64(20)] 5.947sec\n",
      "Epoch 6 tr: l 4.478 a 0.156  clct[np.int64(11), np.int64(9)] ans[np.int64(10), np.int64(10)] lr 0.235370 15.539sec(train) 3.010sec(infer)\n",
      "Epoch 6 tst: l 4.511 a 0.149  clct[np.int64(27), np.int64(13)] ans[np.int64(20), np.int64(20)] 5.932sec\n",
      "Epoch 7 tr: l 4.342 a 0.193  clct[np.int64(16), np.int64(4)] ans[np.int64(7), np.int64(13)] lr 0.233016 16.596sec(train) 3.050sec(infer)\n",
      "Epoch 7 tst: l 4.480 a 0.158  clct[np.int64(28), np.int64(12)] ans[np.int64(20), np.int64(20)] 5.994sec\n",
      "Epoch 8 tr: l 4.492 a 0.149  clct[np.int64(12), np.int64(8)] ans[np.int64(11), np.int64(9)] lr 0.230686 15.563sec(train) 3.104sec(infer)\n",
      "Epoch 8 tst: l 4.452 a 0.163  clct[np.int64(28), np.int64(12)] ans[np.int64(20), np.int64(20)] 6.055sec\n",
      "Epoch 9 tr: l 4.393 a 0.181  clct[np.int64(16), np.int64(4)] ans[np.int64(10), np.int64(10)] lr 0.228379 16.498sec(train) 3.054sec(infer)\n",
      "Epoch 9 tst: l 4.360 a 0.191  clct[np.int64(32), np.int64(8)] ans[np.int64(20), np.int64(20)] 6.059sec\n",
      "Epoch 10 tr: l 4.279 a 0.207  clct[np.int64(18), np.int64(2)] ans[np.int64(8), np.int64(12)] lr 0.226096 15.701sec(train) 3.041sec(infer)\n",
      "Epoch 10 tst: l 4.314 a 0.200  clct[np.int64(35), np.int64(5)] ans[np.int64(20), np.int64(20)] 5.964sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 11 tr: l 4.205 a 0.230  clct[np.int64(17), np.int64(3)] ans[np.int64(11), np.int64(9)] lr 0.223835 15.206sec(train) 3.073sec(infer)\n",
      "Epoch 11 tst: l 4.288 a 0.207  clct[np.int64(35), np.int64(5)] ans[np.int64(20), np.int64(20)] 5.977sec\n",
      "Epoch 12 tr: l 4.236 a 0.206  clct[np.int64(15), np.int64(5)] ans[np.int64(8), np.int64(12)] lr 0.221596 15.480sec(train) 3.076sec(infer)\n",
      "Epoch 12 tst: l 4.230 a 0.222  clct[np.int64(36), np.int64(4)] ans[np.int64(20), np.int64(20)] 5.978sec\n",
      "Epoch 13 tr: l 4.199 a 0.214  clct[np.int64(15), np.int64(5)] ans[np.int64(8), np.int64(12)] lr 0.219380 15.838sec(train) 3.058sec(infer)\n",
      "Epoch 13 tst: l 4.203 a 0.226  clct[np.int64(36), np.int64(4)] ans[np.int64(20), np.int64(20)] 5.995sec\n",
      "Epoch 14 tr: l 4.176 a 0.223  clct[np.int64(18), np.int64(2)] ans[np.int64(11), np.int64(9)] lr 0.217186 15.821sec(train) 3.054sec(infer)\n",
      "Epoch 14 tst: l 4.122 a 0.240  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 6.031sec\n",
      "Epoch 15 tr: l 4.199 a 0.219  clct[np.int64(19), np.int64(1)] ans[np.int64(9), np.int64(11)] lr 0.215015 15.600sec(train) 3.081sec(infer)\n",
      "Epoch 15 tst: l 4.095 a 0.242  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.956sec\n",
      "Epoch 16 tr: l 4.179 a 0.203  clct[np.int64(18), np.int64(2)] ans[np.int64(13), np.int64(7)] lr 0.212864 14.857sec(train) 3.032sec(infer)\n",
      "Epoch 16 tst: l 4.068 a 0.246  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 6.042sec\n",
      "Epoch 17 tr: l 4.138 a 0.214  clct[np.int64(18), np.int64(2)] ans[np.int64(9), np.int64(11)] lr 0.210736 16.012sec(train) 3.019sec(infer)\n",
      "Epoch 17 tst: l 4.023 a 0.255  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 6.005sec\n",
      "Epoch 18 tr: l 4.065 a 0.239  clct[np.int64(17), np.int64(3)] ans[np.int64(11), np.int64(9)] lr 0.208628 15.258sec(train) 3.038sec(infer)\n",
      "Epoch 18 tst: l 4.001 a 0.259  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 5.937sec\n",
      "Epoch 19 tr: l 4.003 a 0.238  clct[np.int64(18), np.int64(2)] ans[np.int64(13), np.int64(7)] lr 0.206542 15.487sec(train) 3.033sec(infer)\n",
      "Epoch 19 tst: l 3.944 a 0.264  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 5.991sec\n",
      "Epoch 20 tr: l 4.046 a 0.233  clct[np.int64(19), np.int64(1)] ans[np.int64(9), np.int64(11)] lr 0.204477 15.150sec(train) 3.052sec(infer)\n",
      "Epoch 20 tst: l 3.913 a 0.270  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 6.016sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 21 tr: l 3.980 a 0.244  clct[np.int64(18), np.int64(2)] ans[np.int64(11), np.int64(9)] lr 0.202432 14.980sec(train) 3.039sec(infer)\n",
      "Epoch 21 tst: l 3.907 a 0.267  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 5.975sec\n",
      "Epoch 22 tr: l 3.911 a 0.263  clct[np.int64(18), np.int64(2)] ans[np.int64(10), np.int64(10)] lr 0.200408 15.248sec(train) 3.034sec(infer)\n",
      "Epoch 22 tst: l 3.875 a 0.275  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 5.931sec\n",
      "Epoch 23 tr: l 3.858 a 0.264  clct[np.int64(20), np.int64(0)] ans[np.int64(13), np.int64(7)] lr 0.198404 15.261sec(train) 3.040sec(infer)\n",
      "Epoch 23 tst: l 3.834 a 0.283  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 5.953sec\n",
      "Epoch 24 tr: l 3.832 a 0.271  clct[np.int64(20), np.int64(0)] ans[np.int64(10), np.int64(10)] lr 0.196420 14.980sec(train) 3.115sec(infer)\n",
      "Epoch 24 tst: l 3.823 a 0.274  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 5.946sec\n",
      "Epoch 25 tr: l 3.718 a 0.312  clct[np.int64(20), np.int64(0)] ans[np.int64(7), np.int64(13)] lr 0.194455 14.790sec(train) 3.029sec(infer)\n",
      "Epoch 25 tst: l 3.811 a 0.268  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 5.976sec\n",
      "Epoch 26 tr: l 3.765 a 0.293  clct[np.int64(18), np.int64(2)] ans[np.int64(12), np.int64(8)] lr 0.192511 14.876sec(train) 3.034sec(infer)\n",
      "Epoch 26 tst: l 3.812 a 0.270  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 6.027sec\n",
      "Epoch 27 tr: l 3.774 a 0.283  clct[np.int64(19), np.int64(1)] ans[np.int64(14), np.int64(6)] lr 0.190586 15.853sec(train) 3.083sec(infer)\n",
      "Epoch 27 tst: l 3.782 a 0.273  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.967sec\n",
      "Epoch 28 tr: l 3.766 a 0.285  clct[np.int64(20), np.int64(0)] ans[np.int64(8), np.int64(12)] lr 0.188680 15.096sec(train) 3.031sec(infer)\n",
      "Epoch 28 tst: l 3.750 a 0.277  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.687sec\n",
      "Epoch 29 tr: l 3.716 a 0.279  clct[np.int64(19), np.int64(1)] ans[np.int64(11), np.int64(9)] lr 0.186793 14.442sec(train) 2.845sec(infer)\n",
      "Epoch 29 tst: l 3.753 a 0.273  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.633sec\n",
      "Epoch 30 tr: l 3.783 a 0.274  clct[np.int64(18), np.int64(2)] ans[np.int64(10), np.int64(10)] lr 0.184925 14.270sec(train) 2.835sec(infer)\n",
      "Epoch 30 tst: l 3.725 a 0.276  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.719sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 31 tr: l 3.773 a 0.290  clct[np.int64(20), np.int64(0)] ans[np.int64(8), np.int64(12)] lr 0.183076 14.653sec(train) 2.913sec(infer)\n",
      "Epoch 31 tst: l 3.712 a 0.280  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.620sec\n",
      "Epoch 32 tr: l 3.569 a 0.326  clct[np.int64(19), np.int64(1)] ans[np.int64(8), np.int64(12)] lr 0.181245 14.417sec(train) 2.864sec(infer)\n",
      "Epoch 32 tst: l 3.703 a 0.280  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.567sec\n",
      "Epoch 33 tr: l 3.728 a 0.288  clct[np.int64(18), np.int64(2)] ans[np.int64(10), np.int64(10)] lr 0.179433 14.299sec(train) 2.899sec(infer)\n",
      "Epoch 33 tst: l 3.699 a 0.281  clct[np.int64(36), np.int64(4)] ans[np.int64(20), np.int64(20)] 5.631sec\n",
      "Epoch 34 tr: l 3.594 a 0.307  clct[np.int64(20), np.int64(0)] ans[np.int64(8), np.int64(12)] lr 0.177638 14.634sec(train) 2.876sec(infer)\n",
      "Epoch 34 tst: l 3.670 a 0.287  clct[np.int64(36), np.int64(4)] ans[np.int64(20), np.int64(20)] 5.717sec\n",
      "Epoch 35 tr: l 3.531 a 0.332  clct[np.int64(20), np.int64(0)] ans[np.int64(9), np.int64(11)] lr 0.175862 14.208sec(train) 2.849sec(infer)\n",
      "Epoch 35 tst: l 3.647 a 0.287  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.606sec\n",
      "Epoch 36 tr: l 3.516 a 0.326  clct[np.int64(20), np.int64(0)] ans[np.int64(10), np.int64(10)] lr 0.174103 14.227sec(train) 2.877sec(infer)\n",
      "Epoch 36 tst: l 3.641 a 0.278  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.660sec\n",
      "Epoch 37 tr: l 3.506 a 0.327  clct[np.int64(19), np.int64(1)] ans[np.int64(9), np.int64(11)] lr 0.172362 14.521sec(train) 2.922sec(infer)\n",
      "Epoch 37 tst: l 3.632 a 0.272  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.755sec\n",
      "Epoch 38 tr: l 3.500 a 0.333  clct[np.int64(19), np.int64(1)] ans[np.int64(9), np.int64(11)] lr 0.170639 14.383sec(train) 2.911sec(infer)\n",
      "Epoch 38 tst: l 3.631 a 0.270  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.712sec\n",
      "Epoch 39 tr: l 3.419 a 0.347  clct[np.int64(20), np.int64(0)] ans[np.int64(7), np.int64(13)] lr 0.168932 14.229sec(train) 2.871sec(infer)\n",
      "Epoch 39 tst: l 3.633 a 0.268  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.617sec\n",
      "Epoch 40 tr: l 3.545 a 0.326  clct[np.int64(19), np.int64(1)] ans[np.int64(9), np.int64(11)] lr 0.167243 14.326sec(train) 2.844sec(infer)\n",
      "Epoch 40 tst: l 3.627 a 0.266  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.625sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 41 tr: l 3.365 a 0.337  clct[np.int64(20), np.int64(0)] ans[np.int64(8), np.int64(12)] lr 0.165571 14.226sec(train) 2.890sec(infer)\n",
      "Epoch 41 tst: l 3.617 a 0.267  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.636sec\n",
      "Epoch 42 tr: l 3.440 a 0.319  clct[np.int64(20), np.int64(0)] ans[np.int64(7), np.int64(13)] lr 0.163915 14.407sec(train) 3.122sec(infer)\n",
      "Epoch 42 tst: l 3.608 a 0.271  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.857sec\n",
      "Epoch 43 tr: l 3.281 a 0.360  clct[np.int64(20), np.int64(0)] ans[np.int64(7), np.int64(13)] lr 0.162276 14.161sec(train) 2.854sec(infer)\n",
      "Epoch 43 tst: l 3.593 a 0.276  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.594sec\n",
      "Epoch 44 tr: l 3.315 a 0.355  clct[np.int64(20), np.int64(0)] ans[np.int64(11), np.int64(9)] lr 0.160653 14.073sec(train) 2.870sec(infer)\n",
      "Epoch 44 tst: l 3.579 a 0.281  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.711sec\n",
      "Epoch 45 tr: l 3.399 a 0.348  clct[np.int64(19), np.int64(1)] ans[np.int64(8), np.int64(12)] lr 0.159046 14.342sec(train) 2.895sec(infer)\n",
      "Epoch 45 tst: l 3.570 a 0.286  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.629sec\n",
      "Epoch 46 tr: l 3.213 a 0.399  clct[np.int64(19), np.int64(1)] ans[np.int64(9), np.int64(11)] lr 0.157456 13.979sec(train) 2.910sec(infer)\n",
      "Epoch 46 tst: l 3.569 a 0.284  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.697sec\n",
      "Epoch 47 tr: l 3.201 a 0.384  clct[np.int64(20), np.int64(0)] ans[np.int64(6), np.int64(14)] lr 0.155881 13.978sec(train) 2.870sec(infer)\n",
      "Epoch 47 tst: l 3.560 a 0.285  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.616sec\n",
      "Epoch 48 tr: l 3.239 a 0.375  clct[np.int64(20), np.int64(0)] ans[np.int64(8), np.int64(12)] lr 0.154323 14.153sec(train) 2.851sec(infer)\n",
      "Epoch 48 tst: l 3.550 a 0.284  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.628sec\n",
      "Epoch 49 tr: l 3.269 a 0.370  clct[np.int64(19), np.int64(1)] ans[np.int64(9), np.int64(11)] lr 0.152779 13.958sec(train) 2.862sec(infer)\n",
      "Epoch 49 tst: l 3.537 a 0.286  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.664sec\n",
      "Epoch 50 tr: l 3.236 a 0.390  clct[np.int64(19), np.int64(1)] ans[np.int64(4), np.int64(16)] lr 0.151252 14.058sec(train) 2.884sec(infer)\n",
      "Epoch 50 tst: l 3.523 a 0.284  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.632sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 51 tr: l 3.327 a 0.353  clct[np.int64(20), np.int64(0)] ans[np.int64(5), np.int64(15)] lr 0.149739 14.112sec(train) 2.916sec(infer)\n",
      "Epoch 51 tst: l 3.535 a 0.281  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.653sec\n",
      "Epoch 52 tr: l 3.303 a 0.371  clct[np.int64(20), np.int64(0)] ans[np.int64(11), np.int64(9)] lr 0.148242 14.024sec(train) 2.905sec(infer)\n",
      "Epoch 52 tst: l 3.514 a 0.285  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.631sec\n",
      "Epoch 53 tr: l 3.137 a 0.409  clct[np.int64(20), np.int64(0)] ans[np.int64(11), np.int64(9)] lr 0.146759 14.014sec(train) 2.865sec(infer)\n",
      "Epoch 53 tst: l 3.522 a 0.281  clct[np.int64(36), np.int64(4)] ans[np.int64(20), np.int64(20)] 5.635sec\n",
      "Epoch 54 tr: l 2.994 a 0.424  clct[np.int64(20), np.int64(0)] ans[np.int64(11), np.int64(9)] lr 0.145292 13.997sec(train) 2.863sec(infer)\n",
      "Epoch 54 tst: l 3.527 a 0.280  clct[np.int64(36), np.int64(4)] ans[np.int64(20), np.int64(20)] 5.616sec\n",
      "Epoch 55 tr: l 3.122 a 0.401  clct[np.int64(20), np.int64(0)] ans[np.int64(10), np.int64(10)] lr 0.143839 14.063sec(train) 2.963sec(infer)\n",
      "Epoch 55 tst: l 3.534 a 0.281  clct[np.int64(36), np.int64(4)] ans[np.int64(20), np.int64(20)] 5.747sec\n",
      "Epoch 56 tr: l 3.151 a 0.405  clct[np.int64(19), np.int64(1)] ans[np.int64(11), np.int64(9)] lr 0.142400 14.106sec(train) 2.853sec(infer)\n",
      "Epoch 56 tst: l 3.527 a 0.285  clct[np.int64(36), np.int64(4)] ans[np.int64(20), np.int64(20)] 5.613sec\n",
      "Epoch 57 tr: l 3.085 a 0.411  clct[np.int64(20), np.int64(0)] ans[np.int64(7), np.int64(13)] lr 0.140976 14.081sec(train) 2.850sec(infer)\n",
      "Epoch 57 tst: l 3.473 a 0.294  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.606sec\n",
      "Epoch 58 tr: l 3.138 a 0.407  clct[np.int64(20), np.int64(0)] ans[np.int64(11), np.int64(9)] lr 0.139567 14.072sec(train) 2.944sec(infer)\n",
      "Epoch 58 tst: l 3.467 a 0.295  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.637sec\n",
      "Epoch 59 tr: l 3.198 a 0.383  clct[np.int64(20), np.int64(0)] ans[np.int64(9), np.int64(11)] lr 0.138171 14.036sec(train) 2.856sec(infer)\n",
      "Epoch 59 tst: l 3.458 a 0.295  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.612sec\n",
      "Epoch 60 tr: l 3.031 a 0.439  clct[np.int64(20), np.int64(0)] ans[np.int64(8), np.int64(12)] lr 0.136789 13.989sec(train) 2.889sec(infer)\n",
      "Epoch 60 tst: l 3.458 a 0.295  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.593sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 61 tr: l 2.915 a 0.457  clct[np.int64(18), np.int64(2)] ans[np.int64(12), np.int64(8)] lr 0.135421 14.001sec(train) 2.844sec(infer)\n",
      "Epoch 61 tst: l 3.471 a 0.294  clct[np.int64(37), np.int64(3)] ans[np.int64(20), np.int64(20)] 5.641sec\n",
      "Epoch 62 tr: l 2.949 a 0.438  clct[np.int64(20), np.int64(0)] ans[np.int64(9), np.int64(11)] lr 0.134067 14.377sec(train) 2.878sec(infer)\n",
      "Epoch 62 tst: l 3.377 a 0.310  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.744sec\n",
      "Epoch 63 tr: l 2.931 a 0.462  clct[np.int64(20), np.int64(0)] ans[np.int64(8), np.int64(12)] lr 0.132726 14.315sec(train) 2.860sec(infer)\n",
      "Epoch 63 tst: l 3.373 a 0.310  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.630sec\n",
      "Epoch 64 tr: l 3.074 a 0.406  clct[np.int64(20), np.int64(0)] ans[np.int64(7), np.int64(13)] lr 0.131399 14.299sec(train) 2.879sec(infer)\n",
      "Epoch 64 tst: l 3.387 a 0.309  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.584sec\n",
      "Epoch 65 tr: l 2.830 a 0.485  clct[np.int64(20), np.int64(0)] ans[np.int64(8), np.int64(12)] lr 0.130085 14.330sec(train) 2.828sec(infer)\n",
      "Epoch 65 tst: l 3.365 a 0.311  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.584sec\n",
      "Epoch 66 tr: l 2.885 a 0.462  clct[np.int64(20), np.int64(0)] ans[np.int64(10), np.int64(10)] lr 0.128784 14.238sec(train) 2.862sec(infer)\n",
      "Epoch 66 tst: l 3.360 a 0.312  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.726sec\n",
      "Epoch 67 tr: l 3.008 a 0.426  clct[np.int64(20), np.int64(0)] ans[np.int64(10), np.int64(10)] lr 0.127496 14.606sec(train) 2.885sec(infer)\n",
      "Epoch 67 tst: l 3.353 a 0.312  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.596sec\n",
      "Epoch 68 tr: l 2.963 a 0.441  clct[np.int64(20), np.int64(0)] ans[np.int64(8), np.int64(12)] lr 0.126221 14.278sec(train) 2.802sec(infer)\n",
      "Epoch 68 tst: l 3.361 a 0.310  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.615sec\n",
      "Epoch 69 tr: l 2.717 a 0.484  clct[np.int64(20), np.int64(0)] ans[np.int64(11), np.int64(9)] lr 0.124959 14.321sec(train) 2.854sec(infer)\n",
      "Epoch 69 tst: l 3.362 a 0.309  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.608sec\n",
      "Epoch 70 tr: l 2.845 a 0.471  clct[np.int64(19), np.int64(1)] ans[np.int64(11), np.int64(9)] lr 0.123710 14.251sec(train) 2.920sec(infer)\n",
      "Epoch 70 tst: l 3.364 a 0.307  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.613sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 71 tr: l 2.865 a 0.477  clct[np.int64(20), np.int64(0)] ans[np.int64(7), np.int64(13)] lr 0.122473 14.332sec(train) 2.863sec(infer)\n",
      "Epoch 71 tst: l 3.363 a 0.307  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.639sec\n",
      "Epoch 72 tr: l 2.747 a 0.498  clct[np.int64(20), np.int64(0)] ans[np.int64(7), np.int64(13)] lr 0.121248 14.444sec(train) 2.868sec(infer)\n",
      "Epoch 72 tst: l 3.358 a 0.310  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.591sec\n",
      "Epoch 73 tr: l 2.914 a 0.440  clct[np.int64(20), np.int64(0)] ans[np.int64(10), np.int64(10)] lr 0.120035 14.200sec(train) 2.839sec(infer)\n",
      "Epoch 73 tst: l 3.367 a 0.309  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.612sec\n",
      "Epoch 74 tr: l 2.881 a 0.461  clct[np.int64(20), np.int64(0)] ans[np.int64(9), np.int64(11)] lr 0.118835 14.196sec(train) 2.831sec(infer)\n",
      "Epoch 74 tst: l 3.374 a 0.308  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.652sec\n",
      "Epoch 75 tr: l 3.043 a 0.435  clct[np.int64(19), np.int64(1)] ans[np.int64(7), np.int64(13)] lr 0.117647 14.239sec(train) 2.901sec(infer)\n",
      "Epoch 75 tst: l 3.376 a 0.308  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.587sec\n",
      "Epoch 76 tr: l 2.719 a 0.500  clct[np.int64(20), np.int64(0)] ans[np.int64(12), np.int64(8)] lr 0.116470 14.311sec(train) 2.855sec(infer)\n",
      "Epoch 76 tst: l 3.376 a 0.310  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.580sec\n",
      "Epoch 77 tr: l 2.646 a 0.527  clct[np.int64(20), np.int64(0)] ans[np.int64(10), np.int64(10)] lr 0.115305 14.346sec(train) 2.870sec(infer)\n",
      "Epoch 77 tst: l 3.376 a 0.309  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.594sec\n",
      "Epoch 78 tr: l 2.782 a 0.487  clct[np.int64(19), np.int64(1)] ans[np.int64(11), np.int64(9)] lr 0.114152 14.226sec(train) 2.820sec(infer)\n",
      "Epoch 78 tst: l 3.376 a 0.309  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.711sec\n",
      "Epoch 79 tr: l 2.669 a 0.513  clct[np.int64(20), np.int64(0)] ans[np.int64(9), np.int64(11)] lr 0.113011 14.272sec(train) 2.841sec(infer)\n",
      "Epoch 79 tst: l 3.379 a 0.307  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.552sec\n",
      "Epoch 80 tr: l 2.685 a 0.507  clct[np.int64(20), np.int64(0)] ans[np.int64(11), np.int64(9)] lr 0.111881 14.272sec(train) 2.815sec(infer)\n",
      "Epoch 80 tst: l 3.373 a 0.306  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 5.581sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 81 tr: l 2.662 a 0.516  clct[np.int64(20), np.int64(0)] ans[np.int64(10), np.int64(10)] lr 0.110762 14.258sec(train) 2.854sec(infer)\n",
      "Epoch 81 tst: l 3.362 a 0.308  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 5.645sec\n",
      "Epoch 82 tr: l 2.720 a 0.491  clct[np.int64(20), np.int64(0)] ans[np.int64(11), np.int64(9)] lr 0.109654 14.178sec(train) 2.813sec(infer)\n",
      "Epoch 82 tst: l 3.351 a 0.311  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 5.606sec\n",
      "Epoch 83 tr: l 2.647 a 0.528  clct[np.int64(20), np.int64(0)] ans[np.int64(8), np.int64(12)] lr 0.108558 13.975sec(train) 2.846sec(infer)\n",
      "Epoch 83 tst: l 3.359 a 0.312  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 5.568sec\n",
      "Epoch 84 tr: l 2.580 a 0.526  clct[np.int64(20), np.int64(0)] ans[np.int64(10), np.int64(10)] lr 0.107472 13.979sec(train) 2.823sec(infer)\n",
      "Epoch 84 tst: l 3.356 a 0.313  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 5.551sec\n",
      "Epoch 85 tr: l 2.681 a 0.514  clct[np.int64(20), np.int64(0)] ans[np.int64(10), np.int64(10)] lr 0.106398 13.922sec(train) 2.809sec(infer)\n",
      "Epoch 85 tst: l 3.351 a 0.315  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 5.601sec\n",
      "Epoch 86 tr: l 2.412 a 0.581  clct[np.int64(20), np.int64(0)] ans[np.int64(12), np.int64(8)] lr 0.105334 13.906sec(train) 2.811sec(infer)\n",
      "Epoch 86 tst: l 3.339 a 0.316  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 5.536sec\n",
      "Epoch 87 tr: l 2.507 a 0.561  clct[np.int64(20), np.int64(0)] ans[np.int64(8), np.int64(12)] lr 0.104280 13.911sec(train) 2.842sec(infer)\n",
      "Epoch 87 tst: l 3.343 a 0.317  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 5.551sec\n",
      "Epoch 88 tr: l 2.428 a 0.587  clct[np.int64(19), np.int64(1)] ans[np.int64(11), np.int64(9)] lr 0.103237 14.049sec(train) 2.827sec(infer)\n",
      "Epoch 88 tst: l 3.355 a 0.316  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 5.613sec\n",
      "Epoch 89 tr: l 2.509 a 0.585  clct[np.int64(20), np.int64(0)] ans[np.int64(6), np.int64(14)] lr 0.102205 13.976sec(train) 2.796sec(infer)\n",
      "Epoch 89 tst: l 3.379 a 0.313  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 5.555sec\n",
      "Epoch 90 tr: l 2.499 a 0.559  clct[np.int64(19), np.int64(1)] ans[np.int64(5), np.int64(15)] lr 0.101183 13.929sec(train) 2.812sec(infer)\n",
      "Epoch 90 tst: l 3.387 a 0.311  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 5.542sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 91 tr: l 2.678 a 0.514  clct[np.int64(20), np.int64(0)] ans[np.int64(12), np.int64(8)] lr 0.100171 13.997sec(train) 2.813sec(infer)\n",
      "Epoch 91 tst: l 3.378 a 0.310  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 5.564sec\n",
      "Epoch 92 tr: l 2.375 a 0.593  clct[np.int64(20), np.int64(0)] ans[np.int64(5), np.int64(15)] lr 0.099169 13.954sec(train) 2.840sec(infer)\n",
      "Epoch 92 tst: l 3.361 a 0.311  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 5.600sec\n",
      "Epoch 93 tr: l 2.328 a 0.622  clct[np.int64(20), np.int64(0)] ans[np.int64(7), np.int64(13)] lr 0.098178 13.854sec(train) 2.813sec(infer)\n",
      "Epoch 93 tst: l 3.357 a 0.313  clct[np.int64(39), np.int64(1)] ans[np.int64(20), np.int64(20)] 5.534sec\n",
      "Epoch 94 tr: l 2.513 a 0.553  clct[np.int64(20), np.int64(0)] ans[np.int64(9), np.int64(11)] lr 0.097196 13.948sec(train) 2.853sec(infer)\n",
      "Epoch 94 tst: l 3.362 a 0.313  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.536sec\n",
      "Epoch 95 tr: l 2.439 a 0.587  clct[np.int64(20), np.int64(0)] ans[np.int64(12), np.int64(8)] lr 0.096224 13.836sec(train) 2.829sec(infer)\n",
      "Epoch 95 tst: l 3.377 a 0.311  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.564sec\n",
      "Epoch 96 tr: l 2.504 a 0.550  clct[np.int64(20), np.int64(0)] ans[np.int64(10), np.int64(10)] lr 0.095262 13.881sec(train) 2.834sec(infer)\n",
      "Epoch 96 tst: l 3.374 a 0.311  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.549sec\n",
      "Epoch 97 tr: l 2.470 a 0.561  clct[np.int64(20), np.int64(0)] ans[np.int64(10), np.int64(10)] lr 0.094309 13.721sec(train) 2.845sec(infer)\n",
      "Epoch 97 tst: l 3.366 a 0.315  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.515sec\n",
      "Epoch 98 tr: l 2.505 a 0.566  clct[np.int64(20), np.int64(0)] ans[np.int64(7), np.int64(13)] lr 0.093366 13.810sec(train) 2.844sec(infer)\n",
      "Epoch 98 tst: l 3.362 a 0.315  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.570sec\n",
      "Epoch 99 tr: l 2.422 a 0.591  clct[np.int64(20), np.int64(0)] ans[np.int64(11), np.int64(9)] lr 0.092432 13.726sec(train) 2.819sec(infer)\n",
      "Epoch 99 tst: l 3.359 a 0.316  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.557sec\n",
      "Epoch 100 tr: l 2.463 a 0.572  clct[np.int64(20), np.int64(0)] ans[np.int64(9), np.int64(11)] lr 0.091508 13.777sec(train) 2.851sec(infer)\n",
      "Epoch 100 tst: l 3.369 a 0.314  clct[np.int64(38), np.int64(2)] ans[np.int64(20), np.int64(20)] 5.551sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 101 tr: l 2.288 a 0.626  clct[np.int64(20), np.int64(0)] ans[np.int64(10), np.int64(10)] lr 0.090593 15.867sec(train) 3.133sec(infer)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m exp \u001b[38;5;241m=\u001b[39m TrainCIFARCluster(config)\n\u001b[0;32m      6\u001b[0m exp\u001b[38;5;241m.\u001b[39msetup()\n\u001b[1;32m----> 7\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 290\u001b[0m, in \u001b[0;36mTrainCIFARCluster.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_epoch_stats(res)\n\u001b[0;32m    289\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 290\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    292\u001b[0m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfer_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m t1\u001b[38;5;241m-\u001b[39mt0\n",
      "Cell \u001b[1;32mIn[4], line 566\u001b[0m, in \u001b[0;36mTrainCIFARCluster.test\u001b[1;34m(self, train, force_full_nodes)\u001b[0m\n\u001b[0;32m    563\u001b[0m t_load_data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m t01\u001b[38;5;241m-\u001b[39mt00\n\u001b[0;32m    565\u001b[0m ti0\u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 566\u001b[0m (loss, correct) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_correct\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_pl\u001b[49m\u001b[43m:\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_pl\u001b[49m\u001b[43m:\u001b[49m\u001b[43my\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m ti1\u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    568\u001b[0m t_infer \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ti1\u001b[38;5;241m-\u001b[39mti0\n",
      "File \u001b[1;32me:\\anaconda\\envs\\dl-new\\lib\\site-packages\\tensorflow\\python\\client\\session.py:977\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    974\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 977\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    979\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m    980\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\dl-new\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1220\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[1;32m-> 1220\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1223\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32me:\\anaconda\\envs\\dl-new\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1400\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1397\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1400\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1401\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1403\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\dl-new\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1407\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m   1406\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1408\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1409\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\dl-new\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1390\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[0;32m   1388\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[0;32m   1389\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[1;32m-> 1390\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\anaconda\\envs\\dl-new\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1483\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1482\u001b[0m                         run_metadata):\n\u001b[1;32m-> 1483\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1485\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "config['train_seed'] = config['data_seed']\n",
    "print(\"config:\",config)\n",
    "\n",
    "exp = TrainCIFARCluster(config)\n",
    "exp.setup()\n",
    "exp.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
