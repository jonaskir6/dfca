{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f53520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from util import *\n",
    "import cifar10\n",
    "\n",
    "\n",
    "\n",
    "LR_DECAY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c81b2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "\n",
    "    # read config json and update the sysarg\n",
    "    with open(\"config.json\", \"r\") as read_file:\n",
    "        config = json.load(read_file)\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48588e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 24\n",
    "\n",
    "def train_transform(reshaped_image):\n",
    "    # copied from cifar10_input.py / distorted_input()\n",
    "\n",
    "    height = IMAGE_SIZE\n",
    "    width = IMAGE_SIZE\n",
    "\n",
    "    # Image processing for training the network. Note the many random\n",
    "    # distortions applied to the image.\n",
    "\n",
    "    # Randomly crop a [height, width] section of the image.\n",
    "    distorted_image = tf.random_crop(reshaped_image, [tf.shape(reshaped_image)[0], height, width, 3])\n",
    "    # tf shape gives dynamic shape\n",
    "\n",
    "    # Randomly flip the image horizontally.\n",
    "    distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "\n",
    "    # Because these operations are not commutative, consider randomizing\n",
    "    # the order their operation.\n",
    "    distorted_image = tf.image.random_brightness(distorted_image,\n",
    "                                               max_delta=63)\n",
    "    distorted_image = tf.image.random_contrast(distorted_image,\n",
    "                                             lower=0.2, upper=1.8)\n",
    "\n",
    "    # Subtract off the mean and divide by the variance of the pixels.\n",
    "    float_image = tf.image.per_image_standardization(distorted_image)\n",
    "\n",
    "    return float_image\n",
    "\n",
    "def test_transform(reshaped_image):\n",
    "    # copied from cifar10_input.py / input()\n",
    "\n",
    "    height = IMAGE_SIZE\n",
    "    width = IMAGE_SIZE\n",
    "\n",
    "    # Image processing for evaluation.\n",
    "    # Crop the central [height, width] of the image.\n",
    "    resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image,\n",
    "                                                         width, height)\n",
    "\n",
    "    # Subtract off the mean and divide by the variance of the pixels.\n",
    "    float_image = tf.image.per_image_standardization(resized_image)\n",
    "\n",
    "    return float_image\n",
    "\n",
    "\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    \n",
    "def create_batches(pmt, batch_size):\n",
    "    batch_indices = []\n",
    "    ct = 0\n",
    "    for b_i in range(int(np.ceil( len(pmt) / batch_size))):\n",
    "        if ct + batch_size > len(pmt):\n",
    "            batch = pmt[ct : len(pmt)]\n",
    "            ct = len(pmt)\n",
    "        else:\n",
    "            batch = pmt[ct : ct + batch_size]\n",
    "            ct += batch_size\n",
    "        batch_indices.append(batch)\n",
    "\n",
    "    return batch_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490bb8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainCIFARCluster(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        assert self.config['m'] % self.config['p'] == 0\n",
    "\n",
    "    def setup(self):\n",
    "\n",
    "        os.makedirs(self.config['project_dir'], exist_ok = True)\n",
    "\n",
    "        self.result_fname = os.path.join(self.config['project_dir'], 'results')\n",
    "        self.checkpoint_fname = os.path.join(self.config['project_dir'], 'checkpoint')\n",
    "\n",
    "        set_random_seed(self.config['data_seed'])\n",
    "        self.setup_datasets()\n",
    "        self.setup_model()\n",
    "\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config)\n",
    "\n",
    "\n",
    "        set_random_seed(self.config['data_seed']+self.config['train_seed'])\n",
    "        self.initialize_models()\n",
    "        self.initialize_assign_ops()\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        self.epoch = None\n",
    "        self.lr = None\n",
    "\n",
    "\n",
    "    def setup_datasets(self):\n",
    "        # tf.enable_eager_execution()\n",
    "\n",
    "        # generate indices for each dataset\n",
    "        # also write cluster info\n",
    "\n",
    "        CIFAR10_TRAINSET_DATA_SIZE = 50000\n",
    "        CIFAR10_TESTSET_DATA_SIZE = 10000\n",
    "\n",
    "        cfg = self.config\n",
    "\n",
    "        self.dataset = {}\n",
    "\n",
    "        dataset = {}\n",
    "        dataset['data_indices'], dataset['cluster_assign'] = \\\n",
    "            self._setup_dataset(CIFAR10_TRAINSET_DATA_SIZE, cfg['p'], cfg['m'], cfg['n'])\n",
    "        dl = self._load_CIFAR(train=True)\n",
    "        dataset['data_loader'] = dl\n",
    "        self.dataset['train'] = dataset\n",
    "\n",
    "        dataset = {}\n",
    "        dataset['data_indices'], dataset['cluster_assign'] = \\\n",
    "            self._setup_dataset(CIFAR10_TESTSET_DATA_SIZE, cfg['p'], cfg['m_test'], cfg['n'], random=False)\n",
    "        dl = self._load_CIFAR(train=False)\n",
    "        dataset['data_loader'] = dl\n",
    "        self.dataset['test'] = dataset\n",
    "\n",
    "        # tf.disable_eager_execution()\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "\n",
    "    def _setup_dataset(self, num_data, p, m, n, random = True):\n",
    "\n",
    "        assert (m // p) * n == num_data\n",
    "\n",
    "        dataset = {}\n",
    "\n",
    "        cfg = self.config\n",
    "\n",
    "        data_indices = []\n",
    "        cluster_assign = []\n",
    "\n",
    "        m_per_cluster = m // p\n",
    "\n",
    "        for p_i in range(p):\n",
    "\n",
    "            if random:\n",
    "                ll = list(np.random.permutation(num_data))\n",
    "            else:\n",
    "                ll = list(range(num_data))\n",
    "\n",
    "            ll2 = chunkify(ll, m_per_cluster) # splits ll into m lists with size n\n",
    "            data_indices += ll2\n",
    "\n",
    "            cluster_assign += [p_i for _ in range(m_per_cluster)]\n",
    "\n",
    "        data_indices = np.array(data_indices)\n",
    "        cluster_assign = np.array(cluster_assign)\n",
    "        assert data_indices.shape[0] == cluster_assign.shape[0]\n",
    "        assert data_indices.shape[0] == m\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        return data_indices, cluster_assign\n",
    "\n",
    "\n",
    "    def _load_CIFAR(self, train=True):\n",
    "        # gives dataloader that gives (X,y) based on asked index\n",
    "\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "        # (50000, 32,32, 3) [0~1] , (50000, 1)\n",
    "\n",
    "        if train:\n",
    "            X = x_train / 255.0\n",
    "            y = y_train.reshape(-1)\n",
    "        else:\n",
    "            X = x_test / 255.0\n",
    "            y = y_test.reshape(-1)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    def setup_model(self):\n",
    "\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "\n",
    "        # setup tensorflow model structure\n",
    "\n",
    "        self.x_pl = tf.placeholder(tf.float32, shape=(None, 24, 24, 3), name='input_x')\n",
    "        self.y_pl = tf.placeholder(tf.int32, shape=(None, ), name='output_y')\n",
    "        self.lr_pl = tf.placeholder(tf.float32, shape=(), name='learning_rate')\n",
    "\n",
    "        self.y_logits = cifar10.inference(self.x_pl) # construct model\n",
    "        self.loss = cifar10.loss(self.y_logits, self.y_pl)\n",
    "\n",
    "        self.y_pred = tf.cast(tf.argmax(self.y_logits, 1), tf.int32)\n",
    "        self.correct_prediction = tf.equal(self.y_pred, self.y_pl) # used for accuracy\n",
    "        self.num_correct = tf.reduce_sum(tf.cast(self.correct_prediction, tf.int64))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.lr_pl)\n",
    "        self.train_op = self.optimizer.minimize(self.loss)\n",
    "        self.opt_reset_op = tf.variables_initializer(self.optimizer.variables())\n",
    "\n",
    "        # import ipdb; ipdb.set_trace() # check self.optimizer.variables()\n",
    "\n",
    "        self.metrics = { # used by self.eval()\n",
    "            'loss':self.loss,\n",
    "            'correct': self.num_correct,\n",
    "            # and add more...\n",
    "        }\n",
    "\n",
    "\n",
    "        # transform ops\n",
    "        self.x_tr_pl = tf.placeholder(tf.float32, shape=(None, 32, 32, 3))\n",
    "        # with tf.device('/cpu:0'):\n",
    "        self.train_transform_op = train_transform(self.x_tr_pl)\n",
    "        self.test_transform_op = test_transform(self.x_tr_pl)\n",
    "\n",
    "\n",
    "    def initialize_models(self):\n",
    "\n",
    "        p = self.config['p']\n",
    "\n",
    "        # initialize p times, to get p different sets of weights.\n",
    "\n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "        self.model_weights = []\n",
    "        for p_i in range(p):\n",
    "            self.sess.run(self.init_op)\n",
    "            weights = self.get_model_weights()\n",
    "            self.model_weights.append(weights)\n",
    "\n",
    "    def get_model_weights(self):\n",
    "        self.collection = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "        names = [var.name for var in self.collection]\n",
    "        weights_arrays = self.sess.run(self.collection)\n",
    "\n",
    "        weights = dict(zip(names, weights_arrays))\n",
    "        # {'conv1/weights:0': np.array, ...}\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def initialize_assign_ops(self):\n",
    "        self.collection = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "        assign_ops = {}\n",
    "        assign_pls = {}\n",
    "        for var in self.collection:\n",
    "            # import ipdb; ipdb.set_trace()\n",
    "            pl = tf.placeholder(tf.float32, shape=var.shape)\n",
    "            assign_pls[var.name] = pl\n",
    "\n",
    "            op = tf.compat.v1.assign(var, pl)\n",
    "            assign_ops[var.name] = op\n",
    "\n",
    "\n",
    "        self.assign_ops = assign_ops\n",
    "        self.assign_pls = assign_pls\n",
    "\n",
    "    def put_model_weights(self, weights):\n",
    "\n",
    "        assign_ops = []\n",
    "\n",
    "        fd = {}\n",
    "        for var_name in self.assign_pls:\n",
    "            # assign_op = tf.assign(var, weights[var.name])\n",
    "            pl = self.assign_pls[var_name]\n",
    "            fd[pl] = weights[var_name]\n",
    "\n",
    "        self.sess.run(self.opt_reset_op) # reset the optimizer state ?\n",
    "        self.sess.run(list(self.assign_ops.values()), feed_dict = fd)\n",
    "\n",
    "    def average_model_weights(self, weights_list):\n",
    "\n",
    "        w2 = {}\n",
    "\n",
    "        for key in weights_list[0].keys():\n",
    "\n",
    "            w2[key] = np.mean([w[key] for w in weights_list], axis=0)\n",
    "\n",
    "        return w2\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        TRAIN_INFER_FULL_NODES = 0\n",
    "\n",
    "        num_epochs = self.config['num_epochs']\n",
    "        lr = self.config['lr']\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # epoch -1\n",
    "        self.epoch = -1\n",
    "\n",
    "        self.find_good_initializer()\n",
    "\n",
    "\n",
    "        result = {}\n",
    "        result['epoch'] = -1\n",
    "\n",
    "        t0 = time.time()\n",
    "        self.set_participating_nodes()\n",
    "        res = self.test(train=True, force_full_nodes =TRAIN_INFER_FULL_NODES)\n",
    "        # res = self.test(train=True)\n",
    "        t1 = time.time()\n",
    "        res['infer_time'] = t1-t0\n",
    "        result['train'] = res\n",
    "\n",
    "        self.print_epoch_stats(res)\n",
    "\n",
    "        t0 = time.time()\n",
    "        res = self.test(train=False)\n",
    "        t1 = time.time()\n",
    "        res['infer_time'] = t1-t0\n",
    "        result['test'] = res\n",
    "        self.print_epoch_stats(res)\n",
    "        results.append(result)\n",
    "\n",
    "        # this will be used in next epoch\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            result = {}\n",
    "            result['epoch'] = epoch\n",
    "\n",
    "            lr = self.lr_schedule(epoch)\n",
    "            result['lr'] = lr\n",
    "\n",
    "            t0 = time.time()\n",
    "            result['train'] = self.train(lr = lr)\n",
    "            t1 = time.time()\n",
    "            train_time = t1-t0\n",
    "\n",
    "            t0 = time.time()\n",
    "            self.set_participating_nodes()\n",
    "            # res = self.test(train=True)\n",
    "            res = self.test(train=True, force_full_nodes =TRAIN_INFER_FULL_NODES)\n",
    "            t1 = time.time()\n",
    "            res['infer_time'] = t1-t0\n",
    "            res['train_time'] = train_time\n",
    "            res['lr'] = lr\n",
    "            result['train'] = res\n",
    "\n",
    "            self.print_epoch_stats(res)\n",
    "\n",
    "            t0 = time.time()\n",
    "            res = self.test(train=False)\n",
    "            t1 = time.time()\n",
    "            res['infer_time'] = t1-t0\n",
    "            result['test'] = res\n",
    "            self.print_epoch_stats(res)\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "            if epoch % 10 == 0 or epoch == num_epochs - 1 :\n",
    "                with open(self.result_fname+\".pickle\", 'wb') as outfile:\n",
    "                    pickle.dump(results, outfile)\n",
    "                    print(f'result written at {self.result_fname+\".pickle\"}')\n",
    "                # self.save_checkpoint()\n",
    "                # print(f'checkpoint written at {self.checkpoint_fname}')\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "\n",
    "    def find_good_initializer(self):\n",
    "        print(\"finding good initializer from train data\")\n",
    "\n",
    "        cfg = self.config\n",
    "\n",
    "        if cfg['p'] == 4:\n",
    "            th = 0.1\n",
    "        elif cfg['p'] == 2:\n",
    "            th = 0.35\n",
    "        elif cfg['p'] == 1:\n",
    "            th = 0.0\n",
    "        else:\n",
    "            raise NotImplementedError(\"only p=1,2,4 supported\")\n",
    "\n",
    "        is_not_good = True\n",
    "        while is_not_good:\n",
    "            self.initialize_models()\n",
    "            t0 = time.time()\n",
    "            self.set_participating_nodes()\n",
    "            # res = self.test(train=True, force_full_nodes = True)\n",
    "            res = self.test(train=True)\n",
    "            t1 = time.time()\n",
    "            res['infer_time'] = t1-t0\n",
    "            self.print_epoch_stats(res)\n",
    "\n",
    "            cl_ct = res['cl_ct']\n",
    "\n",
    "            num_nodes = np.sum(cl_ct)\n",
    "            is_not_good = False\n",
    "            for ct in cl_ct:\n",
    "                if ct / num_nodes < th:\n",
    "                    is_not_good = True\n",
    "\n",
    "        print(\"found good initializer\")\n",
    "\n",
    "\n",
    "\n",
    "    def set_participating_nodes(self):\n",
    "        cfg = self.config\n",
    "        m = cfg['m']\n",
    "        p = cfg['p']\n",
    "        p_rate = cfg['participation_rate']\n",
    "\n",
    "        self.participating_nodes = np.random.choice(m, int(m * p_rate), replace = False)\n",
    "\n",
    "        return self.participating_nodes\n",
    "\n",
    "    def lr_schedule(self, epoch):\n",
    "        if self.lr is None:\n",
    "            self.lr = self.config['lr']\n",
    "\n",
    "        if epoch != 0 and LR_DECAY:\n",
    "            self.lr = self.lr * 0.99\n",
    "\n",
    "        return self.lr\n",
    "\n",
    "\n",
    "    def print_epoch_stats(self, res):\n",
    "        if res['is_train']:\n",
    "            data_str = 'tr'\n",
    "        else:\n",
    "            data_str = 'tst'\n",
    "\n",
    "        if 'train_time' in res:\n",
    "            time_str = f\"{res['train_time']:.3f}sec(train) {res['infer_time']:.3f}sec(infer)\"\n",
    "        else:\n",
    "            time_str = f\"{res['infer_time']:.3f}sec\"\n",
    "\n",
    "        if 'lr' in res:\n",
    "            lr_str = f\" lr {res['lr']:4f}\"\n",
    "        else:\n",
    "            lr_str = \"\"\n",
    "\n",
    "        if 'cl_ct' in res:\n",
    "            cl_str = f\" clct{res['cl_ct']} ans{res['cl_ct_ans']}\"\n",
    "        else:\n",
    "            cl_str = \"\"\n",
    "\n",
    "        str0 = f\"Epoch {self.epoch} {data_str}: l {res['loss']:.3f} a {res['acc']:.3f} {cl_str}{lr_str} {time_str}\"\n",
    "\n",
    "        print(str0)\n",
    "\n",
    "    def train(self, lr):\n",
    "\n",
    "        VERBOSE = 0\n",
    "\n",
    "        cfg = self.config\n",
    "        m = cfg['m']\n",
    "        p = cfg['p']\n",
    "        tau = cfg['tau']\n",
    "        n = cfg['n']\n",
    "        batch_size = cfg['batch_size']\n",
    "\n",
    "        participating_nodes = self.participating_nodes\n",
    "        cluster_assign = self.cluster_assign\n",
    "\n",
    "        t_put_weight = 0\n",
    "        t_get_weight = 0\n",
    "        time_load_data = 0\n",
    "        time_train = 0\n",
    "\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        updated_local_weights = []\n",
    "        for m_i2, m_i in enumerate(participating_nodes):\n",
    "            p_i = cluster_assign[m_i]\n",
    "\n",
    "            # if VERBOSE and m_i2 % 1 == 0: print(f'Local update m_i2 {m_i2}/{len(participating_nodes)} processing \\r', end ='')\n",
    "            if VERBOSE and m_i2 % 1 == 0: print(f'Local update m_i2 {m_i2}/{len(participating_nodes)} processing')\n",
    "\n",
    "            # Local Update process\n",
    "\n",
    "            t_p = time.time()\n",
    "            self.put_model_weights(self.model_weights[p_i])\n",
    "            t_p1 = time.time()\n",
    "            t_put_weight += t_p1-t_p\n",
    "\n",
    "            for l_epoch in range(tau): # local epochs\n",
    "\n",
    "                pmt = np.random.permutation(n)\n",
    "                local_indices_list = create_batches(pmt, batch_size = batch_size)\n",
    "                node_data_indices = self.dataset['train']['data_indices'][m_i]\n",
    "\n",
    "                for b_i, local_indices in enumerate(local_indices_list):\n",
    "                    t00 = time.time()\n",
    "\n",
    "                    current_batch_indices = node_data_indices[local_indices]\n",
    "\n",
    "                    (X_b, y_b) = self.load_data_by_index(current_batch_indices, m_i)\n",
    "\n",
    "                    t01 = time.time()\n",
    "\n",
    "                    fd0 = {\n",
    "                        self.x_pl:X_b,\n",
    "                        self.y_pl:y_b,\n",
    "                        self.lr_pl:self.lr\n",
    "                    }\n",
    "                    self.sess.run([self.train_op], feed_dict= fd0)\n",
    "\n",
    "                    t02 = time.time()\n",
    "\n",
    "                    time_load_data += t01 - t00\n",
    "                    time_train += t02 - t01\n",
    "\n",
    "\n",
    "            t_p = time.time()\n",
    "            updated_local_weight = self.get_model_weights()\n",
    "            t_p1 = time.time()\n",
    "            t_get_weight += t_p1-t_p\n",
    "\n",
    "            updated_local_weights.append(updated_local_weight)\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        # averaging\n",
    "\n",
    "\n",
    "        local_weights_cluster = [[] for _ in range(p)]\n",
    "\n",
    "        for m_i2, m_i in enumerate(participating_nodes):\n",
    "            p_i = cluster_assign[m_i]\n",
    "            local_weights_cluster[p_i].append(updated_local_weights[m_i2])\n",
    "\n",
    "        for p_i in range(p):\n",
    "            if len(local_weights_cluster[p_i]) > 0:\n",
    "                self.model_weights[p_i] = self.average_model_weights(local_weights_cluster[p_i])\n",
    "\n",
    "        t2 = time.time()\n",
    "\n",
    "        if VERBOSE: print(f\"train_whole {t1-t0:.3f} t_gd {time_train:.3f} t load data {time_load_data:.3f} t put model {t_put_weight:.3f} t get mdoel {t_get_weight:.3f}  averaging {t2-t1:.3f}\")\n",
    "\n",
    "    def test(self, train = True, force_full_nodes = False):\n",
    "\n",
    "        VERBOSE = 0\n",
    "\n",
    "        cfg = self.config\n",
    "        p = cfg['p']\n",
    "        p_rate = cfg['participation_rate']\n",
    "\n",
    "\n",
    "        if train:\n",
    "            m = cfg['m']\n",
    "            dataset = self.dataset['train']\n",
    "            if force_full_nodes:\n",
    "                participating_nodes = list(range(m))\n",
    "            else:\n",
    "                participating_nodes = self.participating_nodes\n",
    "        else:\n",
    "            m = cfg['m_test']\n",
    "            dataset = self.dataset['test']\n",
    "            participating_nodes = list(range(m))\n",
    "\n",
    "            # DEBUGGING\n",
    "            # print(\"DEBUGGING MODEe\")\n",
    "            # participating_nodes = np.random.choice(m, int(m * p_rate), replace = False)\n",
    "\n",
    "\n",
    "        # get loss and correct from all data\n",
    "\n",
    "\n",
    "        t_load_model = 0\n",
    "        t_load_data = 0\n",
    "        t_infer = 0\n",
    "\n",
    "        losses = {}\n",
    "        corrects = {}\n",
    "        for p_i in range(p):\n",
    "\n",
    "            tp0= time.time()\n",
    "            self.put_model_weights(self.model_weights[p_i])\n",
    "            tp1= time.time()\n",
    "            t_load_model += tp1-tp0\n",
    "\n",
    "            for m_i in participating_nodes:\n",
    "\n",
    "                t00= time.time()\n",
    "                (X, y) = self.load_node_data(m_i, train=train) # load batch data rotated\n",
    "                t01= time.time()\n",
    "                t_load_data += t01-t00\n",
    "\n",
    "                ti0= time.time()\n",
    "                (loss, correct) = self.sess.run([self.loss, self.num_correct], feed_dict = {self.x_pl:X, self.y_pl:y})\n",
    "                ti1= time.time()\n",
    "                t_infer += ti1-ti0\n",
    "\n",
    "\n",
    "                losses[(m_i,p_i)] = loss\n",
    "                corrects[(m_i,p_i)] = correct\n",
    "\n",
    "\n",
    "        if VERBOSE: print(f\"loadmodel {t_load_model:.3f}, load data {t_load_data:.3f}, infer {t_infer:.3f}\")\n",
    "\n",
    "\n",
    "        # calculate loss and cluster the machines\n",
    "        cluster_assign = [-1 for _ in range(m)]\n",
    "        for m_i in participating_nodes:\n",
    "            machine_losses = [ losses[(m_i,p_i)] for p_i in range(p) ]\n",
    "            min_p_i = np.argmin(machine_losses)\n",
    "            cluster_assign[m_i] = min_p_i\n",
    "\n",
    "        # calculate optimal model's loss, acc over all models\n",
    "\n",
    "        num_data = len(participating_nodes) * cfg['n']\n",
    "        min_corrects = []\n",
    "        min_losses = []\n",
    "        for m_i in participating_nodes:\n",
    "            p_i = cluster_assign[m_i]\n",
    "\n",
    "            min_loss = losses[(m_i,p_i)]\n",
    "            min_losses.append(min_loss)\n",
    "\n",
    "            min_correct = corrects[(m_i,p_i)]\n",
    "            min_corrects.append(min_correct)\n",
    "\n",
    "        loss = np.mean(min_losses)\n",
    "        acc = np.sum(min_corrects) / num_data\n",
    "\n",
    "        # check cluster assignment acc\n",
    "        # cl_acc = np.mean(np.array(cluster_assign) == np.array(dataset['cluster_assign']))\n",
    "        cl_ct = [np.sum(np.array(cluster_assign) == p_i ) for p_i in range(p)]\n",
    "\n",
    "\n",
    "        cluster_assign_ans = dataset['cluster_assign']\n",
    "        cluster_assign_ans_part = np.array(cluster_assign_ans)[participating_nodes]\n",
    "        cl_ct_ans = [np.sum(np.array(cluster_assign_ans_part) == p_i ) for p_i in range(p)]\n",
    "\n",
    "        res = {} # results\n",
    "        # res['losses'] = losses\n",
    "        # res['corrects'] = corrects\n",
    "        # res['cluster_assign'] = cluster_assign\n",
    "        res['loss'] = loss\n",
    "        res['acc'] = acc\n",
    "        # res['cl_acc'] = cl_acc\n",
    "        res['cl_ct'] = cl_ct\n",
    "        res['cl_ct_ans'] = cl_ct_ans\n",
    "        res['is_train'] = train\n",
    "\n",
    "        if train:\n",
    "            self.cluster_assign = cluster_assign\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    def load_node_data(self, m_i, train=True):\n",
    "        if train:\n",
    "            dataset = self.dataset['train']\n",
    "        else:\n",
    "            dataset = self.dataset['test']\n",
    "\n",
    "        indices = dataset['data_indices'][m_i]\n",
    "\n",
    "        return self.load_data_by_index(indices, m_i, train)\n",
    "\n",
    "    def load_data_by_index(self, indices, m_i, train=True):\n",
    "\n",
    "        # transform\n",
    "        # maybe improve speed by tf.data.Dataset.apply?\n",
    "        # or just run pool map to this...\n",
    "        # maybe not needed\n",
    "\n",
    "        cfg = self.config\n",
    "\n",
    "        if train:\n",
    "            dataset = self.dataset['train']\n",
    "            transform_op = self.train_transform_op\n",
    "            # transform_op = self.test_transform_op\n",
    "        else:\n",
    "            dataset = self.dataset['test']\n",
    "            transform_op = self.test_transform_op\n",
    "\n",
    "\n",
    "        X_b = dataset['data_loader'][0][indices]\n",
    "        y_b = dataset['data_loader'][1][indices]\n",
    "\n",
    "        p_i = dataset['cluster_assign'][m_i]\n",
    "\n",
    "        if cfg['p'] == 4:\n",
    "            k = p_i\n",
    "        elif cfg['p'] == 2:\n",
    "            k = (p_i % 2) * 2\n",
    "        elif cfg['p'] == 1:\n",
    "            k = 0\n",
    "        else:\n",
    "            raise NotImplementedError(\"only p=1,2,4 supported\")\n",
    "\n",
    "        X_b2 = np.rot90(X_b, k=k, axes = (1,2)) # X_b: (bs, 32, 32, 3)\n",
    "\n",
    "        X_b3 = self.sess.run(transform_op, feed_dict = { self.x_tr_pl : X_b2 } )\n",
    "\n",
    "        return (X_b3, y_b)\n",
    "\n",
    "\n",
    "    # def save_checkpoint(self):\n",
    "    #     models_to_save = [model.state_dict() for model in self.models]\n",
    "    #     torch.save({'models':models_to_save}, self.checkpoint_fname)\n",
    "\n",
    "    #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab84d2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: {'m': 200, 'm_test': 40, 'p': 2, 'n': 500, 'participation_rate': 0.1, 'num_epochs': 600, 'batch_size': 50, 'tau': 5, 'lr': 0.25, 'data_seed': 0, 'train_seed': 0, 'project_dir': 'output'}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'set_random_seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig:\u001b[39m\u001b[38;5;124m\"\u001b[39m,config)\n\u001b[0;32m      5\u001b[0m exp \u001b[38;5;241m=\u001b[39m TrainCIFARCluster(config)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m exp\u001b[38;5;241m.\u001b[39mrun()\n",
      "Cell \u001b[1;32mIn[20], line 14\u001b[0m, in \u001b[0;36mTrainCIFARCluster.setup\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult_fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproject_dir\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproject_dir\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mset_random_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_seed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_datasets()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_model()\n",
      "Cell \u001b[1;32mIn[19], line 51\u001b[0m, in \u001b[0;36mset_random_seed\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_random_seed\u001b[39m(seed):\n\u001b[0;32m     50\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m---> 51\u001b[0m     \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_random_seed\u001b[49m(seed)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'set_random_seed'"
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "config['train_seed'] = config['data_seed']\n",
    "print(\"config:\",config)\n",
    "\n",
    "exp = TrainCIFARCluster(config)\n",
    "exp.setup()\n",
    "exp.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
