{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "OHJWesKs-tqd"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import itertools\n",
        "import pickle\n",
        "import copy\n",
        "import random\n",
        "import math\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from util import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAghP_o0-tqe"
      },
      "source": [
        "Reads Config file and prepares the arguments you can choose in the config.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "BbUZJ2E--tqe"
      },
      "outputs": [],
      "source": [
        "LR_DECAY = False\n",
        "def get_config():\n",
        "\n",
        "    # read config json and update the sysarg\n",
        "    with open(\"config.json\", \"r\") as read_file:\n",
        "        config = json.load(read_file)\n",
        "\n",
        "    if config[\"config_override\"] == \"\":\n",
        "        del config['config_override']\n",
        "    else:\n",
        "        print(config['config_override'])\n",
        "        config_override = json.loads(config['config_override'])\n",
        "        del config['config_override']\n",
        "        config.update(config_override)\n",
        "\n",
        "    return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-J1YEoM-tqe"
      },
      "source": [
        "Class SimpleLinear with simple MLP for MNIST Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "id5Wyt-V-tqf"
      },
      "outputs": [],
      "source": [
        "class SimpleLinear(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, h1=2048):\n",
        "        super().__init__()\n",
        "        self.fc1 = torch.nn.Linear(28*28, h1)\n",
        "        self.fc2 = torch.nn.Linear(h1, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # x = F.sigmoid(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    # def weight(self):\n",
        "    #     return self.linear1.weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qFBH01M-tqf"
      },
      "source": [
        "Class TrainMNISTCluster with all the methods needed to run the experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "IkGiGQ2G-tqf"
      },
      "outputs": [],
      "source": [
        "class TrainMNISTCluster(object):\n",
        "    def __init__(self, config, device):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        assert self.config['m'] % self.config['p'] == 0\n",
        "\n",
        "    def setup(self):\n",
        "\n",
        "        os.makedirs(self.config['project_dir'], exist_ok = True)\n",
        "\n",
        "        self.result_fname = os.path.join(self.config['project_dir'], 'results.pickle')\n",
        "        self.checkpoint_fname = os.path.join(self.config['project_dir'], 'checkpoint.pt')\n",
        "\n",
        "        self.setup_datasets()\n",
        "        self.setup_models()\n",
        "\n",
        "        self.epoch = None\n",
        "        self.lr = None\n",
        "\n",
        "\n",
        "    def setup_datasets(self):\n",
        "\n",
        "        np.random.seed(self.config['data_seed'])\n",
        "\n",
        "        # generate indices for each dataset\n",
        "        # also write cluster info\n",
        "\n",
        "        MNIST_TRAINSET_DATA_SIZE = 60000\n",
        "        MNIST_TESTSET_DATA_SIZE = 10000\n",
        "\n",
        "        np.random.seed(self.config['data_seed'])\n",
        "\n",
        "        cfg = self.config\n",
        "\n",
        "        self.dataset = {}\n",
        "\n",
        "        if cfg['uneven'] == True:\n",
        "            dataset = {}\n",
        "            dataset['data_indices'], dataset['cluster_assign'] = \\\n",
        "                self._setup_dataset_random_n(MNIST_TRAINSET_DATA_SIZE, cfg['p'], cfg['m'], cfg['n'])\n",
        "            (X, y) = self._load_MNIST(train=True)\n",
        "            dataset['X'] = X\n",
        "            dataset['y'] = y\n",
        "            self.dataset['train'] = dataset\n",
        "\n",
        "            dataset = {}\n",
        "            dataset['data_indices'], dataset['cluster_assign'] = \\\n",
        "                self._setup_dataset_random_n(MNIST_TESTSET_DATA_SIZE, cfg['p'], cfg['m_test'], cfg['n'], random=True)\n",
        "            (X, y) = self._load_MNIST(train=False)\n",
        "            dataset['X'] = X\n",
        "            dataset['y'] = y\n",
        "            self.dataset['test'] = dataset\n",
        "\n",
        "        else:\n",
        "            dataset = {}\n",
        "            dataset['data_indices'], dataset['cluster_assign'] = \\\n",
        "                self._setup_dataset(MNIST_TRAINSET_DATA_SIZE, cfg['p'], cfg['m'], cfg['n'])\n",
        "            (X, y) = self._load_MNIST(train=True)\n",
        "            dataset['X'] = X\n",
        "            dataset['y'] = y\n",
        "            self.dataset['train'] = dataset\n",
        "\n",
        "            dataset = {}\n",
        "            dataset['data_indices'], dataset['cluster_assign'] = \\\n",
        "                self._setup_dataset(MNIST_TESTSET_DATA_SIZE, cfg['p'], cfg['m_test'], cfg['n'], random=True)\n",
        "            (X, y) = self._load_MNIST(train=False)\n",
        "            dataset['X'] = X\n",
        "            dataset['y'] = y\n",
        "            self.dataset['test'] = dataset\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "    def _setup_dataset(self, num_data, p, m, n, random = True):\n",
        "\n",
        "        print(\"m:\",m)\n",
        "        print(\"p:\",p)\n",
        "        print(\"n:\",n)\n",
        "        print(\"num_data:\",num_data)\n",
        "        assert (m // p) * n == num_data\n",
        "\n",
        "        dataset = {}\n",
        "\n",
        "        cfg = self.config\n",
        "\n",
        "        data_indices = []\n",
        "        cluster_assign = []\n",
        "\n",
        "        m_per_cluster = m // p\n",
        "\n",
        "        for p_i in range(p):\n",
        "\n",
        "            if random:\n",
        "                ll = list(np.random.permutation(num_data))\n",
        "            else:\n",
        "                ll = list(range(num_data))\n",
        "\n",
        "            ll2 = chunkify(ll, m_per_cluster) # splits ll into m lists with size n\n",
        "            data_indices += ll2\n",
        "\n",
        "            cluster_assign += [p_i for _ in range(m_per_cluster)]\n",
        "\n",
        "        print(type(data_indices))\n",
        "        data_indices = np.array(data_indices)\n",
        "        cluster_assign = np.array(cluster_assign)\n",
        "        assert data_indices.shape[0] == cluster_assign.shape[0]\n",
        "        assert data_indices.shape[0] == m\n",
        "\n",
        "\n",
        "        return data_indices, cluster_assign\n",
        "\n",
        "\n",
        "    def _setup_dataset_random_n(self, num_data, p, m, n, random = True):\n",
        "\n",
        "        print(\"m:\",m)\n",
        "        print(\"p:\",p)\n",
        "        print(\"num_data:\",num_data)\n",
        "\n",
        "        dataset = {}\n",
        "\n",
        "        cfg = self.config\n",
        "\n",
        "        data_indices = []\n",
        "        cluster_assign = []\n",
        "\n",
        "        m_per_cluster = m // p\n",
        "\n",
        "        for p_i in range(p):\n",
        "\n",
        "            ll = list(np.random.permutation(num_data))\n",
        "\n",
        "            ll2 = chunkify_uneven(ll, m_per_cluster) # splits ll into m lists\n",
        "            data_indices += ll2\n",
        "\n",
        "            cluster_assign += [p_i for _ in range(m_per_cluster)]\n",
        "\n",
        "        data_indices = np.array(data_indices, dtype=object)\n",
        "        cluster_assign = np.array(cluster_assign)\n",
        "        assert data_indices.shape[0] == cluster_assign.shape[0]\n",
        "        assert data_indices.shape[0] == m\n",
        "\n",
        "\n",
        "        return data_indices, cluster_assign\n",
        "\n",
        "\n",
        "    def _load_MNIST(self, train=True):\n",
        "        transforms = torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               # torchvision.transforms.Normalize(\n",
        "                               #   (0.1307,), (0.3081,))\n",
        "                             ])\n",
        "        if train:\n",
        "            mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms)\n",
        "        else:\n",
        "            mnist_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms)\n",
        "\n",
        "        dl = DataLoader(mnist_dataset)\n",
        "\n",
        "        X = dl.dataset.data # (60000,28, 28)\n",
        "        y = dl.dataset.targets #(60000)\n",
        "\n",
        "        # normalize to have 0 ~ 1 range in each pixel\n",
        "\n",
        "        X = X / 255.0\n",
        "        X = X.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "\n",
        "    # Need p models for each client\n",
        "\n",
        "    def setup_models(self):\n",
        "        np.random.seed(self.config['train_seed'])\n",
        "        torch.manual_seed(self.config['train_seed'])\n",
        "\n",
        "        p = self.config['p']\n",
        "\n",
        "        self.models = [ SimpleLinear(h1 = self.config['h1']).to(self.device) for p_i in range(p)] # p models with p different params of dimension(1,d)\n",
        "\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        num_epochs = self.config['num_epochs']\n",
        "        lr = self.config['lr']\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # epoch -1\n",
        "        self.epoch = -1\n",
        "\n",
        "        result = {}\n",
        "        result['epoch'] = -1\n",
        "\n",
        "        t0 = time.time()\n",
        "        res = self.test(train=True)\n",
        "        t1 = time.time()\n",
        "        res['infer_time'] = t1-t0\n",
        "        result['train'] = res\n",
        "\n",
        "        self.print_epoch_stats(res)\n",
        "\n",
        "        t0 = time.time()\n",
        "        res = self.test(train=False)\n",
        "        t1 = time.time()\n",
        "        res['infer_time'] = t1-t0\n",
        "        result['test'] = res\n",
        "        self.print_epoch_stats(res)\n",
        "        results.append(result)\n",
        "\n",
        "        # this will be used in next epoch\n",
        "        cluster_assign = result['train']['cluster_assign']\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            self.epoch = epoch\n",
        "\n",
        "            result = {}\n",
        "            result['epoch'] = epoch\n",
        "\n",
        "            lr = self.lr_schedule(epoch)\n",
        "            result['lr'] = lr\n",
        "\n",
        "            t0 = time.time()\n",
        "            result['train'] = self.train(cluster_assign, lr = lr)\n",
        "            t1 = time.time()\n",
        "            train_time = t1-t0\n",
        "\n",
        "            t0 = time.time()\n",
        "            res = self.test(train=True)\n",
        "            t1 = time.time()\n",
        "            res['infer_time'] = t1-t0\n",
        "            res['train_time'] = train_time\n",
        "            res['lr'] = lr\n",
        "            result['train'] = res\n",
        "\n",
        "            self.print_epoch_stats(res)\n",
        "\n",
        "            t0 = time.time()\n",
        "            res = self.test(train=False)\n",
        "            t1 = time.time()\n",
        "            res['infer_time'] = t1-t0\n",
        "            result['test'] = res\n",
        "            self.print_epoch_stats(res)\n",
        "\n",
        "            results.append(result)\n",
        "\n",
        "            # this will be used in next epoch's gradient update\n",
        "            cluster_assign = result['train']['cluster_assign']\n",
        "\n",
        "            if epoch % 10 == 0 or epoch == num_epochs - 1 :\n",
        "                with open(self.result_fname, 'wb') as outfile:\n",
        "                    pickle.dump(results, outfile)\n",
        "                    print(f'result written at {self.result_fname}')\n",
        "                self.save_checkpoint()\n",
        "                print(f'checkpoint written at {self.checkpoint_fname}')\n",
        "\n",
        "        plt.figure(figsize=(10,5))\n",
        "        plt.plot([r['train']['loss'] for r in results], label='train loss')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('loss')\n",
        "        plt.title('Training Loss per Epoch')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(os.path.join(self.config['project_dir'], 'train_loss.png'))\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "        plt.figure(figsize=(10,5))\n",
        "        plt.plot([r['test']['acc'] for r in results], label='test acc')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('test accuracy')\n",
        "        plt.title('Test Accuracy per Epoch')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(os.path.join(self.config['project_dir'], 'test_acc.png'))\n",
        "\n",
        "    def lr_schedule(self, epoch):\n",
        "        if self.lr is None:\n",
        "            self.lr = self.config['lr']\n",
        "\n",
        "        if epoch % 50 == 0 and epoch != 0 and LR_DECAY:\n",
        "            self.lr = self.lr * 0.1\n",
        "\n",
        "        return self.lr\n",
        "\n",
        "\n",
        "    def print_epoch_stats(self, res):\n",
        "        if res['is_train']:\n",
        "            data_str = 'tr'\n",
        "        else:\n",
        "            data_str = 'tst'\n",
        "\n",
        "        if 'train_time' in res:\n",
        "            time_str = f\"{res['train_time']:.3f}sec(train) {res['infer_time']:.3f}sec(infer)\"\n",
        "        else:\n",
        "            time_str = f\"{res['infer_time']:.3f}sec\"\n",
        "\n",
        "        if 'lr' in res:\n",
        "            lr_str = f\" lr {res['lr']:4f}\"\n",
        "        else:\n",
        "            lr_str = \"\"\n",
        "\n",
        "        str0 = f\"Epoch {self.epoch} {data_str}: l {res['loss']:.3f} a {res['acc']:.3f} cl_acc {res['cl_acc']:.3f} clct{res['cl_ct']}{lr_str} {time_str}\"\n",
        "\n",
        "        print(str0)\n",
        "\n",
        "    def train(self, cluster_assign, lr):\n",
        "        VERBOSE = 0\n",
        "\n",
        "        cfg = self.config\n",
        "        m = cfg['m']\n",
        "        p = cfg['p']\n",
        "        tau = cfg['tau']\n",
        "\n",
        "        # run local update\n",
        "        t0 = time.time()\n",
        "\n",
        "\n",
        "        updated_models = []\n",
        "        for m_i in range(m):\n",
        "            if VERBOSE and m_i % 100 == 0: print(f'm {m_i}/{m} processing \\r', end ='')\n",
        "\n",
        "            (X, y) = self.load_data(m_i)\n",
        "\n",
        "            p_i = cluster_assign[m_i]\n",
        "            model = copy.deepcopy(self.models[p_i])\n",
        "\n",
        "            # LOCAL UPDATE PER MACHINE tau times\n",
        "            for step_i in range(tau):\n",
        "\n",
        "                y_logit = model(X)\n",
        "                loss = self.criterion(y_logit, y)\n",
        "\n",
        "                model.zero_grad()\n",
        "                loss.backward()\n",
        "                self.local_param_update(model, lr)\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            updated_models.append(model)\n",
        "\n",
        "        t02 = time.time()\n",
        "        # print(f'running single ..took {t02-t01:.3f}sec')\n",
        "\n",
        "\n",
        "        t1 = time.time()\n",
        "        if VERBOSE: print(f'local update {t1-t0:.3f}sec')\n",
        "\n",
        "        # apply gradient update\n",
        "        t0 = time.time()\n",
        "\n",
        "        # CLUSTER MACHINES INTO p_i's\n",
        "        local_models = [[] for p_i in range(p)]\n",
        "        for m_i in range(m):\n",
        "            p_i = cluster_assign[m_i]\n",
        "            local_models[p_i].append(updated_models[m_i])\n",
        "\n",
        "        # NEEDS TO BE DECENTRALIZED\n",
        "        for p_i, models in enumerate(local_models):\n",
        "            if len(models) > 0:\n",
        "                # self.dec_param_update(models, self.models[p_i])\n",
        "                self.global_param_update(models, self.models[p_i])\n",
        "        t1 = time.time()\n",
        "\n",
        "        if VERBOSE: print(f'global update {t1-t0:.3f}sec')\n",
        "\n",
        "    def check_local_model_loss(self, local_models):\n",
        "        # for debugging\n",
        "        m = self.config['m']\n",
        "\n",
        "        losses = []\n",
        "        for m_i in range(m):\n",
        "            (X, y) = self.load_data(m_i)\n",
        "            y_logit = local_models[m_i](X)\n",
        "            loss = self.criterion(y_logit, y)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        return np.array(losses)\n",
        "    \n",
        "    def get_cluster_accuracy(self, actual, pred):\n",
        "        \n",
        "        cm = confusion_matrix(actual, pred)\n",
        "\n",
        "        row_ind, col_ind = linear_sum_assignment(-cm)\n",
        "        matching = dict(zip(col_ind, row_ind))\n",
        "\n",
        "        remapped_preds = [matching[p] for p in actual]\n",
        "\n",
        "        cl_acc = np.mean(np.array(remapped_preds) == np.array(pred))\n",
        "\n",
        "        return cl_acc\n",
        "\n",
        "\n",
        "    def get_inference_stats(self, train = True):\n",
        "        cfg = self.config\n",
        "        if train:\n",
        "            m = cfg['m']\n",
        "            dataset = self.dataset['train']\n",
        "        else:\n",
        "            m = cfg['m_test']\n",
        "            dataset = self.dataset['test']\n",
        "\n",
        "        p = cfg['p']\n",
        "\n",
        "\n",
        "        num_data = 0\n",
        "        losses = {}\n",
        "        corrects = {}\n",
        "        for m_i in range(m):\n",
        "            (X, y) = self.load_data(m_i, train=train) # load batch data rotated\n",
        "\n",
        "            for p_i in range(p):\n",
        "                y_logit = self.models[p_i](X)\n",
        "                loss = self.criterion(y_logit, y) # loss of\n",
        "                n_correct = self.n_correct(y_logit, y)\n",
        "\n",
        "                # if torch.isnan(loss):\n",
        "                #     print(\"nan loss: \", dataset['data_indices'][m_i])\n",
        "\n",
        "                losses[(m_i,p_i)] = loss.item()\n",
        "                corrects[(m_i,p_i)] = n_correct\n",
        "\n",
        "            num_data += X.shape[0]\n",
        "\n",
        "        # calculate loss and cluster the machines\n",
        "        cluster_assign = []\n",
        "        for m_i in range(m):\n",
        "            machine_losses = [ losses[(m_i,p_i)] for p_i in range(p) ]\n",
        "            min_p_i = np.argmin(machine_losses)\n",
        "            cluster_assign.append(min_p_i)\n",
        "\n",
        "        # calculate optimal model's loss, acc over all models\n",
        "        min_corrects = []\n",
        "        min_losses = []\n",
        "        for m_i, p_i in enumerate(cluster_assign):\n",
        "\n",
        "            min_loss = losses[(m_i,p_i)]\n",
        "            min_losses.append(min_loss)\n",
        "\n",
        "            min_correct = corrects[(m_i,p_i)]\n",
        "            min_corrects.append(min_correct)\n",
        "\n",
        "        # print(\"losses: \", min_losses)\n",
        "        loss = np.mean(min_losses)\n",
        "        acc = np.sum(min_corrects) / num_data\n",
        "\n",
        "\n",
        "        # check cluster assignment acc\n",
        "        cl_acc = self.get_cluster_accuracy(dataset['cluster_assign'], cluster_assign)\n",
        "        cl_ct = [np.sum(np.array(cluster_assign) == p_i ) for p_i in range(p)]\n",
        "\n",
        "        # ca_t = np.array(cluster_assign)\n",
        "        # ca_acc = []\n",
        "        # portion = m / p\n",
        "        # index=0\n",
        "        # for p_i in range(1, p+1):\n",
        "        #     split = ca_t[index:int(portion*p_i)].tolist()\n",
        "        #     most_common = max(set(split), key=split.count)\n",
        "        #     count = split.count(most_common)\n",
        "        #     ca_acc.append(count / len(split))\n",
        "        #     index = int(portion*p_i)\n",
        "\n",
        "        # print(\"cluster assign acc: \", ca_acc)\n",
        "        # ca_acc = np.mean(ca_acc)\n",
        "\n",
        "        res = {} # results\n",
        "        # res['losses'] = losses\n",
        "        # res['corrects'] = corrects\n",
        "        res['cluster_assign'] = cluster_assign\n",
        "        res['num_data'] = num_data\n",
        "        res['loss'] = loss\n",
        "        res['acc'] = acc\n",
        "        res['cl_acc'] = cl_acc\n",
        "        res['cl_ct'] = cl_ct\n",
        "        res['is_train'] = train\n",
        "        # res['ca_acc'] = ca_acc\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "        return res\n",
        "\n",
        "    def n_correct(self, y_logit, y):\n",
        "        _, predicted = torch.max(y_logit.data, 1)\n",
        "        correct = (predicted == y).sum().item()\n",
        "\n",
        "        return correct\n",
        "\n",
        "    # TODO Does every Cluster get 4 clients with the same data, but rotated differently?\n",
        "\n",
        "    def load_data(self, m_i, train=True):\n",
        "        # this part is very fast since its just rearranging models\n",
        "        cfg = self.config\n",
        "\n",
        "        if train:\n",
        "            dataset = self.dataset['train']\n",
        "        else:\n",
        "            dataset = self.dataset['test']\n",
        "\n",
        "        indices = dataset['data_indices'][m_i]\n",
        "        p_i = dataset['cluster_assign'][m_i]\n",
        "\n",
        "        X_batch = dataset['X'][indices]\n",
        "        y_batch = dataset['y'][indices]\n",
        "\n",
        "        # k : how many times rotate 90 degree\n",
        "        # k =1 : 90 , k=2 180, k=3 270\n",
        "\n",
        "        if cfg['p'] == 4:\n",
        "            k = p_i\n",
        "        elif cfg['p'] == 2:\n",
        "            k = (p_i % 2) * 2\n",
        "        elif cfg['p'] == 1:\n",
        "            k = 0\n",
        "        else:\n",
        "            raise NotImplementedError(\"only p=1,2,4 supported\")\n",
        "\n",
        "        X_batch2 = torch.rot90(X_batch, k=int(k), dims = (1,2))\n",
        "        X_batch3 = X_batch2.reshape(-1, 28 * 28)\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "        return X_batch3, y_batch\n",
        "\n",
        "\n",
        "    def local_param_update(self, model, lr):\n",
        "\n",
        "        # gradient update manually\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                param.data -= lr * param.grad\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        # import ipdb; ipdb.set_trace() # we need to check the output of name, check if duplicate exists\n",
        "\n",
        "\n",
        "    def global_param_update(self, local_models, global_model):\n",
        "\n",
        "        # average of each weight\n",
        "\n",
        "        weights = {}\n",
        "\n",
        "        for m_i, local_model in enumerate(local_models):\n",
        "            for name, param in local_model.named_parameters():\n",
        "                if name not in weights:\n",
        "                    weights[name] = torch.zeros_like(param.data)\n",
        "\n",
        "                weights[name] += param.data\n",
        "\n",
        "        for name, param in global_model.named_parameters():\n",
        "            weights[name] /= len(local_models)\n",
        "            param.data = weights[name]\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "\n",
        "    def dec_param_update(self, local_models, global_model):\n",
        "\n",
        "        num_clients = len(local_models)\n",
        "\n",
        "        if num_clients == 0:\n",
        "            return\n",
        "\n",
        "        if num_clients == 1:\n",
        "            bc_client = dict(local_models[0].named_parameters())\n",
        "            for name, param in global_model.named_parameters():\n",
        "                param.data = bc_client[name].data.clone()\n",
        "            return\n",
        "\n",
        "        max_e = 100\n",
        "        if num_clients <= max_e:\n",
        "            e = num_clients - 1\n",
        "        else:\n",
        "            e = min(max_e, int(np.log(num_clients) * 10))\n",
        "\n",
        "        if e >= num_clients:\n",
        "            e = num_clients - 1\n",
        "\n",
        "        client_indices = list(range(num_clients))\n",
        "\n",
        "        for m_i, local_model in enumerate(local_models):\n",
        "            selected_clients = random.sample([i for i in client_indices if i != m_i], e)\n",
        "\n",
        "            for m_j in selected_clients:\n",
        "\n",
        "                m_j_params = dict(local_models[m_j].named_parameters())\n",
        "\n",
        "                for name, param in local_model.named_parameters():\n",
        "                    m_i_param = param.data.clone()\n",
        "                    m_j_param = m_j_params[name].data.clone()\n",
        "                    param.data = (m_i_param + m_j_param) / 2\n",
        "\n",
        "        bc_client = random.choice(client_indices)\n",
        "        bc_client_params = dict(local_models[bc_client].named_parameters())\n",
        "        for name, param in global_model.named_parameters():\n",
        "            param.data = bc_client_params[name].data.clone()\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "\n",
        "    def test(self, train=False):\n",
        "        return self.get_inference_stats(train=train)\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        models_to_save = [model.state_dict() for model in self.models]\n",
        "        torch.save({'models':models_to_save}, self.checkpoint_fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ADsUSUi-tqf"
      },
      "source": [
        "Running the Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T_XDv25r-tqf",
        "outputId": "1dda7e5a-27bd-46d0-c125-b6987383650d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config: {'m': 2400, 'm_test': 400, 'p': 4, 'n': 100, 'uneven': True, 'h1': 200, 'num_epochs': 300, 'batch_size': 100, 'tau': 10, 'lr': 0.1, 'data_seed': 8, 'train_seed': 8, 'project_dir': 'output'}\n",
            "Using device: cuda\n",
            "m: 2400\n",
            "p: 4\n",
            "num_data: 60000\n",
            "len:  60000\n",
            "len:  60000\n",
            "len:  60000\n",
            "len:  60000\n",
            "m: 400\n",
            "p: 4\n",
            "num_data: 10000\n",
            "len:  10000\n",
            "len:  10000\n",
            "len:  10000\n",
            "len:  10000\n",
            "Epoch -1 tr: l 2.289 a 0.131 cl_acc 0.217 clct[603, 1039, 507, 251] 3.055sec\n",
            "Epoch -1 tst: l 2.289 a 0.129 cl_acc 0.212 clct[95, 191, 75, 39] 0.472sec\n",
            "Epoch 0 tr: l 2.184 a 0.309 cl_acc 0.077 clct[719, 728, 600, 353] lr 0.100000 9.707sec(train) 2.717sec(infer)\n",
            "Epoch 0 tst: l 2.182 a 0.309 cl_acc 0.083 clct[116, 133, 89, 62] 0.474sec\n",
            "result written at output\\results.pickle\n",
            "checkpoint written at output\\checkpoint.pt\n",
            "Epoch 1 tr: l 1.961 a 0.531 cl_acc 0.006 clct[600, 604, 607, 589] lr 0.100000 9.819sec(train) 2.877sec(infer)\n",
            "Epoch 1 tst: l 1.952 a 0.534 cl_acc 0.000 clct[101, 100, 100, 99] 0.484sec\n",
            "Epoch 2 tr: l 1.621 a 0.724 cl_acc 0.002 clct[599, 604, 602, 595] lr 0.100000 10.275sec(train) 2.891sec(infer)\n",
            "Epoch 2 tst: l 1.603 a 0.732 cl_acc 0.000 clct[100, 100, 100, 100] 0.491sec\n",
            "Epoch 3 tr: l 1.306 a 0.770 cl_acc 0.001 clct[599, 602, 602, 597] lr 0.100000 9.585sec(train) 2.732sec(infer)\n",
            "Epoch 3 tst: l 1.282 a 0.779 cl_acc 0.000 clct[100, 100, 100, 100] 0.473sec\n",
            "Epoch 4 tr: l 1.069 a 0.799 cl_acc 0.001 clct[599, 601, 602, 598] lr 0.100000 9.625sec(train) 2.817sec(infer)\n",
            "Epoch 4 tst: l 1.041 a 0.807 cl_acc 0.000 clct[99, 100, 100, 101] 0.478sec\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[100], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m exp \u001b[38;5;241m=\u001b[39m TrainMNISTCluster(config, device)\n\u001b[0;32m     11\u001b[0m exp\u001b[38;5;241m.\u001b[39msetup()\n\u001b[1;32m---> 12\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m duration \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---train cluster Ended in \u001b[39m\u001b[38;5;132;01m%0.2f\u001b[39;00m\u001b[38;5;124m hour (\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m sec) \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (duration\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m3600\u001b[39m), duration))\n",
            "Cell \u001b[1;32mIn[99], line 227\u001b[0m, in \u001b[0;36mTrainMNISTCluster.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m lr\n\u001b[0;32m    226\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 227\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster_assign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    229\u001b[0m train_time \u001b[38;5;241m=\u001b[39m t1\u001b[38;5;241m-\u001b[39mt0\n",
            "Cell \u001b[1;32mIn[99], line 337\u001b[0m, in \u001b[0;36mTrainMNISTCluster.train\u001b[1;34m(self, cluster_assign, lr)\u001b[0m\n\u001b[0;32m    334\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(y_logit, y)\n\u001b[0;32m    336\u001b[0m     model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 337\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_param_update(model, lr)\n\u001b[0;32m    340\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[1;32me:\\anaconda\\envs\\deep_learning\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32me:\\anaconda\\envs\\deep_learning\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "config = get_config()\n",
        "\n",
        "config['train_seed'] = config['data_seed']\n",
        "\n",
        "print(\"config:\",config)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "exp = TrainMNISTCluster(config, device)\n",
        "exp.setup()\n",
        "exp.run()\n",
        "duration = (time.time() - start_time)\n",
        "print(\"---train cluster Ended in %0.2f hour (%.3f sec) \" % (duration/float(3600), duration))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deep_learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
