{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f53520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_18532\\451422013.py:14: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\anaconda\\envs\\dl-new\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from util import *\n",
    "import cifar10\n",
    "\n",
    "\n",
    "\n",
    "LR_DECAY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c81b2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "\n",
    "    # read config json and update the sysarg\n",
    "    with open(\"config.json\", \"r\") as read_file:\n",
    "        config = json.load(read_file)\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b48588e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 24\n",
    "\n",
    "def train_transform(reshaped_image):\n",
    "    # copied from cifar10_input.py / distorted_input()\n",
    "\n",
    "    height = IMAGE_SIZE\n",
    "    width = IMAGE_SIZE\n",
    "\n",
    "    # Image processing for training the network. Note the many random\n",
    "    # distortions applied to the image.\n",
    "\n",
    "    # Randomly crop a [height, width] section of the image.\n",
    "    distorted_image = tf.random_crop(reshaped_image, [tf.shape(reshaped_image)[0], height, width, 3])\n",
    "    # tf shape gives dynamic shape\n",
    "\n",
    "    # Randomly flip the image horizontally.\n",
    "    distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "\n",
    "    # Because these operations are not commutative, consider randomizing\n",
    "    # the order their operation.\n",
    "    distorted_image = tf.image.random_brightness(distorted_image,\n",
    "                                               max_delta=63)\n",
    "    distorted_image = tf.image.random_contrast(distorted_image,\n",
    "                                             lower=0.2, upper=1.8)\n",
    "\n",
    "    # Subtract off the mean and divide by the variance of the pixels.\n",
    "    float_image = tf.image.per_image_standardization(distorted_image)\n",
    "\n",
    "    return float_image\n",
    "\n",
    "def test_transform(reshaped_image):\n",
    "    # copied from cifar10_input.py / input()\n",
    "\n",
    "    height = IMAGE_SIZE\n",
    "    width = IMAGE_SIZE\n",
    "\n",
    "    # Image processing for evaluation.\n",
    "    # Crop the central [height, width] of the image.\n",
    "    resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image,\n",
    "                                                         width, height)\n",
    "\n",
    "    # Subtract off the mean and divide by the variance of the pixels.\n",
    "    float_image = tf.image.per_image_standardization(resized_image)\n",
    "\n",
    "    return float_image\n",
    "\n",
    "\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "\n",
    "def create_batches(pmt, batch_size):\n",
    "    batch_indices = []\n",
    "    ct = 0\n",
    "    for b_i in range(int(np.ceil( len(pmt) / batch_size))):\n",
    "        if ct + batch_size > len(pmt):\n",
    "            batch = pmt[ct : len(pmt)]\n",
    "            ct = len(pmt)\n",
    "        else:\n",
    "            batch = pmt[ct : ct + batch_size]\n",
    "            ct += batch_size\n",
    "        batch_indices.append(batch)\n",
    "\n",
    "    return batch_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "490bb8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainCIFARCluster(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        assert self.config['m'] % self.config['p'] == 0\n",
    "\n",
    "    def setup(self):\n",
    "\n",
    "        os.makedirs(self.config['project_dir'], exist_ok = True)\n",
    "\n",
    "        self.result_fname = os.path.join(self.config['project_dir'], 'results')\n",
    "        self.checkpoint_fname = os.path.join(self.config['project_dir'], 'checkpoint')\n",
    "\n",
    "        set_random_seed(self.config['data_seed'])\n",
    "        self.setup_datasets()\n",
    "        self.setup_model()\n",
    "\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        with tf.device('/gpu:0'):\n",
    "            self.sess = tf.Session(config=config)\n",
    "\n",
    "\n",
    "        set_random_seed(self.config['data_seed']+self.config['train_seed'])\n",
    "        self.initialize_models()\n",
    "        self.initialize_assign_ops()\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        self.epoch = None\n",
    "        self.lr = None\n",
    "\n",
    "\n",
    "    def setup_datasets(self):\n",
    "        # tf.enable_eager_execution()\n",
    "\n",
    "        # generate indices for each dataset\n",
    "        # also write cluster info\n",
    "\n",
    "        CIFAR10_TRAINSET_DATA_SIZE = 50000\n",
    "        CIFAR10_TESTSET_DATA_SIZE = 10000\n",
    "\n",
    "        cfg = self.config\n",
    "\n",
    "        self.dataset = {}\n",
    "\n",
    "        dataset = {}\n",
    "        dataset['data_indices'], dataset['cluster_assign'] = \\\n",
    "            self._setup_dataset(CIFAR10_TRAINSET_DATA_SIZE, cfg['p'], cfg['m'], cfg['n'])\n",
    "        dl = self._load_CIFAR(train=True)\n",
    "        dataset['data_loader'] = dl\n",
    "        self.dataset['train'] = dataset\n",
    "\n",
    "        dataset = {}\n",
    "        dataset['data_indices'], dataset['cluster_assign'] = \\\n",
    "            self._setup_dataset(CIFAR10_TESTSET_DATA_SIZE, cfg['p'], cfg['m_test'], cfg['n'], random=False)\n",
    "        dl = self._load_CIFAR(train=False)\n",
    "        dataset['data_loader'] = dl\n",
    "        self.dataset['test'] = dataset\n",
    "\n",
    "        # tf.disable_eager_execution()\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "\n",
    "    def _setup_dataset(self, num_data, p, m, n, random = True):\n",
    "\n",
    "        assert (m // p) * n == num_data\n",
    "\n",
    "        dataset = {}\n",
    "\n",
    "        cfg = self.config\n",
    "\n",
    "        data_indices = []\n",
    "        cluster_assign = []\n",
    "\n",
    "        m_per_cluster = m // p\n",
    "\n",
    "        for p_i in range(p):\n",
    "\n",
    "            if random:\n",
    "                ll = list(np.random.permutation(num_data))\n",
    "            else:\n",
    "                ll = list(range(num_data))\n",
    "\n",
    "            ll2 = chunkify(ll, m_per_cluster) # splits ll into m lists with size n\n",
    "            data_indices += ll2\n",
    "\n",
    "            cluster_assign += [p_i for _ in range(m_per_cluster)]\n",
    "\n",
    "        data_indices = np.array(data_indices)\n",
    "        cluster_assign = np.array(cluster_assign)\n",
    "        assert data_indices.shape[0] == cluster_assign.shape[0]\n",
    "        assert data_indices.shape[0] == m\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        return data_indices, cluster_assign\n",
    "\n",
    "\n",
    "    def _load_CIFAR(self, train=True):\n",
    "        # gives dataloader that gives (X,y) based on asked index\n",
    "\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "        # (50000, 32,32, 3) [0~1] , (50000, 1)\n",
    "\n",
    "        if train:\n",
    "            X = x_train / 255.0\n",
    "            y = y_train.reshape(-1)\n",
    "        else:\n",
    "            X = x_test / 255.0\n",
    "            y = y_test.reshape(-1)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    def setup_model(self):\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "\n",
    "        # setup tensorflow model structure\n",
    "\n",
    "        self.x_pl = tf.placeholder(tf.float32, shape=(None, 24, 24, 3), name='input_x')\n",
    "        self.y_pl = tf.placeholder(tf.int32, shape=(None, ), name='output_y')\n",
    "        self.lr_pl = tf.placeholder(tf.float32, shape=(), name='learning_rate')\n",
    "\n",
    "        self.y_logits = cifar10.inference(self.x_pl) # construct model\n",
    "        self.loss = cifar10.loss(self.y_logits, self.y_pl)\n",
    "\n",
    "        self.y_pred = tf.cast(tf.argmax(self.y_logits, 1), tf.int32)\n",
    "        self.correct_prediction = tf.equal(self.y_pred, self.y_pl) # used for accuracy\n",
    "        self.num_correct = tf.reduce_sum(tf.cast(self.correct_prediction, tf.int64))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.lr_pl)\n",
    "        self.train_op = self.optimizer.minimize(self.loss)\n",
    "        self.opt_reset_op = tf.variables_initializer(self.optimizer.variables())\n",
    "\n",
    "        # import ipdb; ipdb.set_trace() # check self.optimizer.variables()\n",
    "\n",
    "        self.metrics = { # used by self.eval()\n",
    "            'loss':self.loss,\n",
    "            'correct': self.num_correct,\n",
    "            # and add more...\n",
    "        }\n",
    "\n",
    "\n",
    "        # transform ops\n",
    "        self.x_tr_pl = tf.placeholder(tf.float32, shape=(None, 32, 32, 3))\n",
    "        # with tf.device('/cpu:0'):\n",
    "        self.train_transform_op = train_transform(self.x_tr_pl)\n",
    "        self.test_transform_op = test_transform(self.x_tr_pl)\n",
    "\n",
    "\n",
    "    def initialize_models(self):\n",
    "\n",
    "        p = self.config['p']\n",
    "        m = self.config['m']\n",
    "\n",
    "        # initialize p times, to get p different sets of weights.\n",
    "\n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "        self.model_weights = []\n",
    "        models = []\n",
    "        for p_i in range(p):\n",
    "            self.sess.run(self.init_op)\n",
    "            weights = self.get_model_weights()\n",
    "            models.append(weights)\n",
    "\n",
    "        for m_i in range(m):\n",
    "            self.model_weights.append(models)\n",
    "\n",
    "    def get_model_weights(self):\n",
    "        self.collection = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "        names = [var.name for var in self.collection]\n",
    "        weights_arrays = self.sess.run(self.collection)\n",
    "\n",
    "        weights = dict(zip(names, weights_arrays))\n",
    "        # {'conv1/weights:0': np.array, ...}\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def initialize_assign_ops(self):\n",
    "        self.collection = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "        assign_ops = {}\n",
    "        assign_pls = {}\n",
    "        for var in self.collection:\n",
    "            # import ipdb; ipdb.set_trace()\n",
    "            pl = tf.placeholder(tf.float32, shape=var.shape)\n",
    "            assign_pls[var.name] = pl\n",
    "\n",
    "            op = tf.compat.v1.assign(var, pl)\n",
    "            assign_ops[var.name] = op\n",
    "\n",
    "\n",
    "        self.assign_ops = assign_ops\n",
    "        self.assign_pls = assign_pls\n",
    "\n",
    "    def put_model_weights(self, weights):\n",
    "\n",
    "        assign_ops = []\n",
    "\n",
    "        fd = {}\n",
    "        for var_name in self.assign_pls:\n",
    "            # assign_op = tf.assign(var, weights[var.name])\n",
    "            pl = self.assign_pls[var_name]\n",
    "            fd[pl] = weights[var_name]\n",
    "\n",
    "        self.sess.run(self.opt_reset_op) # reset the optimizer state ?\n",
    "        self.sess.run(list(self.assign_ops.values()), feed_dict = fd)\n",
    "\n",
    "    def average_model_weights(self, weights_list):\n",
    "\n",
    "        w2 = {}\n",
    "\n",
    "        for key in weights_list[0].keys():\n",
    "\n",
    "            w2[key] = np.mean([w[key] for w in weights_list], axis=0)\n",
    "\n",
    "        return w2\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        TRAIN_INFER_FULL_NODES = 0\n",
    "\n",
    "        num_epochs = self.config['num_epochs']\n",
    "        lr = self.config['lr']\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # epoch -1\n",
    "        self.epoch = -1\n",
    "\n",
    "        self.find_good_initializer()\n",
    "\n",
    "\n",
    "        result = {}\n",
    "        result['epoch'] = -1\n",
    "\n",
    "        t0 = time.time()\n",
    "        self.set_participating_nodes()\n",
    "        res = self.test(train=True, force_full_nodes =TRAIN_INFER_FULL_NODES)\n",
    "        # res = self.test(train=True)\n",
    "        t1 = time.time()\n",
    "        res['infer_time'] = t1-t0\n",
    "        result['train'] = res\n",
    "\n",
    "        self.print_epoch_stats(res)\n",
    "\n",
    "        t0 = time.time()\n",
    "        res = self.test(train=False)\n",
    "        t1 = time.time()\n",
    "        res['infer_time'] = t1-t0\n",
    "        result['test'] = res\n",
    "        self.print_epoch_stats(res)\n",
    "        results.append(result)\n",
    "\n",
    "        # this will be used in next epoch\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            result = {}\n",
    "            result['epoch'] = epoch\n",
    "\n",
    "            lr = self.lr_schedule(epoch)\n",
    "            result['lr'] = lr\n",
    "\n",
    "            t0 = time.time()\n",
    "            result['train'] = self.train(lr = lr)\n",
    "            t1 = time.time()\n",
    "            train_time = t1-t0\n",
    "\n",
    "            t0 = time.time()\n",
    "            self.set_participating_nodes()\n",
    "            # res = self.test(train=True)\n",
    "            res = self.test(train=True, force_full_nodes =TRAIN_INFER_FULL_NODES)\n",
    "            t1 = time.time()\n",
    "            res['infer_time'] = t1-t0\n",
    "            res['train_time'] = train_time\n",
    "            res['lr'] = lr\n",
    "            result['train'] = res\n",
    "\n",
    "            self.print_epoch_stats(res)\n",
    "\n",
    "            t0 = time.time()\n",
    "            res = self.test(train=False)\n",
    "            t1 = time.time()\n",
    "            res['infer_time'] = t1-t0\n",
    "            result['test'] = res\n",
    "            self.print_epoch_stats(res)\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "            if epoch % 10 == 0 or epoch == num_epochs - 1 :\n",
    "                with open(self.result_fname+\".pickle\", 'wb') as outfile:\n",
    "                    pickle.dump(results, outfile)\n",
    "                    print(f'result written at {self.result_fname+\".pickle\"}')\n",
    "                # self.save_checkpoint()\n",
    "                # print(f'checkpoint written at {self.checkpoint_fname}')\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "\n",
    "    def find_good_initializer(self):\n",
    "        print(\"finding good initializer from train data\")\n",
    "\n",
    "        cfg = self.config\n",
    "\n",
    "        if cfg['p'] == 4:\n",
    "            th = 0.1\n",
    "        elif cfg['p'] == 2:\n",
    "            th = 0.4\n",
    "        elif cfg['p'] == 1:\n",
    "            th = 0.0\n",
    "        else:\n",
    "            raise NotImplementedError(\"only p=1,2,4 supported\")\n",
    "\n",
    "        is_not_good = True\n",
    "        while is_not_good:\n",
    "            self.initialize_models()\n",
    "            t0 = time.time()\n",
    "            self.set_participating_nodes()\n",
    "            # res = self.test(train=True, force_full_nodes = True)\n",
    "            res = self.test(train=True)\n",
    "            t1 = time.time()\n",
    "            res['infer_time'] = t1-t0\n",
    "            self.print_epoch_stats(res)\n",
    "\n",
    "            cl_ct = res['cl_ct']\n",
    "\n",
    "            num_nodes = np.sum(cl_ct)\n",
    "            is_not_good = False\n",
    "            for ct in cl_ct:\n",
    "                if ct / num_nodes < th:\n",
    "                    is_not_good = True\n",
    "\n",
    "        print(\"found good initializer\")\n",
    "\n",
    "\n",
    "\n",
    "    def set_participating_nodes(self):\n",
    "        cfg = self.config\n",
    "        m = cfg['m']\n",
    "        p = cfg['p']\n",
    "        p_rate = cfg['participation_rate']\n",
    "\n",
    "        self.participating_nodes = np.random.choice(m, int(m * p_rate), replace = False)\n",
    "\n",
    "        return self.participating_nodes\n",
    "\n",
    "    def lr_schedule(self, epoch):\n",
    "        if self.lr is None:\n",
    "            self.lr = self.config['lr']\n",
    "\n",
    "        if epoch != 0 and LR_DECAY:\n",
    "            self.lr = self.lr * 0.99\n",
    "\n",
    "        return self.lr\n",
    "\n",
    "\n",
    "    def print_epoch_stats(self, res):\n",
    "        if res['is_train']:\n",
    "            data_str = 'tr'\n",
    "        else:\n",
    "            data_str = 'tst'\n",
    "\n",
    "        if 'train_time' in res:\n",
    "            time_str = f\"{res['train_time']:.3f}sec(train) {res['infer_time']:.3f}sec(infer)\"\n",
    "        else:\n",
    "            time_str = f\"{res['infer_time']:.3f}sec\"\n",
    "\n",
    "        if 'lr' in res:\n",
    "            lr_str = f\" lr {res['lr']:4f}\"\n",
    "        else:\n",
    "            lr_str = \"\"\n",
    "\n",
    "        if 'cl_ct' in res:\n",
    "            cl_str = f\" clct{res['cl_ct']} ans{res['cl_ct_ans']}\"\n",
    "        else:\n",
    "            cl_str = \"\"\n",
    "\n",
    "        str0 = f\"Epoch {self.epoch} {data_str}: l {res['loss']:.3f} a {res['acc']:.3f} {cl_str}{lr_str} {time_str}\"\n",
    "\n",
    "        print(str0)\n",
    "\n",
    "    def train(self, lr):\n",
    "\n",
    "        VERBOSE = 0\n",
    "\n",
    "        cfg = self.config\n",
    "        m = cfg['m']\n",
    "        p = cfg['p']\n",
    "        tau = cfg['tau']\n",
    "        n = cfg['n']\n",
    "        batch_size = cfg['batch_size']\n",
    "\n",
    "        participating_nodes = self.participating_nodes\n",
    "        cluster_assign = self.cluster_assign\n",
    "\n",
    "        t_put_weight = 0\n",
    "        t_get_weight = 0\n",
    "        time_load_data = 0\n",
    "        time_train = 0\n",
    "\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        updated_local_weights = []\n",
    "        for m_i2, m_i in enumerate(participating_nodes):\n",
    "            p_i = cluster_assign[m_i]\n",
    "\n",
    "            # if VERBOSE and m_i2 % 1 == 0: print(f'Local update m_i2 {m_i2}/{len(participating_nodes)} processing \\r', end ='')\n",
    "            if VERBOSE and m_i2 % 1 == 0: print(f'Local update m_i2 {m_i2}/{len(participating_nodes)} processing')\n",
    "\n",
    "            # Local Update process\n",
    "\n",
    "            t_p = time.time()\n",
    "            self.put_model_weights(self.model_weights[m_i][p_i])\n",
    "            t_p1 = time.time()\n",
    "            t_put_weight += t_p1-t_p\n",
    "\n",
    "            for l_epoch in range(tau): # local epochs\n",
    "\n",
    "                pmt = np.random.permutation(n)\n",
    "                local_indices_list = create_batches(pmt, batch_size = batch_size)\n",
    "                node_data_indices = self.dataset['train']['data_indices'][m_i]\n",
    "\n",
    "                for b_i, local_indices in enumerate(local_indices_list):\n",
    "                    t00 = time.time()\n",
    "\n",
    "                    current_batch_indices = node_data_indices[local_indices]\n",
    "\n",
    "                    (X_b, y_b) = self.load_data_by_index(current_batch_indices, m_i)\n",
    "\n",
    "                    t01 = time.time()\n",
    "\n",
    "                    fd0 = {\n",
    "                        self.x_pl:X_b,\n",
    "                        self.y_pl:y_b,\n",
    "                        self.lr_pl:self.lr\n",
    "                    }\n",
    "                    self.sess.run([self.train_op], feed_dict= fd0)\n",
    "\n",
    "                    t02 = time.time()\n",
    "\n",
    "                    time_load_data += t01 - t00\n",
    "                    time_train += t02 - t01\n",
    "\n",
    "\n",
    "            t_p = time.time()\n",
    "            updated_local_weight = self.get_model_weights()\n",
    "            t_p1 = time.time()\n",
    "            t_get_weight += t_p1-t_p\n",
    "\n",
    "            self.model_weights[m_i][p_i] = copy.deepcopy(updated_local_weight)\n",
    "            # updated_local_weights.append(updated_local_weight)\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        # averaging\n",
    "\n",
    "\n",
    "        # calculate the maximum number of possible exchange partners for m_i (capped at 100)\n",
    "        num_clients = len(participating_nodes)\n",
    "        min_partners = num_clients-1\n",
    "        th = min(min_partners, 10)\n",
    "        exchanges = 0\n",
    "\n",
    "        if th <= 1:\n",
    "            return\n",
    "\n",
    "        exchange_list = [random.sample([i for i in participating_nodes if i != m_i], int(np.random.randint(min(1, th-1), th, (1,)).item())) for m_i in range(num_clients)]\n",
    "\n",
    "        for m_i2, m_i in enumerate(participating_nodes):\n",
    "            selected_clients = exchange_list[m_i2]\n",
    "            m_i_cluster = cluster_assign[m_i]\n",
    "\n",
    "            for m_j in selected_clients:\n",
    "                exchanges += 2\n",
    "                m_j_cluster = cluster_assign[m_j]\n",
    "                m_j2 = list(participating_nodes).index(m_j)\n",
    "                \n",
    "                self.model_weights[m_i][m_j_cluster] = self.average_model_weights([self.model_weights[m_i][m_j_cluster], self.model_weights[m_j][m_j_cluster]])\n",
    "                self.model_weights[m_j][m_i_cluster] = self.average_model_weights([self.model_weights[m_j][m_i_cluster], self.model_weights[m_i][m_i_cluster]])\n",
    "\n",
    "                # remove m_i from m_j's list of selected clients\n",
    "                if m_i in exchange_list[m_j2]:\n",
    "                    exchange_list[m_j2].remove(m_i)\n",
    "\n",
    "                # done to keep the number of exchanges at len(selected_clients[m_j]) for each client\n",
    "                elif m_i not in exchange_list[m_j2] and exchange_list[m_j2]:\n",
    "                    exchange_list[m_j2].remove(random.choice(exchange_list[m_j2]))\n",
    "\n",
    "                else:\n",
    "                    for partners in exchange_list:\n",
    "                        if m_j in partners:\n",
    "                            partners.remove(m_j)\n",
    "        \n",
    "        print(\"exchanges\", exchanges)\n",
    "\n",
    "        # for p_i in range(p):\n",
    "        #     if len(local_weights_cluster[p_i]) > 0:\n",
    "        #         self.model_weights[p_i] = self.average_model_weights(local_weights_cluster[p_i])\n",
    "\n",
    "        t2 = time.time()\n",
    "\n",
    "        if VERBOSE: print(f\"train_whole {t1-t0:.3f} t_gd {time_train:.3f} t load data {time_load_data:.3f} t put model {t_put_weight:.3f} t get mdoel {t_get_weight:.3f}  averaging {t2-t1:.3f}\")\n",
    "\n",
    "    def get_cluster_accuracy(self, actual, pred):\n",
    "        \n",
    "        cm = confusion_matrix(actual, pred)\n",
    "\n",
    "        row_ind, col_ind = linear_sum_assignment(-cm)\n",
    "        matching = dict(zip(col_ind, row_ind))\n",
    "\n",
    "        remapped_preds = [matching[p] for p in pred]\n",
    "\n",
    "        cl_acc = np.mean(np.array(remapped_preds) == np.array(actual))\n",
    "\n",
    "        return cl_acc\n",
    "\n",
    "\n",
    "    def test(self, train = True, force_full_nodes = False):\n",
    "\n",
    "        VERBOSE = 0\n",
    "\n",
    "        cfg = self.config\n",
    "        p = cfg['p']\n",
    "        p_rate = cfg['participation_rate']\n",
    "\n",
    "\n",
    "        if train:\n",
    "            m = cfg['m']\n",
    "            dataset = self.dataset['train']\n",
    "            if force_full_nodes:\n",
    "                participating_nodes = list(range(m))\n",
    "            else:\n",
    "                participating_nodes = self.participating_nodes\n",
    "        else:\n",
    "            m = cfg['m_test']\n",
    "            dataset = self.dataset['test']\n",
    "            participating_nodes = list(range(m))\n",
    "\n",
    "            # DEBUGGING\n",
    "            # print(\"DEBUGGING MODEe\")\n",
    "            # participating_nodes = np.random.choice(m, int(m * p_rate), replace = False)\n",
    "\n",
    "\n",
    "        # get loss and correct from all data\n",
    "\n",
    "\n",
    "        t_load_model = 0\n",
    "        t_load_data = 0\n",
    "        t_infer = 0\n",
    "\n",
    "        losses = {}\n",
    "        corrects = {}\n",
    "        num_data = 0\n",
    "        for m_i in participating_nodes:\n",
    "            for p_i in range(p):\n",
    "\n",
    "                tp0= time.time()\n",
    "                self.put_model_weights(self.model_weights[m_i][p_i])\n",
    "                tp1= time.time()\n",
    "                t_load_model += tp1-tp0\n",
    "\n",
    "                t00= time.time()\n",
    "                (X, y) = self.load_node_data(m_i, train=train) # load batch data rotated\n",
    "                t01= time.time()\n",
    "                t_load_data += t01-t00\n",
    "\n",
    "                ti0= time.time()\n",
    "                (loss, correct) = self.sess.run([self.loss, self.num_correct], feed_dict = {self.x_pl:X, self.y_pl:y})\n",
    "                ti1= time.time()\n",
    "                t_infer += ti1-ti0\n",
    "\n",
    "\n",
    "                losses[(m_i,p_i)] = loss\n",
    "                corrects[(m_i,p_i)] = correct\n",
    "\n",
    "            num_data += X.shape[0]\n",
    "\n",
    "\n",
    "        if VERBOSE: print(f\"loadmodel {t_load_model:.3f}, load data {t_load_data:.3f}, infer {t_infer:.3f}\")\n",
    "\n",
    "\n",
    "        # calculate loss and cluster the machines\n",
    "        cluster_assign = [-1 for _ in range(m)]\n",
    "        for m_i in participating_nodes:\n",
    "            machine_losses = [ losses[(m_i,p_i)] for p_i in range(p) ]\n",
    "            min_p_i = np.argmin(machine_losses)\n",
    "            cluster_assign[m_i] = min_p_i\n",
    "\n",
    "        # calculate optimal model's loss, acc over all models\n",
    "\n",
    "        min_corrects = []\n",
    "        min_losses = []\n",
    "        for m_i in participating_nodes:\n",
    "            p_i = cluster_assign[m_i]\n",
    "\n",
    "            min_loss = losses[(m_i,p_i)]\n",
    "            min_losses.append(min_loss)\n",
    "\n",
    "            min_correct = corrects[(m_i,p_i)]\n",
    "            min_corrects.append(min_correct)\n",
    "\n",
    "        # if train:\n",
    "        #     loss = np.mean(min_losses)\n",
    "        #     acc = np.sum(min_corrects) / num_data\n",
    "\n",
    "        # else:\n",
    "        #     loss, acc = self.test_all()\n",
    "\n",
    "        loss = np.mean(min_losses)\n",
    "        acc = np.sum(min_corrects) / num_data\n",
    "\n",
    "        # check cluster assignment acc\n",
    "        # cl_acc = self.get_cluster_accuracy(dataset['cluster_assign'], cluster_assign)\n",
    "        cl_ct = [np.sum(np.array(cluster_assign) == p_i ) for p_i in range(p)]\n",
    "\n",
    "\n",
    "        cluster_assign_ans = dataset['cluster_assign']\n",
    "        cluster_assign_ans_part = np.array(cluster_assign_ans)[participating_nodes]\n",
    "        cl_ct_ans = [np.sum(np.array(cluster_assign_ans_part) == p_i ) for p_i in range(p)]\n",
    "\n",
    "        res = {} # results\n",
    "        # res['losses'] = losses\n",
    "        # res['corrects'] = corrects\n",
    "        # res['cluster_assign'] = cluster_assign\n",
    "        res['loss'] = loss\n",
    "        res['acc'] = acc\n",
    "        # res['cl_acc'] = cl_acc\n",
    "        res['cl_ct'] = cl_ct\n",
    "        res['cl_ct_ans'] = cl_ct_ans\n",
    "        res['is_train'] = train\n",
    "\n",
    "        if train:\n",
    "            self.cluster_assign = cluster_assign\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    def load_node_data(self, m_i, train=True):\n",
    "        if train:\n",
    "            dataset = self.dataset['train']\n",
    "        else:\n",
    "            dataset = self.dataset['test']\n",
    "\n",
    "        indices = dataset['data_indices'][m_i]\n",
    "\n",
    "        return self.load_data_by_index(indices, m_i, train)\n",
    "\n",
    "    def load_data_by_index(self, indices, m_i, train=True):\n",
    "\n",
    "        # transform\n",
    "        # maybe improve speed by tf.data.Dataset.apply?\n",
    "        # or just run pool map to this...\n",
    "        # maybe not needed\n",
    "\n",
    "        cfg = self.config\n",
    "\n",
    "        if train:\n",
    "            dataset = self.dataset['train']\n",
    "            transform_op = self.train_transform_op\n",
    "            # transform_op = self.test_transform_op\n",
    "        else:\n",
    "            dataset = self.dataset['test']\n",
    "            transform_op = self.test_transform_op\n",
    "\n",
    "\n",
    "        X_b = dataset['data_loader'][0][indices]\n",
    "        y_b = dataset['data_loader'][1][indices]\n",
    "\n",
    "        p_i = dataset['cluster_assign'][m_i]\n",
    "\n",
    "        if cfg['p'] == 4:\n",
    "            k = p_i\n",
    "        elif cfg['p'] == 2:\n",
    "            k = (p_i % 2) * 2\n",
    "        elif cfg['p'] == 1:\n",
    "            k = 0\n",
    "        else:\n",
    "            raise NotImplementedError(\"only p=1,2,4 supported\")\n",
    "\n",
    "        X_b2 = np.rot90(X_b, k=k, axes = (1,2)) # X_b: (bs, 32, 32, 3)\n",
    "\n",
    "        X_b3 = self.sess.run(transform_op, feed_dict = { self.x_tr_pl : X_b2 } )\n",
    "\n",
    "        return (X_b3, y_b)\n",
    "\n",
    "\n",
    "    # def save_checkpoint(self):\n",
    "    #     models_to_save = [model.state_dict() for model in self.models]\n",
    "    #     torch.save({'models':models_to_save}, self.checkpoint_fname)\n",
    "\n",
    "    #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab84d2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: {'m': 200, 'm_test': 40, 'p': 2, 'n': 500, 'participation_rate': 0.3, 'num_epochs': 600, 'batch_size': 50, 'tau': 5, 'lr': 0.25, 'data_seed': 0, 'train_seed': 0, 'project_dir': 'output'}\n",
      "WARNING:tensorflow:From C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_18532\\3820373181.py:118: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\GIT Repos\\decentralized-ifca\\cifar_tf\\cifar10.py:143: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From d:\\GIT Repos\\decentralized-ifca\\cifar_tf\\cifar10.py:231: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_18532\\3820373181.py:134: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_18532\\2967275566.py:39: The name tf.image.resize_image_with_crop_or_pad is deprecated. Please use tf.image.resize_with_crop_or_pad instead.\n",
      "\n",
      "finding good initializer from train data\n",
      "Epoch -1 tr: l 4.670 a 0.108  clct[np.int64(60), np.int64(0)] ans[np.int64(29), np.int64(31)] 13.953sec\n",
      "Epoch -1 tr: l 4.674 a 0.109  clct[np.int64(60), np.int64(0)] ans[np.int64(29), np.int64(31)] 13.679sec\n",
      "Epoch -1 tr: l 4.676 a 0.110  clct[np.int64(30), np.int64(30)] ans[np.int64(26), np.int64(34)] 13.604sec\n",
      "found good initializer\n",
      "Epoch -1 tr: l 4.676 a 0.112  clct[np.int64(19), np.int64(41)] ans[np.int64(32), np.int64(28)] 13.482sec\n",
      "Epoch -1 tst: l 4.675 a 0.111  clct[np.int64(15), np.int64(25)] ans[np.int64(20), np.int64(20)] 8.930sec\n",
      "exchanges 412\n",
      "Epoch 0 tr: l 1.992 a 0.405  clct[np.int64(28), np.int64(32)] ans[np.int64(28), np.int64(32)] lr 0.250000 74.061sec(train) 13.506sec(infer)\n",
      "Epoch 0 tst: l 1.871 a 0.446  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 8.889sec\n",
      "result written at output\\results.pickle\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m exp \u001b[38;5;241m=\u001b[39m TrainCIFARCluster(config)\n\u001b[0;32m      6\u001b[0m exp\u001b[38;5;241m.\u001b[39msetup()\n\u001b[1;32m----> 7\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 274\u001b[0m, in \u001b[0;36mTrainCIFARCluster.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    271\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m lr\n\u001b[0;32m    273\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 274\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    276\u001b[0m train_time \u001b[38;5;241m=\u001b[39m t1\u001b[38;5;241m-\u001b[39mt0\n",
      "Cell \u001b[1;32mIn[4], line 447\u001b[0m, in \u001b[0;36mTrainCIFARCluster.train\u001b[1;34m(self, lr)\u001b[0m\n\u001b[0;32m    440\u001b[0m t01 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    442\u001b[0m fd0 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_pl:X_b,\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_pl:y_b,\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_pl:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr\n\u001b[0;32m    446\u001b[0m }\n\u001b[1;32m--> 447\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_op\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfd0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    449\u001b[0m t02 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    451\u001b[0m time_load_data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m t01 \u001b[38;5;241m-\u001b[39m t00\n",
      "File \u001b[1;32me:\\anaconda\\envs\\dl-new\\lib\\site-packages\\tensorflow\\python\\client\\session.py:977\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    974\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 977\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    979\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m    980\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\dl-new\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1220\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[1;32m-> 1220\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1223\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32me:\\anaconda\\envs\\dl-new\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1400\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1397\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1400\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1401\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1403\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\dl-new\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1407\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m   1406\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1408\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1409\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\dl-new\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1390\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[0;32m   1388\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[0;32m   1389\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[1;32m-> 1390\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\anaconda\\envs\\dl-new\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1483\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1482\u001b[0m                         run_metadata):\n\u001b[1;32m-> 1483\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1485\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "config['train_seed'] = config['data_seed']\n",
    "print(\"config:\",config)\n",
    "\n",
    "exp = TrainCIFARCluster(config)\n",
    "exp.setup()\n",
    "exp.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
