{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f53520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_2444\\451422013.py:14: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\anaconda\\envs\\dl-new\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from util import *\n",
    "import cifar10\n",
    "\n",
    "\n",
    "\n",
    "LR_DECAY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c81b2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "\n",
    "    # read config json and update the sysarg\n",
    "    with open(\"config.json\", \"r\") as read_file:\n",
    "        config = json.load(read_file)\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b48588e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 24\n",
    "\n",
    "def train_transform(reshaped_image):\n",
    "    # copied from cifar10_input.py / distorted_input()\n",
    "\n",
    "    height = IMAGE_SIZE\n",
    "    width = IMAGE_SIZE\n",
    "\n",
    "    # Image processing for training the network. Note the many random\n",
    "    # distortions applied to the image.\n",
    "\n",
    "    # Randomly crop a [height, width] section of the image.\n",
    "    distorted_image = tf.random_crop(reshaped_image, [tf.shape(reshaped_image)[0], height, width, 3])\n",
    "    # tf shape gives dynamic shape\n",
    "\n",
    "    # Randomly flip the image horizontally.\n",
    "    distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "\n",
    "    # Because these operations are not commutative, consider randomizing\n",
    "    # the order their operation.\n",
    "    distorted_image = tf.image.random_brightness(distorted_image,\n",
    "                                               max_delta=63)\n",
    "    distorted_image = tf.image.random_contrast(distorted_image,\n",
    "                                             lower=0.2, upper=1.8)\n",
    "\n",
    "    # Subtract off the mean and divide by the variance of the pixels.\n",
    "    float_image = tf.image.per_image_standardization(distorted_image)\n",
    "\n",
    "    return float_image\n",
    "\n",
    "def test_transform(reshaped_image):\n",
    "    # copied from cifar10_input.py / input()\n",
    "\n",
    "    height = IMAGE_SIZE\n",
    "    width = IMAGE_SIZE\n",
    "\n",
    "    # Image processing for evaluation.\n",
    "    # Crop the central [height, width] of the image.\n",
    "    resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image,\n",
    "                                                         width, height)\n",
    "\n",
    "    # Subtract off the mean and divide by the variance of the pixels.\n",
    "    float_image = tf.image.per_image_standardization(resized_image)\n",
    "\n",
    "    return float_image\n",
    "\n",
    "\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    \n",
    "def create_batches(pmt, batch_size):\n",
    "    batch_indices = []\n",
    "    ct = 0\n",
    "    for b_i in range(int(np.ceil( len(pmt) / batch_size))):\n",
    "        if ct + batch_size > len(pmt):\n",
    "            batch = pmt[ct : len(pmt)]\n",
    "            ct = len(pmt)\n",
    "        else:\n",
    "            batch = pmt[ct : ct + batch_size]\n",
    "            ct += batch_size\n",
    "        batch_indices.append(batch)\n",
    "\n",
    "    return batch_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "490bb8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainCIFARCluster(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        assert self.config['m'] % self.config['p'] == 0\n",
    "\n",
    "    def setup(self):\n",
    "\n",
    "        os.makedirs(self.config['project_dir'], exist_ok = True)\n",
    "\n",
    "        self.result_fname = os.path.join(self.config['project_dir'], 'results')\n",
    "        self.checkpoint_fname = os.path.join(self.config['project_dir'], 'checkpoint')\n",
    "\n",
    "        set_random_seed(self.config['data_seed'])\n",
    "        self.setup_datasets()\n",
    "        self.setup_model()\n",
    "\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config)\n",
    "\n",
    "\n",
    "        set_random_seed(self.config['data_seed']+self.config['train_seed'])\n",
    "        self.initialize_models()\n",
    "        self.initialize_assign_ops()\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        self.epoch = None\n",
    "        self.lr = None\n",
    "\n",
    "\n",
    "    def setup_datasets(self):\n",
    "        # tf.enable_eager_execution()\n",
    "\n",
    "        # generate indices for each dataset\n",
    "        # also write cluster info\n",
    "\n",
    "        CIFAR10_TRAINSET_DATA_SIZE = 50000\n",
    "        CIFAR10_TESTSET_DATA_SIZE = 10000\n",
    "\n",
    "        cfg = self.config\n",
    "\n",
    "        self.dataset = {}\n",
    "\n",
    "        dataset = {}\n",
    "        dataset['data_indices'], dataset['cluster_assign'] = \\\n",
    "            self._setup_dataset(CIFAR10_TRAINSET_DATA_SIZE, cfg['p'], cfg['m'], cfg['n'])\n",
    "        dl = self._load_CIFAR(train=True)\n",
    "        dataset['data_loader'] = dl\n",
    "        self.dataset['train'] = dataset\n",
    "\n",
    "        dataset = {}\n",
    "        dataset['data_indices'], dataset['cluster_assign'] = \\\n",
    "            self._setup_dataset(CIFAR10_TESTSET_DATA_SIZE, cfg['p'], cfg['m_test'], cfg['n'], random=False)\n",
    "        dl = self._load_CIFAR(train=False)\n",
    "        dataset['data_loader'] = dl\n",
    "        self.dataset['test'] = dataset\n",
    "\n",
    "        # tf.disable_eager_execution()\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "\n",
    "    def _setup_dataset(self, num_data, p, m, n, random = True):\n",
    "\n",
    "        assert (m // p) * n == num_data\n",
    "\n",
    "        dataset = {}\n",
    "\n",
    "        cfg = self.config\n",
    "\n",
    "        data_indices = []\n",
    "        cluster_assign = []\n",
    "\n",
    "        m_per_cluster = m // p\n",
    "\n",
    "        for p_i in range(p):\n",
    "\n",
    "            if random:\n",
    "                ll = list(np.random.permutation(num_data))\n",
    "            else:\n",
    "                ll = list(range(num_data))\n",
    "\n",
    "            ll2 = chunkify(ll, m_per_cluster) # splits ll into m lists with size n\n",
    "            data_indices += ll2\n",
    "\n",
    "            cluster_assign += [p_i for _ in range(m_per_cluster)]\n",
    "\n",
    "        data_indices = np.array(data_indices)\n",
    "        cluster_assign = np.array(cluster_assign)\n",
    "        assert data_indices.shape[0] == cluster_assign.shape[0]\n",
    "        assert data_indices.shape[0] == m\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        return data_indices, cluster_assign\n",
    "\n",
    "\n",
    "    def _load_CIFAR(self, train=True):\n",
    "        # gives dataloader that gives (X,y) based on asked index\n",
    "\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "        # (50000, 32,32, 3) [0~1] , (50000, 1)\n",
    "\n",
    "        if train:\n",
    "            X = x_train / 255.0\n",
    "            y = y_train.reshape(-1)\n",
    "        else:\n",
    "            X = x_test / 255.0\n",
    "            y = y_test.reshape(-1)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    def setup_model(self):\n",
    "\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "\n",
    "        # setup tensorflow model structure\n",
    "\n",
    "        self.x_pl = tf.placeholder(tf.float32, shape=(None, 24, 24, 3), name='input_x')\n",
    "        self.y_pl = tf.placeholder(tf.int32, shape=(None, ), name='output_y')\n",
    "        self.lr_pl = tf.placeholder(tf.float32, shape=(), name='learning_rate')\n",
    "\n",
    "        self.y_logits = cifar10.inference(self.x_pl) # construct model\n",
    "        self.loss = cifar10.loss(self.y_logits, self.y_pl)\n",
    "\n",
    "        self.y_pred = tf.cast(tf.argmax(self.y_logits, 1), tf.int32)\n",
    "        self.correct_prediction = tf.equal(self.y_pred, self.y_pl) # used for accuracy\n",
    "        self.num_correct = tf.reduce_sum(tf.cast(self.correct_prediction, tf.int64))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.lr_pl)\n",
    "        self.train_op = self.optimizer.minimize(self.loss)\n",
    "        self.opt_reset_op = tf.variables_initializer(self.optimizer.variables())\n",
    "\n",
    "        # import ipdb; ipdb.set_trace() # check self.optimizer.variables()\n",
    "\n",
    "        self.metrics = { # used by self.eval()\n",
    "            'loss':self.loss,\n",
    "            'correct': self.num_correct,\n",
    "            # and add more...\n",
    "        }\n",
    "\n",
    "\n",
    "        # transform ops\n",
    "        self.x_tr_pl = tf.placeholder(tf.float32, shape=(None, 32, 32, 3))\n",
    "        # with tf.device('/cpu:0'):\n",
    "        self.train_transform_op = train_transform(self.x_tr_pl)\n",
    "        self.test_transform_op = test_transform(self.x_tr_pl)\n",
    "\n",
    "\n",
    "    def initialize_models(self):\n",
    "\n",
    "        p = self.config['p']\n",
    "\n",
    "        # initialize p times, to get p different sets of weights.\n",
    "\n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "        self.model_weights = []\n",
    "        for p_i in range(p):\n",
    "            self.sess.run(self.init_op)\n",
    "            weights = self.get_model_weights()\n",
    "            self.model_weights.append(weights)\n",
    "\n",
    "    def get_model_weights(self):\n",
    "        self.collection = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "        names = [var.name for var in self.collection]\n",
    "        weights_arrays = self.sess.run(self.collection)\n",
    "\n",
    "        weights = dict(zip(names, weights_arrays))\n",
    "        # {'conv1/weights:0': np.array, ...}\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def initialize_assign_ops(self):\n",
    "        self.collection = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "        assign_ops = {}\n",
    "        assign_pls = {}\n",
    "        for var in self.collection:\n",
    "            # import ipdb; ipdb.set_trace()\n",
    "            pl = tf.placeholder(tf.float32, shape=var.shape)\n",
    "            assign_pls[var.name] = pl\n",
    "\n",
    "            op = tf.compat.v1.assign(var, pl)\n",
    "            assign_ops[var.name] = op\n",
    "\n",
    "\n",
    "        self.assign_ops = assign_ops\n",
    "        self.assign_pls = assign_pls\n",
    "\n",
    "    def put_model_weights(self, weights):\n",
    "\n",
    "        assign_ops = []\n",
    "\n",
    "        fd = {}\n",
    "        for var_name in self.assign_pls:\n",
    "            # assign_op = tf.assign(var, weights[var.name])\n",
    "            pl = self.assign_pls[var_name]\n",
    "            fd[pl] = weights[var_name]\n",
    "\n",
    "        self.sess.run(self.opt_reset_op) # reset the optimizer state ?\n",
    "        self.sess.run(list(self.assign_ops.values()), feed_dict = fd)\n",
    "\n",
    "    def average_model_weights(self, weights_list):\n",
    "\n",
    "        w2 = {}\n",
    "\n",
    "        for key in weights_list[0].keys():\n",
    "\n",
    "            w2[key] = np.mean([w[key] for w in weights_list], axis=0)\n",
    "\n",
    "        return w2\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        TRAIN_INFER_FULL_NODES = 0\n",
    "\n",
    "        num_epochs = self.config['num_epochs']\n",
    "        lr = self.config['lr']\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # epoch -1\n",
    "        self.epoch = -1\n",
    "\n",
    "        self.find_good_initializer()\n",
    "\n",
    "\n",
    "        result = {}\n",
    "        result['epoch'] = -1\n",
    "\n",
    "        t0 = time.time()\n",
    "        self.set_participating_nodes()\n",
    "        res = self.test(train=True, force_full_nodes =TRAIN_INFER_FULL_NODES)\n",
    "        # res = self.test(train=True)\n",
    "        t1 = time.time()\n",
    "        res['infer_time'] = t1-t0\n",
    "        result['train'] = res\n",
    "\n",
    "        self.print_epoch_stats(res)\n",
    "\n",
    "        t0 = time.time()\n",
    "        res = self.test(train=False)\n",
    "        t1 = time.time()\n",
    "        res['infer_time'] = t1-t0\n",
    "        result['test'] = res\n",
    "        self.print_epoch_stats(res)\n",
    "        results.append(result)\n",
    "\n",
    "        # this will be used in next epoch\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            result = {}\n",
    "            result['epoch'] = epoch\n",
    "\n",
    "            lr = self.lr_schedule(epoch)\n",
    "            result['lr'] = lr\n",
    "\n",
    "            t0 = time.time()\n",
    "            result['train'] = self.train(lr = lr)\n",
    "            t1 = time.time()\n",
    "            train_time = t1-t0\n",
    "\n",
    "            t0 = time.time()\n",
    "            self.set_participating_nodes()\n",
    "            # res = self.test(train=True)\n",
    "            res = self.test(train=True, force_full_nodes =TRAIN_INFER_FULL_NODES)\n",
    "            t1 = time.time()\n",
    "            res['infer_time'] = t1-t0\n",
    "            res['train_time'] = train_time\n",
    "            res['lr'] = lr\n",
    "            result['train'] = res\n",
    "\n",
    "            self.print_epoch_stats(res)\n",
    "\n",
    "            t0 = time.time()\n",
    "            res = self.test(train=False)\n",
    "            t1 = time.time()\n",
    "            res['infer_time'] = t1-t0\n",
    "            result['test'] = res\n",
    "            self.print_epoch_stats(res)\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "            if epoch % 10 == 0 or epoch == num_epochs - 1 :\n",
    "                with open(self.result_fname+\".pickle\", 'wb') as outfile:\n",
    "                    pickle.dump(results, outfile)\n",
    "                    print(f'result written at {self.result_fname+\".pickle\"}')\n",
    "                # self.save_checkpoint()\n",
    "                # print(f'checkpoint written at {self.checkpoint_fname}')\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "\n",
    "    def find_good_initializer(self):\n",
    "        print(\"finding good initializer from train data\")\n",
    "\n",
    "        cfg = self.config\n",
    "\n",
    "        if cfg['p'] == 4:\n",
    "            th = 0.1\n",
    "        elif cfg['p'] == 2:\n",
    "            th = 0.35\n",
    "        elif cfg['p'] == 1:\n",
    "            th = 0.0\n",
    "        else:\n",
    "            raise NotImplementedError(\"only p=1,2,4 supported\")\n",
    "\n",
    "        is_not_good = True\n",
    "        while is_not_good:\n",
    "            self.initialize_models()\n",
    "            t0 = time.time()\n",
    "            self.set_participating_nodes()\n",
    "            # res = self.test(train=True, force_full_nodes = True)\n",
    "            res = self.test(train=True)\n",
    "            t1 = time.time()\n",
    "            res['infer_time'] = t1-t0\n",
    "            self.print_epoch_stats(res)\n",
    "\n",
    "            cl_ct = res['cl_ct']\n",
    "\n",
    "            num_nodes = np.sum(cl_ct)\n",
    "            is_not_good = False\n",
    "            for ct in cl_ct:\n",
    "                if ct / num_nodes < th:\n",
    "                    is_not_good = True\n",
    "\n",
    "        print(\"found good initializer\")\n",
    "\n",
    "\n",
    "\n",
    "    def set_participating_nodes(self):\n",
    "        cfg = self.config\n",
    "        m = cfg['m']\n",
    "        p = cfg['p']\n",
    "        p_rate = cfg['participation_rate']\n",
    "\n",
    "        self.participating_nodes = np.random.choice(m, int(m * p_rate), replace = False)\n",
    "\n",
    "        return self.participating_nodes\n",
    "\n",
    "    def lr_schedule(self, epoch):\n",
    "        if self.lr is None:\n",
    "            self.lr = self.config['lr']\n",
    "\n",
    "        if epoch != 0 and LR_DECAY:\n",
    "            self.lr = self.lr * 0.99\n",
    "\n",
    "        return self.lr\n",
    "\n",
    "\n",
    "    def print_epoch_stats(self, res):\n",
    "        if res['is_train']:\n",
    "            data_str = 'tr'\n",
    "        else:\n",
    "            data_str = 'tst'\n",
    "\n",
    "        if 'train_time' in res:\n",
    "            time_str = f\"{res['train_time']:.3f}sec(train) {res['infer_time']:.3f}sec(infer)\"\n",
    "        else:\n",
    "            time_str = f\"{res['infer_time']:.3f}sec\"\n",
    "\n",
    "        if 'lr' in res:\n",
    "            lr_str = f\" lr {res['lr']:4f}\"\n",
    "        else:\n",
    "            lr_str = \"\"\n",
    "\n",
    "        if 'cl_ct' in res:\n",
    "            cl_str = f\" clct{res['cl_ct']} ans{res['cl_ct_ans']}\"\n",
    "        else:\n",
    "            cl_str = \"\"\n",
    "\n",
    "        str0 = f\"Epoch {self.epoch} {data_str}: l {res['loss']:.3f} a {res['acc']:.3f} {cl_str}{lr_str} {time_str}\"\n",
    "\n",
    "        print(str0)\n",
    "\n",
    "    def train(self, lr):\n",
    "\n",
    "        VERBOSE = 0\n",
    "\n",
    "        cfg = self.config\n",
    "        m = cfg['m']\n",
    "        p = cfg['p']\n",
    "        tau = cfg['tau']\n",
    "        n = cfg['n']\n",
    "        batch_size = cfg['batch_size']\n",
    "\n",
    "        participating_nodes = self.participating_nodes\n",
    "        cluster_assign = self.cluster_assign\n",
    "\n",
    "        t_put_weight = 0\n",
    "        t_get_weight = 0\n",
    "        time_load_data = 0\n",
    "        time_train = 0\n",
    "\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        updated_local_weights = []\n",
    "        for m_i2, m_i in enumerate(participating_nodes):\n",
    "            p_i = cluster_assign[m_i]\n",
    "\n",
    "            # if VERBOSE and m_i2 % 1 == 0: print(f'Local update m_i2 {m_i2}/{len(participating_nodes)} processing \\r', end ='')\n",
    "            if VERBOSE and m_i2 % 1 == 0: print(f'Local update m_i2 {m_i2}/{len(participating_nodes)} processing')\n",
    "\n",
    "            # Local Update process\n",
    "\n",
    "            t_p = time.time()\n",
    "            self.put_model_weights(self.model_weights[p_i])\n",
    "            t_p1 = time.time()\n",
    "            t_put_weight += t_p1-t_p\n",
    "\n",
    "            for l_epoch in range(tau): # local epochs\n",
    "\n",
    "                pmt = np.random.permutation(n)\n",
    "                local_indices_list = create_batches(pmt, batch_size = batch_size)\n",
    "                node_data_indices = self.dataset['train']['data_indices'][m_i]\n",
    "\n",
    "                for b_i, local_indices in enumerate(local_indices_list):\n",
    "                    t00 = time.time()\n",
    "\n",
    "                    current_batch_indices = node_data_indices[local_indices]\n",
    "\n",
    "                    (X_b, y_b) = self.load_data_by_index(current_batch_indices, m_i)\n",
    "\n",
    "                    t01 = time.time()\n",
    "\n",
    "                    fd0 = {\n",
    "                        self.x_pl:X_b,\n",
    "                        self.y_pl:y_b,\n",
    "                        self.lr_pl:self.lr\n",
    "                    }\n",
    "                    self.sess.run([self.train_op], feed_dict= fd0)\n",
    "\n",
    "                    t02 = time.time()\n",
    "\n",
    "                    time_load_data += t01 - t00\n",
    "                    time_train += t02 - t01\n",
    "\n",
    "\n",
    "            t_p = time.time()\n",
    "            updated_local_weight = self.get_model_weights()\n",
    "            t_p1 = time.time()\n",
    "            t_get_weight += t_p1-t_p\n",
    "\n",
    "            updated_local_weights.append(updated_local_weight)\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        # averaging\n",
    "\n",
    "\n",
    "        local_weights_cluster = [[] for _ in range(p)]\n",
    "\n",
    "        for m_i2, m_i in enumerate(participating_nodes):\n",
    "            p_i = cluster_assign[m_i]\n",
    "            local_weights_cluster[p_i].append(updated_local_weights[m_i2])\n",
    "\n",
    "        for p_i in range(p):\n",
    "            if len(local_weights_cluster[p_i]) > 0:\n",
    "                self.model_weights[p_i] = self.average_model_weights(local_weights_cluster[p_i])\n",
    "\n",
    "        t2 = time.time()\n",
    "\n",
    "        if VERBOSE: print(f\"train_whole {t1-t0:.3f} t_gd {time_train:.3f} t load data {time_load_data:.3f} t put model {t_put_weight:.3f} t get mdoel {t_get_weight:.3f}  averaging {t2-t1:.3f}\")\n",
    "\n",
    "    def test(self, train = True, force_full_nodes = False):\n",
    "\n",
    "        VERBOSE = 0\n",
    "\n",
    "        cfg = self.config\n",
    "        p = cfg['p']\n",
    "        p_rate = cfg['participation_rate']\n",
    "\n",
    "\n",
    "        if train:\n",
    "            m = cfg['m']\n",
    "            dataset = self.dataset['train']\n",
    "            if force_full_nodes:\n",
    "                participating_nodes = list(range(m))\n",
    "            else:\n",
    "                participating_nodes = self.participating_nodes\n",
    "        else:\n",
    "            m = cfg['m_test']\n",
    "            dataset = self.dataset['test']\n",
    "            participating_nodes = list(range(m))\n",
    "\n",
    "            # DEBUGGING\n",
    "            # print(\"DEBUGGING MODEe\")\n",
    "            # participating_nodes = np.random.choice(m, int(m * p_rate), replace = False)\n",
    "\n",
    "\n",
    "        # get loss and correct from all data\n",
    "\n",
    "\n",
    "        t_load_model = 0\n",
    "        t_load_data = 0\n",
    "        t_infer = 0\n",
    "\n",
    "        losses = {}\n",
    "        corrects = {}\n",
    "        for p_i in range(p):\n",
    "\n",
    "            tp0= time.time()\n",
    "            self.put_model_weights(self.model_weights[p_i])\n",
    "            tp1= time.time()\n",
    "            t_load_model += tp1-tp0\n",
    "\n",
    "            for m_i in participating_nodes:\n",
    "\n",
    "                t00= time.time()\n",
    "                (X, y) = self.load_node_data(m_i, train=train) # load batch data rotated\n",
    "                t01= time.time()\n",
    "                t_load_data += t01-t00\n",
    "\n",
    "                ti0= time.time()\n",
    "                (loss, correct) = self.sess.run([self.loss, self.num_correct], feed_dict = {self.x_pl:X, self.y_pl:y})\n",
    "                ti1= time.time()\n",
    "                t_infer += ti1-ti0\n",
    "\n",
    "\n",
    "                losses[(m_i,p_i)] = loss\n",
    "                corrects[(m_i,p_i)] = correct\n",
    "\n",
    "\n",
    "        if VERBOSE: print(f\"loadmodel {t_load_model:.3f}, load data {t_load_data:.3f}, infer {t_infer:.3f}\")\n",
    "\n",
    "\n",
    "        # calculate loss and cluster the machines\n",
    "        cluster_assign = [-1 for _ in range(m)]\n",
    "        for m_i in participating_nodes:\n",
    "            machine_losses = [ losses[(m_i,p_i)] for p_i in range(p) ]\n",
    "            min_p_i = np.argmin(machine_losses)\n",
    "            cluster_assign[m_i] = min_p_i\n",
    "\n",
    "        # calculate optimal model's loss, acc over all models\n",
    "\n",
    "        num_data = len(participating_nodes) * cfg['n']\n",
    "        min_corrects = []\n",
    "        min_losses = []\n",
    "        for m_i in participating_nodes:\n",
    "            p_i = cluster_assign[m_i]\n",
    "\n",
    "            min_loss = losses[(m_i,p_i)]\n",
    "            min_losses.append(min_loss)\n",
    "\n",
    "            min_correct = corrects[(m_i,p_i)]\n",
    "            min_corrects.append(min_correct)\n",
    "\n",
    "        loss = np.mean(min_losses)\n",
    "        acc = np.sum(min_corrects) / num_data\n",
    "\n",
    "        # check cluster assignment acc\n",
    "        # cl_acc = np.mean(np.array(cluster_assign) == np.array(dataset['cluster_assign']))\n",
    "        cl_ct = [np.sum(np.array(cluster_assign) == p_i ) for p_i in range(p)]\n",
    "\n",
    "\n",
    "        cluster_assign_ans = dataset['cluster_assign']\n",
    "        cluster_assign_ans_part = np.array(cluster_assign_ans)[participating_nodes]\n",
    "        cl_ct_ans = [np.sum(np.array(cluster_assign_ans_part) == p_i ) for p_i in range(p)]\n",
    "\n",
    "        res = {} # results\n",
    "        # res['losses'] = losses\n",
    "        # res['corrects'] = corrects\n",
    "        # res['cluster_assign'] = cluster_assign\n",
    "        res['loss'] = loss\n",
    "        res['acc'] = acc\n",
    "        # res['cl_acc'] = cl_acc\n",
    "        res['cl_ct'] = cl_ct\n",
    "        res['cl_ct_ans'] = cl_ct_ans\n",
    "        res['is_train'] = train\n",
    "\n",
    "        if train:\n",
    "            self.cluster_assign = cluster_assign\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    def load_node_data(self, m_i, train=True):\n",
    "        if train:\n",
    "            dataset = self.dataset['train']\n",
    "        else:\n",
    "            dataset = self.dataset['test']\n",
    "\n",
    "        indices = dataset['data_indices'][m_i]\n",
    "\n",
    "        return self.load_data_by_index(indices, m_i, train)\n",
    "\n",
    "    def load_data_by_index(self, indices, m_i, train=True):\n",
    "\n",
    "        # transform\n",
    "        # maybe improve speed by tf.data.Dataset.apply?\n",
    "        # or just run pool map to this...\n",
    "        # maybe not needed\n",
    "\n",
    "        cfg = self.config\n",
    "\n",
    "        if train:\n",
    "            dataset = self.dataset['train']\n",
    "            transform_op = self.train_transform_op\n",
    "            # transform_op = self.test_transform_op\n",
    "        else:\n",
    "            dataset = self.dataset['test']\n",
    "            transform_op = self.test_transform_op\n",
    "\n",
    "\n",
    "        X_b = dataset['data_loader'][0][indices]\n",
    "        y_b = dataset['data_loader'][1][indices]\n",
    "\n",
    "        p_i = dataset['cluster_assign'][m_i]\n",
    "\n",
    "        if cfg['p'] == 4:\n",
    "            k = p_i\n",
    "        elif cfg['p'] == 2:\n",
    "            k = (p_i % 2) * 2\n",
    "        elif cfg['p'] == 1:\n",
    "            k = 0\n",
    "        else:\n",
    "            raise NotImplementedError(\"only p=1,2,4 supported\")\n",
    "\n",
    "        X_b2 = np.rot90(X_b, k=k, axes = (1,2)) # X_b: (bs, 32, 32, 3)\n",
    "\n",
    "        X_b3 = self.sess.run(transform_op, feed_dict = { self.x_tr_pl : X_b2 } )\n",
    "\n",
    "        return (X_b3, y_b)\n",
    "\n",
    "\n",
    "    # def save_checkpoint(self):\n",
    "    #     models_to_save = [model.state_dict() for model in self.models]\n",
    "    #     torch.save({'models':models_to_save}, self.checkpoint_fname)\n",
    "\n",
    "    #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab84d2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: {'m': 200, 'm_test': 40, 'p': 2, 'n': 500, 'participation_rate': 0.1, 'num_epochs': 600, 'batch_size': 50, 'tau': 5, 'lr': 0.25, 'data_seed': 0, 'train_seed': 0, 'project_dir': 'output'}\n",
      "WARNING:tensorflow:From C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_2444\\3097515119.py:118: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\GIT Repos\\decentralized-ifca\\cifar_tf\\cifar10.py:143: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From d:\\GIT Repos\\decentralized-ifca\\cifar_tf\\cifar10.py:231: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_2444\\3097515119.py:134: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_2444\\2102401459.py:39: The name tf.image.resize_image_with_crop_or_pad is deprecated. Please use tf.image.resize_with_crop_or_pad instead.\n",
      "\n",
      "finding good initializer from train data\n",
      "Epoch -1 tr: l 4.676 a 0.092  clct[np.int64(0), np.int64(20)] ans[np.int64(10), np.int64(10)] 3.049sec\n",
      "Epoch -1 tr: l 4.674 a 0.096  clct[np.int64(18), np.int64(2)] ans[np.int64(9), np.int64(11)] 2.904sec\n",
      "Epoch -1 tr: l 4.676 a 0.094  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] 3.082sec\n",
      "found good initializer\n",
      "Epoch -1 tr: l 4.676 a 0.098  clct[np.int64(7), np.int64(13)] ans[np.int64(9), np.int64(11)] 3.057sec\n",
      "Epoch -1 tst: l 4.676 a 0.102  clct[np.int64(17), np.int64(23)] ans[np.int64(20), np.int64(20)] 5.934sec\n",
      "Epoch 0 tr: l 4.383 a 0.200  clct[np.int64(14), np.int64(6)] ans[np.int64(8), np.int64(12)] lr 0.250000 15.278sec(train) 2.959sec(infer)\n",
      "Epoch 0 tst: l 4.379 a 0.215  clct[np.int64(28), np.int64(12)] ans[np.int64(20), np.int64(20)] 5.627sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 1 tr: l 4.099 a 0.243  clct[np.int64(7), np.int64(13)] ans[np.int64(8), np.int64(12)] lr 0.247500 14.621sec(train) 2.937sec(infer)\n",
      "Epoch 1 tst: l 4.092 a 0.275  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.535sec\n",
      "Epoch 2 tr: l 3.796 a 0.281  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.245025 14.853sec(train) 2.812sec(infer)\n",
      "Epoch 2 tst: l 3.774 a 0.301  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.530sec\n",
      "Epoch 3 tr: l 3.524 a 0.317  clct[np.int64(13), np.int64(7)] ans[np.int64(13), np.int64(7)] lr 0.242575 16.165sec(train) 2.887sec(infer)\n",
      "Epoch 3 tst: l 3.473 a 0.352  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.498sec\n",
      "Epoch 4 tr: l 3.289 a 0.360  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.240149 14.795sec(train) 2.894sec(infer)\n",
      "Epoch 4 tst: l 3.241 a 0.401  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.904sec\n",
      "Epoch 5 tr: l 3.107 a 0.379  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.237748 16.197sec(train) 2.928sec(infer)\n",
      "Epoch 5 tst: l 3.031 a 0.422  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.527sec\n",
      "Epoch 6 tr: l 2.979 a 0.378  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.235370 14.522sec(train) 2.816sec(infer)\n",
      "Epoch 6 tst: l 2.907 a 0.427  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.467sec\n",
      "Epoch 7 tr: l 2.799 a 0.413  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.233016 15.503sec(train) 3.048sec(infer)\n",
      "Epoch 7 tst: l 2.707 a 0.465  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.562sec\n",
      "Epoch 8 tr: l 2.673 a 0.417  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.230686 14.592sec(train) 2.923sec(infer)\n",
      "Epoch 8 tst: l 2.575 a 0.463  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.435sec\n",
      "Epoch 9 tr: l 2.552 a 0.429  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.228379 18.260sec(train) 2.854sec(infer)\n",
      "Epoch 9 tst: l 2.444 a 0.478  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.525sec\n",
      "Epoch 10 tr: l 2.460 a 0.440  clct[np.int64(7), np.int64(13)] ans[np.int64(7), np.int64(13)] lr 0.226096 14.625sec(train) 2.793sec(infer)\n",
      "Epoch 10 tst: l 2.354 a 0.485  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.460sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 11 tr: l 2.339 a 0.453  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.223835 15.010sec(train) 2.686sec(infer)\n",
      "Epoch 11 tst: l 2.207 a 0.516  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.939sec\n",
      "Epoch 12 tr: l 2.217 a 0.485  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.221596 15.069sec(train) 2.834sec(infer)\n",
      "Epoch 12 tst: l 2.121 a 0.522  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.507sec\n",
      "Epoch 13 tr: l 2.132 a 0.479  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.219380 14.600sec(train) 2.785sec(infer)\n",
      "Epoch 13 tst: l 2.049 a 0.522  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.508sec\n",
      "Epoch 14 tr: l 2.081 a 0.489  clct[np.int64(14), np.int64(6)] ans[np.int64(14), np.int64(6)] lr 0.217186 14.668sec(train) 2.849sec(infer)\n",
      "Epoch 14 tst: l 1.956 a 0.542  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.485sec\n",
      "Epoch 15 tr: l 1.973 a 0.515  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.215015 14.731sec(train) 2.771sec(infer)\n",
      "Epoch 15 tst: l 1.884 a 0.554  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.544sec\n",
      "Epoch 16 tr: l 1.945 a 0.509  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.212864 14.896sec(train) 2.805sec(infer)\n",
      "Epoch 16 tst: l 1.842 a 0.550  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.502sec\n",
      "Epoch 17 tr: l 1.876 a 0.530  clct[np.int64(15), np.int64(5)] ans[np.int64(15), np.int64(5)] lr 0.210736 14.545sec(train) 2.792sec(infer)\n",
      "Epoch 17 tst: l 1.776 a 0.567  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.477sec\n",
      "Epoch 18 tr: l 1.879 a 0.513  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.208628 14.677sec(train) 2.831sec(infer)\n",
      "Epoch 18 tst: l 1.718 a 0.574  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.223sec\n",
      "Epoch 19 tr: l 1.755 a 0.550  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.206542 15.957sec(train) 3.056sec(infer)\n",
      "Epoch 19 tst: l 1.657 a 0.592  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.466sec\n",
      "Epoch 20 tr: l 1.704 a 0.568  clct[np.int64(7), np.int64(13)] ans[np.int64(7), np.int64(13)] lr 0.204477 16.464sec(train) 3.099sec(infer)\n",
      "Epoch 20 tst: l 1.612 a 0.595  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.008sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 21 tr: l 1.688 a 0.544  clct[np.int64(15), np.int64(5)] ans[np.int64(15), np.int64(5)] lr 0.202432 15.234sec(train) 2.796sec(infer)\n",
      "Epoch 21 tst: l 1.565 a 0.605  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.500sec\n",
      "Epoch 22 tr: l 1.663 a 0.548  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.200408 14.507sec(train) 2.811sec(infer)\n",
      "Epoch 22 tst: l 1.535 a 0.601  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.552sec\n",
      "Epoch 23 tr: l 1.578 a 0.579  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.198404 14.586sec(train) 2.803sec(infer)\n",
      "Epoch 23 tst: l 1.489 a 0.616  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.728sec\n",
      "Epoch 24 tr: l 1.540 a 0.585  clct[np.int64(14), np.int64(6)] ans[np.int64(14), np.int64(6)] lr 0.196420 15.161sec(train) 2.819sec(infer)\n",
      "Epoch 24 tst: l 1.453 a 0.623  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.477sec\n",
      "Epoch 25 tr: l 1.520 a 0.593  clct[np.int64(14), np.int64(6)] ans[np.int64(14), np.int64(6)] lr 0.194455 14.564sec(train) 2.765sec(infer)\n",
      "Epoch 25 tst: l 1.435 a 0.624  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.489sec\n",
      "Epoch 26 tr: l 1.447 a 0.604  clct[np.int64(14), np.int64(6)] ans[np.int64(14), np.int64(6)] lr 0.192511 14.571sec(train) 2.886sec(infer)\n",
      "Epoch 26 tst: l 1.373 a 0.639  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.754sec\n",
      "Epoch 27 tr: l 1.466 a 0.599  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.190586 14.530sec(train) 2.814sec(infer)\n",
      "Epoch 27 tst: l 1.363 a 0.636  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.492sec\n",
      "Epoch 28 tr: l 1.403 a 0.616  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.188680 14.718sec(train) 2.777sec(infer)\n",
      "Epoch 28 tst: l 1.326 a 0.645  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.849sec\n",
      "Epoch 29 tr: l 1.384 a 0.624  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.186793 14.872sec(train) 2.802sec(infer)\n",
      "Epoch 29 tst: l 1.288 a 0.655  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.504sec\n",
      "Epoch 30 tr: l 1.344 a 0.622  clct[np.int64(7), np.int64(13)] ans[np.int64(7), np.int64(13)] lr 0.184925 14.708sec(train) 2.819sec(infer)\n",
      "Epoch 30 tst: l 1.261 a 0.666  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.507sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 31 tr: l 1.367 a 0.614  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.183076 14.564sec(train) 2.787sec(infer)\n",
      "Epoch 31 tst: l 1.236 a 0.669  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.460sec\n",
      "Epoch 32 tr: l 1.351 a 0.618  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.181245 14.568sec(train) 2.801sec(infer)\n",
      "Epoch 32 tst: l 1.225 a 0.674  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.514sec\n",
      "Epoch 33 tr: l 1.272 a 0.649  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.179433 14.747sec(train) 2.755sec(infer)\n",
      "Epoch 33 tst: l 1.200 a 0.674  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.711sec\n",
      "Epoch 34 tr: l 1.319 a 0.624  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.177638 14.760sec(train) 2.859sec(infer)\n",
      "Epoch 34 tst: l 1.198 a 0.675  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.476sec\n",
      "Epoch 35 tr: l 1.272 a 0.643  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.175862 15.788sec(train) 2.946sec(infer)\n",
      "Epoch 35 tst: l 1.157 a 0.685  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.655sec\n",
      "Epoch 36 tr: l 1.225 a 0.655  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.174103 14.531sec(train) 2.778sec(infer)\n",
      "Epoch 36 tst: l 1.158 a 0.680  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.452sec\n",
      "Epoch 37 tr: l 1.241 a 0.649  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.172362 14.584sec(train) 2.805sec(infer)\n",
      "Epoch 37 tst: l 1.132 a 0.688  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.459sec\n",
      "Epoch 38 tr: l 1.216 a 0.650  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.170639 14.810sec(train) 2.778sec(infer)\n",
      "Epoch 38 tst: l 1.129 a 0.687  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.412sec\n",
      "Epoch 39 tr: l 1.179 a 0.668  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.168932 14.810sec(train) 2.774sec(infer)\n",
      "Epoch 39 tst: l 1.116 a 0.693  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.526sec\n",
      "Epoch 40 tr: l 1.186 a 0.660  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.167243 14.646sec(train) 2.785sec(infer)\n",
      "Epoch 40 tst: l 1.103 a 0.694  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.446sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 41 tr: l 1.177 a 0.665  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.165571 14.695sec(train) 2.783sec(infer)\n",
      "Epoch 41 tst: l 1.080 a 0.703  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.384sec\n",
      "Epoch 42 tr: l 1.171 a 0.661  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.163915 14.603sec(train) 2.776sec(infer)\n",
      "Epoch 42 tst: l 1.077 a 0.698  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.427sec\n",
      "Epoch 43 tr: l 1.146 a 0.672  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.162276 14.708sec(train) 2.765sec(infer)\n",
      "Epoch 43 tst: l 1.060 a 0.704  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.474sec\n",
      "Epoch 44 tr: l 1.126 a 0.681  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.160653 14.585sec(train) 2.827sec(infer)\n",
      "Epoch 44 tst: l 1.062 a 0.705  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.927sec\n",
      "Epoch 45 tr: l 1.135 a 0.675  clct[np.int64(13), np.int64(7)] ans[np.int64(13), np.int64(7)] lr 0.159046 14.732sec(train) 2.889sec(infer)\n",
      "Epoch 45 tst: l 1.042 a 0.707  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.718sec\n",
      "Epoch 46 tr: l 1.114 a 0.683  clct[np.int64(13), np.int64(7)] ans[np.int64(13), np.int64(7)] lr 0.157456 15.974sec(train) 2.722sec(infer)\n",
      "Epoch 46 tst: l 1.040 a 0.714  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 4.944sec\n",
      "Epoch 47 tr: l 1.111 a 0.685  clct[np.int64(13), np.int64(7)] ans[np.int64(13), np.int64(7)] lr 0.155881 14.596sec(train) 2.477sec(infer)\n",
      "Epoch 47 tst: l 1.034 a 0.712  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 4.863sec\n",
      "Epoch 48 tr: l 1.112 a 0.678  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.154323 14.645sec(train) 2.493sec(infer)\n",
      "Epoch 48 tst: l 1.024 a 0.714  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 4.834sec\n",
      "Epoch 49 tr: l 1.028 a 0.708  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.152779 14.675sec(train) 2.535sec(infer)\n",
      "Epoch 49 tst: l 1.023 a 0.713  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 4.920sec\n",
      "Epoch 50 tr: l 1.102 a 0.680  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.151252 14.646sec(train) 2.476sec(infer)\n",
      "Epoch 50 tst: l 1.016 a 0.717  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 4.800sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 51 tr: l 1.123 a 0.676  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.149739 15.119sec(train) 2.524sec(infer)\n",
      "Epoch 51 tst: l 1.001 a 0.719  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 4.894sec\n",
      "Epoch 52 tr: l 1.086 a 0.690  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.148242 15.205sec(train) 2.494sec(infer)\n",
      "Epoch 52 tst: l 0.990 a 0.725  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 4.874sec\n",
      "Epoch 53 tr: l 1.072 a 0.688  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.146759 15.130sec(train) 2.453sec(infer)\n",
      "Epoch 53 tst: l 0.998 a 0.721  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 4.683sec\n",
      "Epoch 54 tr: l 1.061 a 0.694  clct[np.int64(7), np.int64(13)] ans[np.int64(7), np.int64(13)] lr 0.145292 15.035sec(train) 2.617sec(infer)\n",
      "Epoch 54 tst: l 0.986 a 0.724  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.193sec\n",
      "Epoch 55 tr: l 1.035 a 0.701  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.143839 15.361sec(train) 2.790sec(infer)\n",
      "Epoch 55 tst: l 0.988 a 0.724  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.291sec\n",
      "Epoch 56 tr: l 1.029 a 0.705  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.142400 15.015sec(train) 2.796sec(infer)\n",
      "Epoch 56 tst: l 0.973 a 0.727  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.615sec\n",
      "Epoch 57 tr: l 1.052 a 0.696  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.140976 14.897sec(train) 2.706sec(infer)\n",
      "Epoch 57 tst: l 0.967 a 0.731  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.173sec\n",
      "Epoch 58 tr: l 1.030 a 0.704  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.139567 14.981sec(train) 2.759sec(infer)\n",
      "Epoch 58 tst: l 0.983 a 0.722  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.422sec\n",
      "Epoch 59 tr: l 1.012 a 0.707  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.138171 15.484sec(train) 2.740sec(infer)\n",
      "Epoch 59 tst: l 0.963 a 0.730  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.542sec\n",
      "Epoch 60 tr: l 1.041 a 0.699  clct[np.int64(13), np.int64(7)] ans[np.int64(13), np.int64(7)] lr 0.136789 14.998sec(train) 2.791sec(infer)\n",
      "Epoch 60 tst: l 0.959 a 0.732  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.502sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 61 tr: l 0.986 a 0.722  clct[np.int64(6), np.int64(14)] ans[np.int64(6), np.int64(14)] lr 0.135421 14.992sec(train) 2.816sec(infer)\n",
      "Epoch 61 tst: l 0.955 a 0.733  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.466sec\n",
      "Epoch 62 tr: l 1.036 a 0.702  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.134067 15.138sec(train) 2.993sec(infer)\n",
      "Epoch 62 tst: l 0.947 a 0.734  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.815sec\n",
      "Epoch 63 tr: l 0.973 a 0.727  clct[np.int64(6), np.int64(14)] ans[np.int64(6), np.int64(14)] lr 0.132726 15.585sec(train) 2.942sec(infer)\n",
      "Epoch 63 tst: l 0.945 a 0.734  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.708sec\n",
      "Epoch 64 tr: l 1.039 a 0.701  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.131399 16.212sec(train) 3.004sec(infer)\n",
      "Epoch 64 tst: l 0.951 a 0.735  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.021sec\n",
      "Epoch 65 tr: l 1.001 a 0.711  clct[np.int64(7), np.int64(13)] ans[np.int64(7), np.int64(13)] lr 0.130085 15.547sec(train) 2.969sec(infer)\n",
      "Epoch 65 tst: l 0.943 a 0.735  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.735sec\n",
      "Epoch 66 tr: l 0.979 a 0.717  clct[np.int64(13), np.int64(7)] ans[np.int64(13), np.int64(7)] lr 0.128784 15.433sec(train) 2.846sec(infer)\n",
      "Epoch 66 tst: l 0.936 a 0.739  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.484sec\n",
      "Epoch 67 tr: l 1.004 a 0.711  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.127496 14.782sec(train) 2.800sec(infer)\n",
      "Epoch 67 tst: l 0.932 a 0.739  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.578sec\n",
      "Epoch 68 tr: l 1.028 a 0.706  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.126221 14.818sec(train) 3.109sec(infer)\n",
      "Epoch 68 tst: l 0.937 a 0.736  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.762sec\n",
      "Epoch 69 tr: l 0.937 a 0.730  clct[np.int64(7), np.int64(13)] ans[np.int64(7), np.int64(13)] lr 0.124959 14.647sec(train) 2.862sec(infer)\n",
      "Epoch 69 tst: l 0.922 a 0.743  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.614sec\n",
      "Epoch 70 tr: l 0.965 a 0.725  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.123710 15.337sec(train) 2.831sec(infer)\n",
      "Epoch 70 tst: l 0.927 a 0.743  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.561sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 71 tr: l 1.002 a 0.709  clct[np.int64(7), np.int64(13)] ans[np.int64(7), np.int64(13)] lr 0.122473 14.670sec(train) 2.817sec(infer)\n",
      "Epoch 71 tst: l 0.915 a 0.745  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.617sec\n",
      "Epoch 72 tr: l 0.949 a 0.732  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.121248 15.340sec(train) 2.911sec(infer)\n",
      "Epoch 72 tst: l 0.917 a 0.744  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.794sec\n",
      "Epoch 73 tr: l 0.956 a 0.727  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.120035 15.096sec(train) 2.884sec(infer)\n",
      "Epoch 73 tst: l 0.906 a 0.748  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.493sec\n",
      "Epoch 74 tr: l 0.928 a 0.740  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.118835 14.729sec(train) 2.786sec(infer)\n",
      "Epoch 74 tst: l 0.904 a 0.750  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.443sec\n",
      "Epoch 75 tr: l 0.957 a 0.729  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.117647 14.608sec(train) 2.798sec(infer)\n",
      "Epoch 75 tst: l 0.904 a 0.747  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.427sec\n",
      "Epoch 76 tr: l 0.964 a 0.727  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.116470 14.827sec(train) 2.765sec(infer)\n",
      "Epoch 76 tst: l 0.907 a 0.745  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.430sec\n",
      "Epoch 77 tr: l 0.943 a 0.730  clct[np.int64(13), np.int64(7)] ans[np.int64(13), np.int64(7)] lr 0.115305 14.734sec(train) 2.786sec(infer)\n",
      "Epoch 77 tst: l 0.891 a 0.752  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.553sec\n",
      "Epoch 78 tr: l 0.951 a 0.729  clct[np.int64(7), np.int64(13)] ans[np.int64(7), np.int64(13)] lr 0.114152 14.609sec(train) 2.765sec(infer)\n",
      "Epoch 78 tst: l 0.886 a 0.754  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.485sec\n",
      "Epoch 79 tr: l 0.921 a 0.739  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.113011 14.715sec(train) 2.850sec(infer)\n",
      "Epoch 79 tst: l 0.898 a 0.748  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.463sec\n",
      "Epoch 80 tr: l 0.963 a 0.722  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.111881 14.783sec(train) 2.878sec(infer)\n",
      "Epoch 80 tst: l 0.892 a 0.751  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.447sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 81 tr: l 0.932 a 0.733  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.110762 14.762sec(train) 2.840sec(infer)\n",
      "Epoch 81 tst: l 0.888 a 0.752  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.661sec\n",
      "Epoch 82 tr: l 0.902 a 0.746  clct[np.int64(15), np.int64(5)] ans[np.int64(15), np.int64(5)] lr 0.109654 15.074sec(train) 2.801sec(infer)\n",
      "Epoch 82 tst: l 0.886 a 0.754  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.516sec\n",
      "Epoch 83 tr: l 0.899 a 0.746  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.108558 14.589sec(train) 2.761sec(infer)\n",
      "Epoch 83 tst: l 0.885 a 0.754  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.486sec\n",
      "Epoch 84 tr: l 0.908 a 0.745  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.107472 14.599sec(train) 2.839sec(infer)\n",
      "Epoch 84 tst: l 0.873 a 0.756  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.519sec\n",
      "Epoch 85 tr: l 0.902 a 0.743  clct[np.int64(6), np.int64(14)] ans[np.int64(6), np.int64(14)] lr 0.106398 15.059sec(train) 2.951sec(infer)\n",
      "Epoch 85 tst: l 0.873 a 0.757  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.776sec\n",
      "Epoch 86 tr: l 0.901 a 0.740  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.105334 14.760sec(train) 2.801sec(infer)\n",
      "Epoch 86 tst: l 0.881 a 0.751  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.457sec\n",
      "Epoch 87 tr: l 0.890 a 0.752  clct[np.int64(14), np.int64(6)] ans[np.int64(14), np.int64(6)] lr 0.104280 18.233sec(train) 3.207sec(infer)\n",
      "Epoch 87 tst: l 0.871 a 0.758  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 6.483sec\n",
      "Epoch 88 tr: l 0.881 a 0.750  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.103237 16.484sec(train) 2.798sec(infer)\n",
      "Epoch 88 tst: l 0.873 a 0.756  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.573sec\n",
      "Epoch 89 tr: l 0.887 a 0.749  clct[np.int64(14), np.int64(6)] ans[np.int64(14), np.int64(6)] lr 0.102205 14.908sec(train) 2.947sec(infer)\n",
      "Epoch 89 tst: l 0.861 a 0.760  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.871sec\n",
      "Epoch 90 tr: l 0.898 a 0.745  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.101183 15.519sec(train) 2.918sec(infer)\n",
      "Epoch 90 tst: l 0.865 a 0.760  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.716sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 91 tr: l 0.919 a 0.737  clct[np.int64(11), np.int64(9)] ans[np.int64(11), np.int64(9)] lr 0.100171 15.210sec(train) 2.897sec(infer)\n",
      "Epoch 91 tst: l 0.866 a 0.759  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.886sec\n",
      "Epoch 92 tr: l 0.860 a 0.760  clct[np.int64(13), np.int64(7)] ans[np.int64(13), np.int64(7)] lr 0.099169 15.013sec(train) 2.816sec(infer)\n",
      "Epoch 92 tst: l 0.856 a 0.761  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.554sec\n",
      "Epoch 93 tr: l 0.906 a 0.743  clct[np.int64(12), np.int64(8)] ans[np.int64(12), np.int64(8)] lr 0.098178 14.705sec(train) 2.793sec(infer)\n",
      "Epoch 93 tst: l 0.855 a 0.763  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.443sec\n",
      "Epoch 94 tr: l 0.867 a 0.754  clct[np.int64(13), np.int64(7)] ans[np.int64(13), np.int64(7)] lr 0.097196 14.749sec(train) 2.763sec(infer)\n",
      "Epoch 94 tst: l 0.852 a 0.763  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.444sec\n",
      "Epoch 95 tr: l 0.818 a 0.769  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.096224 14.916sec(train) 2.799sec(infer)\n",
      "Epoch 95 tst: l 0.859 a 0.762  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.510sec\n",
      "Epoch 96 tr: l 0.841 a 0.759  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.095262 15.594sec(train) 2.871sec(infer)\n",
      "Epoch 96 tst: l 0.851 a 0.766  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.414sec\n",
      "Epoch 97 tr: l 0.882 a 0.750  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.094309 14.889sec(train) 2.978sec(infer)\n",
      "Epoch 97 tst: l 0.848 a 0.766  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.835sec\n",
      "Epoch 98 tr: l 0.817 a 0.776  clct[np.int64(14), np.int64(6)] ans[np.int64(14), np.int64(6)] lr 0.093366 15.369sec(train) 2.917sec(infer)\n",
      "Epoch 98 tst: l 0.847 a 0.764  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.960sec\n",
      "Epoch 99 tr: l 0.869 a 0.755  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.092432 14.768sec(train) 2.833sec(infer)\n",
      "Epoch 99 tst: l 0.853 a 0.762  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.555sec\n",
      "Epoch 100 tr: l 0.858 a 0.757  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.091508 14.806sec(train) 2.807sec(infer)\n",
      "Epoch 100 tst: l 0.842 a 0.765  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.932sec\n",
      "result written at output\\results.pickle\n",
      "Epoch 101 tr: l 0.836 a 0.762  clct[np.int64(4), np.int64(16)] ans[np.int64(4), np.int64(16)] lr 0.090593 14.711sec(train) 2.854sec(infer)\n",
      "Epoch 101 tst: l 0.839 a 0.767  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.492sec\n",
      "Epoch 102 tr: l 0.875 a 0.751  clct[np.int64(10), np.int64(10)] ans[np.int64(10), np.int64(10)] lr 0.089687 15.611sec(train) 3.154sec(infer)\n",
      "Epoch 102 tst: l 0.846 a 0.767  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.688sec\n",
      "Epoch 103 tr: l 0.840 a 0.766  clct[np.int64(8), np.int64(12)] ans[np.int64(8), np.int64(12)] lr 0.088790 14.829sec(train) 2.834sec(infer)\n",
      "Epoch 103 tst: l 0.832 a 0.771  clct[np.int64(20), np.int64(20)] ans[np.int64(20), np.int64(20)] 5.488sec\n",
      "Epoch 104 tr: l 0.832 a 0.769  clct[np.int64(9), np.int64(11)] ans[np.int64(9), np.int64(11)] lr 0.087902 14.842sec(train) 2.856sec(infer)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m exp \u001b[38;5;241m=\u001b[39m TrainCIFARCluster(config)\n\u001b[0;32m      6\u001b[0m exp\u001b[38;5;241m.\u001b[39msetup()\n\u001b[1;32m----> 7\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 286\u001b[0m, in \u001b[0;36mTrainCIFARCluster.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_epoch_stats(res)\n\u001b[0;32m    285\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 286\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    288\u001b[0m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfer_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m t1\u001b[38;5;241m-\u001b[39mt0\n",
      "Cell \u001b[1;32mIn[4], line 521\u001b[0m, in \u001b[0;36mTrainCIFARCluster.test\u001b[1;34m(self, train, force_full_nodes)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m_i \u001b[38;5;129;01min\u001b[39;00m participating_nodes:\n\u001b[0;32m    520\u001b[0m     t00\u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 521\u001b[0m     (X, y) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_node_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# load batch data rotated\u001b[39;00m\n\u001b[0;32m    522\u001b[0m     t01\u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    523\u001b[0m     t_load_data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m t01\u001b[38;5;241m-\u001b[39mt00\n",
      "Cell \u001b[1;32mIn[4], line 598\u001b[0m, in \u001b[0;36mTrainCIFARCluster.load_node_data\u001b[1;34m(self, m_i, train)\u001b[0m\n\u001b[0;32m    594\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    596\u001b[0m indices \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_indices\u001b[39m\u001b[38;5;124m'\u001b[39m][m_i]\n\u001b[1;32m--> 598\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data_by_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 634\u001b[0m, in \u001b[0;36mTrainCIFARCluster.load_data_by_index\u001b[1;34m(self, indices, m_i, train)\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly p=1,2,4 supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    632\u001b[0m X_b2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrot90(X_b, k\u001b[38;5;241m=\u001b[39mk, axes \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;66;03m# X_b: (bs, 32, 32, 3)\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m X_b3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_tr_pl\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_b2\u001b[49m\u001b[43m \u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (X_b3, y_b)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\dl-new\\lib\\site-packages\\tensorflow\\python\\client\\session.py:977\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    974\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 977\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    979\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m    980\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\dl-new\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1220\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[1;32m-> 1220\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1223\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32me:\\anaconda\\envs\\dl-new\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1400\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1397\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1400\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1401\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1403\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\dl-new\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1407\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m   1406\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1408\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1409\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[1;32me:\\anaconda\\envs\\dl-new\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1390\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[0;32m   1388\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[0;32m   1389\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[1;32m-> 1390\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\anaconda\\envs\\dl-new\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1483\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1482\u001b[0m                         run_metadata):\n\u001b[1;32m-> 1483\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1485\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "config['train_seed'] = config['data_seed']\n",
    "print(\"config:\",config)\n",
    "\n",
    "exp = TrainCIFARCluster(config)\n",
    "exp.setup()\n",
    "exp.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
