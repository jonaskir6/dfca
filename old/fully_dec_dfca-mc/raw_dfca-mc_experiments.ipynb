{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "OHJWesKs-tqd"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import itertools\n",
        "import pickle\n",
        "import copy\n",
        "import random\n",
        "import math\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from util import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAghP_o0-tqe"
      },
      "source": [
        "Reads Config file and prepares the arguments you can choose in the config.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "BbUZJ2E--tqe"
      },
      "outputs": [],
      "source": [
        "LR_DECAY = False\n",
        "def get_config():\n",
        "\n",
        "    # read config json and update the sysarg\n",
        "    with open(\"config.json\", \"r\") as read_file:\n",
        "        config = json.load(read_file)\n",
        "\n",
        "    if config[\"config_override\"] == \"\":\n",
        "        del config['config_override']\n",
        "    else:\n",
        "        print(config['config_override'])\n",
        "        config_override = json.loads(config['config_override'])\n",
        "        del config['config_override']\n",
        "        config.update(config_override)\n",
        "\n",
        "    return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-J1YEoM-tqe"
      },
      "source": [
        "Class SimpleLinear with simple MLP for MNIST Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "id5Wyt-V-tqf"
      },
      "outputs": [],
      "source": [
        "class SimpleLinear(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, h1=2048):\n",
        "        super().__init__()\n",
        "        self.fc1 = torch.nn.Linear(28*28, h1)\n",
        "        self.fc2 = torch.nn.Linear(h1, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # x = F.sigmoid(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    # def weight(self):\n",
        "    #     return self.linear1.weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qFBH01M-tqf"
      },
      "source": [
        "Class TrainMNISTCluster with all the methods needed to run the experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkGiGQ2G-tqf"
      },
      "outputs": [],
      "source": [
        "# get_inference_stats not updated yet, iterative clustering doesnt work when decentralizing everything. Clients would only have their models instead of all models, so you cant\n",
        "# run inference on different models to check cluster estimates.\n",
        "\n",
        "\n",
        "class TrainMNISTCluster(object):\n",
        "    def __init__(self, config, device):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        assert self.config['m'] % self.config['p'] == 0\n",
        "\n",
        "    def setup(self):\n",
        "\n",
        "        os.makedirs(self.config['project_dir'], exist_ok = True)\n",
        "\n",
        "        self.result_fname = os.path.join(self.config['project_dir'], 'results.pickle')\n",
        "        self.checkpoint_fname = os.path.join(self.config['project_dir'], 'checkpoint.pt')\n",
        "\n",
        "        self.setup_datasets()\n",
        "        self.setup_models()\n",
        "\n",
        "        self.epoch = None\n",
        "        self.lr = None\n",
        "\n",
        "\n",
        "    def setup_datasets(self):\n",
        "\n",
        "        np.random.seed(self.config['data_seed'])\n",
        "\n",
        "        # generate indices for each dataset\n",
        "        # also write cluster info\n",
        "\n",
        "        MNIST_TRAINSET_DATA_SIZE = 60000\n",
        "        MNIST_TESTSET_DATA_SIZE = 10000\n",
        "\n",
        "        np.random.seed(self.config['data_seed'])\n",
        "\n",
        "        cfg = self.config\n",
        "\n",
        "        self.dataset = {}\n",
        "\n",
        "        if cfg['uneven'] == True:\n",
        "            dataset = {}\n",
        "            dataset['data_indices'], dataset['cluster_assign'] = \\\n",
        "                self._setup_dataset_random_n(MNIST_TRAINSET_DATA_SIZE, cfg['p'], cfg['m'], cfg['n'])\n",
        "            (X, y) = self._load_MNIST(train=True)\n",
        "            dataset['X'] = X\n",
        "            dataset['y'] = y\n",
        "            self.dataset['train'] = dataset\n",
        "\n",
        "            dataset = {}\n",
        "            dataset['data_indices'], dataset['cluster_assign'] = \\\n",
        "                self._setup_dataset_random_n(MNIST_TESTSET_DATA_SIZE, cfg['p'], cfg['m_test'], cfg['n'], random=True)\n",
        "            (X, y) = self._load_MNIST(train=False)\n",
        "            dataset['X'] = X\n",
        "            dataset['y'] = y\n",
        "            self.dataset['test'] = dataset\n",
        "\n",
        "        else:\n",
        "            dataset = {}\n",
        "            dataset['data_indices'], dataset['cluster_assign'] = \\\n",
        "                self._setup_dataset(MNIST_TRAINSET_DATA_SIZE, cfg['p'], cfg['m'], cfg['n'])\n",
        "            (X, y) = self._load_MNIST(train=True)\n",
        "            dataset['X'] = X\n",
        "            dataset['y'] = y\n",
        "            self.dataset['train'] = dataset\n",
        "\n",
        "            dataset = {}\n",
        "            dataset['data_indices'], dataset['cluster_assign'] = \\\n",
        "                self._setup_dataset(MNIST_TESTSET_DATA_SIZE, cfg['p'], cfg['m_test'], cfg['n'], random=True)\n",
        "            (X, y) = self._load_MNIST(train=False)\n",
        "            dataset['X'] = X\n",
        "            dataset['y'] = y\n",
        "            self.dataset['test'] = dataset\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "\n",
        "    def _setup_dataset(self, num_data, p, m, n, random = True):\n",
        "\n",
        "        print(\"m:\",m)\n",
        "        print(\"p:\",p)\n",
        "        print(\"n:\",n)\n",
        "        print(\"num_data:\",num_data)\n",
        "        assert (m // p) * n == num_data\n",
        "\n",
        "        dataset = {}\n",
        "\n",
        "        cfg = self.config\n",
        "\n",
        "        data_indices = []\n",
        "        cluster_assign = [[] for _ in range(m)]\n",
        "\n",
        "        m_per_cluster = m // p\n",
        "\n",
        "        for p_i in range(p):\n",
        "\n",
        "            if random:\n",
        "                ll = list(np.random.permutation(num_data))\n",
        "            else:\n",
        "                ll = list(range(num_data))\n",
        "\n",
        "            ll2 = chunkify(ll, m_per_cluster) # splits ll into m lists with size n\n",
        "            data_indices += ll2\n",
        "\n",
        "            for i in range(m_per_cluster):\n",
        "                cluster_assign[p_i * m_per_cluster + i].append(p_i)\n",
        "        \n",
        "        for m_i in range(m):\n",
        "            p_i_ = cluster_assign[m_i]\n",
        "            for i in range(cfg['k\\'']):\n",
        "                if random.random() < 0.2:  # 20% chance\n",
        "                    if i + p_i_[0] > 3:\n",
        "                        cluster_assign[m_i].append(0)\n",
        "                    else:\n",
        "                        cluster_assign[m_i].append(p_i[0] + i)\n",
        "        \n",
        "        print(cluster_assign)\n",
        "\n",
        "        data_indices = np.array(data_indices)\n",
        "        # cluster_assign = np.array(cluster_assign, dtype=object)\n",
        "        #assert data_indices.shape[0] == cluster_assign.shape[0]\n",
        "\n",
        "        return data_indices, cluster_assign\n",
        "    \n",
        "    def _setup_dataset_random_n(self, num_data, p, m, n, random = True):\n",
        "\n",
        "        print(\"m:\",m)\n",
        "        print(\"p:\",p)\n",
        "        print(\"num_data:\",num_data)\n",
        "\n",
        "        dataset = {}\n",
        "\n",
        "        cfg = self.config\n",
        "\n",
        "        data_indices = []\n",
        "        cluster_assign = [[] for _ in range(m)]\n",
        "\n",
        "        m_per_cluster = m // p\n",
        "\n",
        "        for p_i in range(p):\n",
        "\n",
        "            ll = list(np.random.permutation(num_data))\n",
        "\n",
        "            ll2 = chunkify_uneven(ll, m_per_cluster) # splits ll into m lists\n",
        "            data_indices += ll2\n",
        "\n",
        "            for i in range(m_per_cluster):\n",
        "                cluster_assign[p_i * m_per_cluster + i].append(p_i)\n",
        "\n",
        "        data_indices = np.array(data_indices, dtype=object)\n",
        "        # cluster_assign = np.array(cluster_assign, dtype=object)\n",
        "        #assert data_indices.shape[0] == cluster_assign.shape[0]\n",
        "        assert data_indices.shape[0] == m\n",
        "\n",
        "\n",
        "        return data_indices, cluster_assign\n",
        "\n",
        "\n",
        "    def _load_MNIST(self, train=True):\n",
        "        transforms = torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               # torchvision.transforms.Normalize(\n",
        "                               #   (0.1307,), (0.3081,))\n",
        "                             ])\n",
        "        if train:\n",
        "            mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms)\n",
        "        else:\n",
        "            mnist_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms)\n",
        "\n",
        "        dl = DataLoader(mnist_dataset)\n",
        "\n",
        "        X = dl.dataset.data # (60000,28, 28)\n",
        "        y = dl.dataset.targets #(60000)\n",
        "\n",
        "        # normalize to have 0 ~ 1 range in each pixel\n",
        "\n",
        "        X = X / 255.0\n",
        "        X = X.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "\n",
        "    # Need p models for each client\n",
        "\n",
        "    def setup_models(self):\n",
        "        np.random.seed(self.config['train_seed'])\n",
        "        torch.manual_seed(self.config['train_seed'])\n",
        "\n",
        "        p = self.config['p']\n",
        "        m = self.config['m']\n",
        "\n",
        "        self.models = [[SimpleLinear(h1 = self.config['h1']).to(self.device) for p_i in range(p)] for m_i in range(m)] # p models with p different params of dimension(1,d) for each client m_i\n",
        "\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        num_epochs = self.config['num_epochs']\n",
        "        lr = self.config['lr']\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # epoch -1\n",
        "        self.epoch = -1\n",
        "\n",
        "        result = {}\n",
        "        result['epoch'] = -1\n",
        "\n",
        "        t0 = time.time()\n",
        "        res = self.test(train=True)\n",
        "        t1 = time.time()\n",
        "        res['infer_time'] = t1-t0\n",
        "        result['train'] = res\n",
        "\n",
        "        self.print_epoch_stats(res)\n",
        "\n",
        "        t0 = time.time()\n",
        "        res = self.test(train=False)\n",
        "        t1 = time.time()\n",
        "        res['infer_time'] = t1-t0\n",
        "        result['test'] = res\n",
        "        self.print_epoch_stats(res)\n",
        "        results.append(result)\n",
        "\n",
        "        # this will be used in next epoch\n",
        "        cluster_assign = result['train']['cluster_assign']\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            self.epoch = epoch\n",
        "\n",
        "            result = {}\n",
        "            result['epoch'] = epoch\n",
        "\n",
        "            lr = self.lr_schedule(epoch)\n",
        "            result['lr'] = lr\n",
        "\n",
        "            t0 = time.time()\n",
        "            result['train'] = self.train(cluster_assign, lr = lr)\n",
        "            t1 = time.time()\n",
        "            train_time = t1-t0\n",
        "\n",
        "            t0 = time.time()\n",
        "            res = self.test(train=True)\n",
        "            t1 = time.time()\n",
        "            res['infer_time'] = t1-t0\n",
        "            res['train_time'] = train_time\n",
        "            res['lr'] = lr\n",
        "            result['train'] = res\n",
        "\n",
        "            self.print_epoch_stats(res)\n",
        "\n",
        "            t0 = time.time()\n",
        "            res = self.test(train=False)\n",
        "            t1 = time.time()\n",
        "            res['infer_time'] = t1-t0\n",
        "            result['test'] = res\n",
        "            self.print_epoch_stats(res)\n",
        "\n",
        "            results.append(result)\n",
        "\n",
        "            # this will be used in next epoch's gradient update\n",
        "            cluster_assign = result['train']['cluster_assign']\n",
        "\n",
        "            if epoch % 10 == 0 or epoch == num_epochs - 1 :\n",
        "                with open(self.result_fname, 'wb') as outfile:\n",
        "                    pickle.dump(results, outfile)\n",
        "                    print(f'result written at {self.result_fname}')\n",
        "                self.save_checkpoint()\n",
        "                print(f'checkpoint written at {self.checkpoint_fname}')\n",
        "\n",
        "        plt.figure(figsize=(10,5))\n",
        "        plt.plot([r['train']['loss'] for r in results], label='train')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('loss')\n",
        "        plt.title('Training Loss per Epoch')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(os.path.join(self.config['project_dir'], 'train_loss.png'))\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "        plt.figure(figsize=(10,5))\n",
        "        plt.plot([r['test']['acc'] for r in results], label='train')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('test accuracy')\n",
        "        plt.title('Test Accuracy per Epoch')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(os.path.join(self.config['project_dir'], 'test_acc.png'))\n",
        "\n",
        "\n",
        "    def lr_schedule(self, epoch):\n",
        "        if self.lr is None:\n",
        "            self.lr = self.config['lr']\n",
        "\n",
        "        if epoch % 50 == 0 and epoch != 0 and LR_DECAY:\n",
        "            self.lr = self.lr * 0.1\n",
        "\n",
        "        return self.lr\n",
        "\n",
        "\n",
        "    def print_epoch_stats(self, res):\n",
        "        if res['is_train']:\n",
        "            data_str = 'tr'\n",
        "        else:\n",
        "            data_str = 'tst'\n",
        "\n",
        "        if 'train_time' in res:\n",
        "            time_str = f\"{res['train_time']:.3f}sec(train) {res['infer_time']:.3f}sec(infer)\"\n",
        "        else:\n",
        "            time_str = f\"{res['infer_time']:.3f}sec\"\n",
        "\n",
        "        if 'lr' in res:\n",
        "            lr_str = f\" lr {res['lr']:4f}\"\n",
        "        else:\n",
        "            lr_str = \"\"\n",
        "\n",
        "        str0 = f\"Epoch {self.epoch} {data_str}: l {res['loss']:.3f} a {res['acc']:.3f} clct{res['cl_ct']}{lr_str} {time_str}\"\n",
        "\n",
        "        print(str0)\n",
        "\n",
        "    def train(self, cluster_assign, lr):\n",
        "        VERBOSE = 0\n",
        "\n",
        "        cfg = self.config\n",
        "        m = cfg['m']\n",
        "        p = cfg['p']\n",
        "        tau = cfg['tau']\n",
        "\n",
        "        # run local update\n",
        "        t0 = time.time()\n",
        "\n",
        "\n",
        "        for m_i in range(m):\n",
        "            if VERBOSE and m_i % 100 == 0: print(f'm {m_i}/{m} processing \\r', end ='')\n",
        "\n",
        "            (X, y) = self.load_data(m_i)\n",
        "\n",
        "            for p_i in range(p):\n",
        "\n",
        "                model = self.models[m_i][p_i]\n",
        "\n",
        "                # LOCAL UPDATE PER MACHINE tau times\n",
        "                for step_i in range(tau):\n",
        "\n",
        "                    y_logit = model(X)\n",
        "                    loss = self.criterion(y_logit, y)\n",
        "\n",
        "                    model.zero_grad()\n",
        "                    loss.backward()\n",
        "                    self.local_param_update(model, lr)\n",
        "\n",
        "                model.zero_grad()\n",
        "\n",
        "        t02 = time.time()\n",
        "        # print(f'running single ..took {t02-t01:.3f}sec')\n",
        "\n",
        "\n",
        "        t1 = time.time()\n",
        "        if VERBOSE: print(f'local update {t1-t0:.3f}sec')\n",
        "\n",
        "        # apply gradient update\n",
        "        t0 = time.time()\n",
        "\n",
        "        local_models = [[] for _ in range(p)]\n",
        "\n",
        "        for m_i in range(m):\n",
        "            assigned_clusters = cluster_assign[m_i]\n",
        "            for c in assigned_clusters:\n",
        "                local_models[c].append(m_i)\n",
        "\n",
        "        # NEEDS TO BE DECENTRALIZED\n",
        "        for p_i, models in enumerate(local_models):\n",
        "            if len(models) > 1:\n",
        "                self.dec_param_update(models, p_i)\n",
        "        t1 = time.time()\n",
        "\n",
        "        if VERBOSE: print(f'global update {t1-t0:.3f}sec')\n",
        "\n",
        "    def check_local_model_loss(self, local_models):\n",
        "        # for debugging\n",
        "        m = self.config['m']\n",
        "\n",
        "        losses = []\n",
        "        for m_i in range(m):\n",
        "            (X, y) = self.load_data(m_i)\n",
        "            y_logit = local_models[m_i](X)\n",
        "            loss = self.criterion(y_logit, y)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        return np.array(losses)\n",
        "\n",
        "\n",
        "    def get_inference_stats(self, train = True):\n",
        "        cfg = self.config\n",
        "        if train:\n",
        "            m = cfg['m']\n",
        "            dataset = self.dataset['train']\n",
        "        else:\n",
        "            m = cfg['m_test']\n",
        "            dataset = self.dataset['test']\n",
        "\n",
        "        p = cfg['p']\n",
        "\n",
        "\n",
        "        num_data = 0\n",
        "        losses = {}\n",
        "        corrects = {}\n",
        "        for m_i in range(m):\n",
        "            (X, y) = self.load_data(m_i, train=train) # load batch data rotated\n",
        "\n",
        "            for p_i in range(p):\n",
        "                y_logit = self.models[p_i](X)\n",
        "                loss = self.criterion(y_logit, y) # loss of\n",
        "                n_correct = self.n_correct(y_logit, y)\n",
        "\n",
        "                # if torch.isnan(loss):\n",
        "                #     print(\"nan loss: \", dataset['data_indices'][m_i])\n",
        "\n",
        "                losses[(m_i,p_i)] = loss.item()\n",
        "                corrects[(m_i,p_i)] = n_correct\n",
        "\n",
        "            num_data += X.shape[0]\n",
        "\n",
        "        # calculate loss and cluster the machines\n",
        "        cluster_assign = [[] for _ in range(m)]\n",
        "        for m_i in range(m):\n",
        "            machine_losses = [ losses[(m_i,p_i)] for p_i in range(p) ]\n",
        "            min_p_i = np.argmin(machine_losses)\n",
        "            cp_machine_losses = copy.deepcopy(machine_losses)\n",
        "            cp_machine_losses.pop(min_p_i)\n",
        "            sec_min_p_i = np.argmin(cp_machine_losses)\n",
        "            rho = (machine_losses[min_p_i] + 0.1*(abs(machine_losses[min_p_i] - machine_losses[sec_min_p_i])))\n",
        "            cnt = 0\n",
        "            assigned_clusters = set()\n",
        "            while cnt < cfg[\"k'\"]:\n",
        "                for p_i in range(p):\n",
        "                    if machine_losses[p_i] <= rho and p_i not in assigned_clusters:\n",
        "                        cluster_assign[m_i].append(p_i)\n",
        "                        assigned_clusters.add(p_i)\n",
        "                        break\n",
        "                cnt += 1\n",
        "        print(len(cluster_assign))\n",
        "        print(cluster_assign)\n",
        "\n",
        "        # calculate optimal model's loss, acc over all models\n",
        "        min_corrects = []\n",
        "        min_losses = []\n",
        "        for m_i, p_i_list in enumerate(cluster_assign):\n",
        "            for p_i in p_i_list:\n",
        "                min_loss = losses[(m_i, p_i)]\n",
        "                min_losses.append(min_loss)\n",
        "\n",
        "                min_correct = corrects[(m_i, p_i)]\n",
        "                min_corrects.append(min_correct)\n",
        "\n",
        "        # print(\"losses: \", min_losses)\n",
        "        loss = np.mean(min_losses)\n",
        "        acc = np.sum(min_corrects) / num_data\n",
        "\n",
        "\n",
        "        # check cluster assignment acc\n",
        "        cl_acc = np.mean([set(cluster_assign[m_i]) == set(dataset['cluster_assign'][m_i]) for m_i in range(m)])\n",
        "        cl_ct = [np.sum([p_i in cluster_assign[m_i] for m_i in range(m)]) for p_i in range(p)]\n",
        "\n",
        "        res = {} # results\n",
        "        # res['losses'] = losses\n",
        "        # res['corrects'] = corrects\n",
        "        res['cluster_assign'] = cluster_assign\n",
        "        res['num_data'] = num_data\n",
        "        res['loss'] = loss\n",
        "        res['acc'] = acc\n",
        "        res['cl_acc'] = cl_acc\n",
        "        res['cl_ct'] = cl_ct\n",
        "        res['is_train'] = train\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "        return res\n",
        "    def n_correct(self, y_logit, y):\n",
        "        _, predicted = torch.max(y_logit.data, 1)\n",
        "        correct = (predicted == y).sum().item()\n",
        "\n",
        "        return correct\n",
        "\n",
        "    # TODO Does every Cluster get 4 clients with the same data, but rotated differently?\n",
        "\n",
        "    def load_data(self, m_i, train=True):\n",
        "        # this part is very fast since its just rearranging models\n",
        "        cfg = self.config\n",
        "\n",
        "        if train:\n",
        "            dataset = self.dataset['train']\n",
        "        else:\n",
        "            dataset = self.dataset['test']\n",
        "\n",
        "        indices = dataset['data_indices'][m_i]\n",
        "        p_i = dataset['cluster_assign'][m_i]\n",
        "\n",
        "        X_batch = dataset['X'][indices]\n",
        "        y_batch = dataset['y'][indices]\n",
        "\n",
        "        # k : how many times rotate 90 degree\n",
        "        # k =1 : 90 , k=2 180, k=3 270\n",
        "        if cfg['p'] == 4:\n",
        "            k = p_i[0]\n",
        "        elif cfg['p'] == 2:\n",
        "            k = (p_i[0] % 2) * 2\n",
        "        elif cfg['p'] == 1:\n",
        "            k = 0\n",
        "        else:\n",
        "            raise NotImplementedError(\"only p=1,2,4 supported\")\n",
        "\n",
        "        X_batch2 = torch.rot90(X_batch, k=int(k), dims=(1, 2))\n",
        "        X_batch3 = X_batch2.reshape(-1, 28 * 28)\n",
        "\n",
        "        if len(p_i) > 1:\n",
        "            additional_X_batches = []\n",
        "            additional_y_batches = []       \n",
        "\n",
        "            for i in range(p_i-1):\n",
        "                additional_rotation = (p_i[0] + i) % 4\n",
        "                X_batch_additional_rot = torch.rot90(X_batch, k=additional_rotation, dims=(1, 2))\n",
        "                X_batch_additional_rot = X_batch_additional_rot.reshape(-1, 28 * 28)\n",
        "                additional_X_batches.append(X_batch_additional_rot)\n",
        "                additional_y_batches.append(y_batch)\n",
        "\n",
        "            X_batch3 = torch.cat([X_batch3] + additional_X_batches, dim=0)\n",
        "            y_batch = torch.cat([y_batch] + additional_y_batches, dim=0)\n",
        "\n",
        "        return X_batch3, y_batch\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "\n",
        "    def local_param_update(self, model, lr):\n",
        "\n",
        "        # gradient update manually\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                param.data -= lr * param.grad\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        # import ipdb; ipdb.set_trace() # we need to check the output of name, check if duplicate exists\n",
        "\n",
        "\n",
        "    def dec_param_update(self, local_models, p_i):\n",
        "\n",
        "        num_clients = len(local_models)\n",
        "\n",
        "        max_e = 100\n",
        "        if num_clients <= max_e:\n",
        "            e = num_clients - 1\n",
        "        else:\n",
        "            e = min(max_e, int(np.log(num_clients) * 10))\n",
        "\n",
        "        if e >= num_clients:\n",
        "            e = num_clients - 1\n",
        "\n",
        "        client_indices = list(range(num_clients))\n",
        "\n",
        "        for m_i in (local_models):\n",
        "            selected_clients = random.sample([i for i in client_indices if i != m_i], e)\n",
        "\n",
        "            for m_j in selected_clients:\n",
        "\n",
        "                m_j_params = dict(self.models[local_models[m_j]][p_i].named_parameters())\n",
        "\n",
        "                for name, param in self.models[local_models[m_i]][p_i].named_parameters():\n",
        "                    m_i_param = param.data.clone()\n",
        "                    m_j_param = m_j_params[name].data.clone()\n",
        "                    param.data = (m_i_param + m_j_param) / 2\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "\n",
        "    def test(self, train=False):\n",
        "        return self.get_inference_stats(train=train)\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        models_to_save = [model.state_dict() for model in self.models]\n",
        "        torch.save({'models':models_to_save}, self.checkpoint_fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ADsUSUi-tqf"
      },
      "source": [
        "Running the Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T_XDv25r-tqf",
        "outputId": "9c8f4300-c792-4e49-be40-c694fa066e6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config: {'m': 1200, 'm_test': 200, 'p': 4, 'n': 200, 'uneven': True, \"k'\": 2, 'h1': 200, 'num_epochs': 300, 'batch_size': 100, 'tau': 10, 'lr': 0.1, 'data_seed': 10, 'train_seed': 10, 'project_dir': 'output'}\n",
            "Using device: cuda\n",
            "m: 1200\n",
            "p: 4\n",
            "num_data: 60000\n",
            "len:  60000\n",
            "len:  60000\n",
            "len:  60000\n",
            "len:  60000\n",
            "m: 200\n",
            "p: 4\n",
            "num_data: 10000\n",
            "len:  10000\n",
            "len:  10000\n",
            "len:  10000\n",
            "len:  10000\n",
            "1200\n",
            "[[0], [0, 3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [1], [0], [0], [0], [0], [3], [0], [2], [0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [3], [1], [0], [0], [3], [0], [1], [0], [2], [2], [0], [0], [0], [0], [1], [3], [0], [2], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [0], [1], [3], [3], [0, 2], [0], [3], [0], [0], [0], [1], [0], [0], [0], [3], [0], [0], [3], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [0], [3], [0], [0], [1], [0], [3], [0], [3], [0], [3], [3], [0], [0], [0], [0], [0], [1], [0], [0], [2], [3], [0], [0], [0], [3], [0], [0], [0], [0], [3], [0], [0], [0], [1], [0, 3], [0], [0], [0], [1], [3], [0, 3], [0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [3], [0], [0], [3], [2], [0], [3], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [3], [0], [0, 3], [0], [0], [0], [0], [3], [0], [3], [0], [0], [0], [0], [0], [1, 3], [0], [0], [0], [0], [3], [0], [3], [0], [3], [0], [1], [0], [0], [0], [0], [0], [0], [2], [3], [0], [2], [0], [0], [0], [0], [3], [0], [0, 2], [3], [0], [0], [1], [2], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [3], [0], [0], [3], [3], [0], [3], [0], [0], [3], [2], [0], [0], [0], [1], [0], [3], [0], [0], [0], [1, 3], [3], [0], [0], [0], [0], [0], [0], [3], [0], [0], [0, 2], [0], [0], [0], [0], [3], [3], [3], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [0], [0], [0], [1], [0], [0], [1], [3], [0], [2], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [2], [3], [1], [2], [3], [3], [3], [1], [2], [3], [3], [3], [2], [3], [3], [1], [0], [2], [3], [3], [3], [1], [3], [1], [2], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [3], [3], [3], [3], [2], [3], [3], [3], [3], [3], [3], [0, 3], [3], [1], [2], [3], [3], [3], [3], [3], [3], [3], [2], [2], [3], [3], [1], [2], [3], [3], [3], [3], [3], [3], [3], [2], [3], [3], [3], [2], [3], [3], [3], [3], [2], [3], [3], [1], [3], [2], [3], [3], [3], [3], [3], [1], [3], [3], [2], [3], [3], [1], [3], [3], [3], [3], [3], [2], [3], [3], [3], [3], [1], [3], [1], [3], [3], [3], [3], [2], [2], [3], [3], [3], [2], [2], [3], [3], [3], [3], [1], [3], [3], [3], [3], [3], [2], [3], [1], [3], [3], [3], [0], [3], [1], [1], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [3], [3], [0], [3], [2], [3], [2], [3], [1], [3], [2], [3], [3], [3], [1], [2], [3], [3], [3], [1], [3], [3], [3], [3], [3], [1], [3], [1], [3], [1], [0, 3], [2], [3], [0], [1], [3], [3], [3], [3], [2], [3], [3], [3], [3], [2], [2], [3], [1], [3], [3], [3], [3], [3], [1], [2], [3], [0], [3], [3], [3], [1, 3], [1], [3], [3], [3], [1], [3], [2], [2], [3], [2], [3], [3], [3], [3], [2], [3], [3], [3], [3], [3], [2], [3], [2], [3], [3], [0], [3], [3], [3], [1], [3], [0], [3], [2], [3], [3], [3], [3], [3], [2], [3], [1], [3], [0], [3], [2], [3], [3], [2], [2], [3], [3], [3], [2], [3], [3], [3], [3], [2], [3], [2], [3], [3], [3], [3], [3], [2], [2], [3], [3], [3], [1], [3], [3], [3], [3], [2], [3], [0], [3], [1], [2], [3], [0], [0], [3], [1], [3], [1], [1], [2], [1], [3], [1], [3], [2], [3], [2], [1], [1], [2], [1], [1], [2], [1], [1], [3], [2], [3], [2], [1], [1], [1, 3], [3], [2], [1], [1], [2], [1], [1], [1], [1, 3], [2], [1], [2], [1], [3], [1], [1], [3], [2], [3], [3], [1], [1], [1], [1], [2], [2], [1], [3], [1], [1], [3], [0], [3], [1], [1], [2], [2], [3], [1], [1], [1], [1], [2], [1], [1], [1], [1], [1], [2], [1], [3], [2], [1], [3], [1], [2], [1], [1], [3], [1], [1], [1], [2], [3], [1], [3], [3], [2], [3], [1], [1], [1], [1, 3], [2], [3], [3], [2], [1], [1], [1], [3], [2], [1], [3], [2], [2], [1], [1], [1], [1], [1], [1], [2], [1], [2], [1, 3], [2], [1], [3], [1], [3], [2], [3], [1], [2], [3], [3], [1], [3], [1], [1, 3], [2], [2], [1], [2], [3], [3], [3], [2], [1], [1], [1, 3], [2], [2], [1], [1], [1], [1], [1], [1], [1], [3], [1], [1], [1], [2], [1], [1], [1], [3], [1], [3], [1], [3], [1], [3], [2], [1], [1], [1], [1], [2], [2], [2], [1], [1], [1], [1], [2], [1], [1], [2], [2], [2], [1], [3], [1], [2], [2], [1], [1], [1], [2], [3], [2], [1], [1], [1], [2], [3], [2], [3], [1], [3], [3], [3], [1], [3], [1, 3], [1], [1], [2], [1], [1], [3], [3], [3], [3], [1], [1], [1], [1], [0], [2], [2], [1], [3], [1], [1], [3], [0], [0], [1], [1], [2], [1], [3], [1], [2], [3], [1], [1], [1], [3], [1, 3], [2], [1], [1], [2], [1, 3], [1], [2], [3], [1], [1], [1], [1], [2], [1], [2], [3], [1], [1], [1], [2], [2], [1], [2], [3], [3], [2], [3], [3], [2], [2], [1], [0], [2], [2], [2], [1], [2], [1], [1], [2], [1], [0, 2], [2], [1], [3], [1, 3], [1], [2], [1], [0], [0], [3], [1], [1], [2], [3], [0], [3], [0], [1], [1, 3], [1], [0], [1], [1], [0], [3], [0], [3], [3], [0], [0], [0], [0], [1], [1], [2], [2], [0], [1, 3], [3], [0, 2], [1], [1], [0], [3], [1], [0], [0], [1], [3], [1], [1], [0], [0], [0], [3], [1], [0], [0], [2], [3], [3], [1], [3], [0], [3], [3], [0], [3], [1], [1], [1], [0], [1], [0], [0], [1], [0], [1], [0], [3], [1], [1], [1], [0], [3], [3], [1], [1], [1], [0], [0], [0], [1], [3], [0], [0], [1], [1], [3], [3], [0], [0], [1], [0], [0], [3], [0], [1], [0], [2], [0], [3], [1], [1], [1], [3], [1], [3], [1], [1], [0], [1], [0], [3], [3], [1], [0], [0], [1], [2], [3], [0], [3], [3], [0], [0], [1], [2], [3], [3], [3], [3], [2], [0], [1], [1], [0], [3], [1], [3], [1], [3], [0], [0], [3], [2], [1], [0], [0], [0], [0], [1], [3], [0], [1], [0], [1], [0], [1], [0], [0], [1], [1], [1], [0], [1], [1], [0], [0], [1], [0], [3], [1], [0], [1], [3], [3], [3], [1], [1], [1], [0], [1], [0], [1], [1], [3], [0], [1], [3], [0], [3], [0], [3], [0], [1], [0], [1], [1], [0], [0], [0], [3], [3], [1], [2], [2], [0], [1], [1], [3], [0], [0], [1], [0], [1], [3], [0], [3], [1], [3], [3], [0, 3], [3], [0], [0], [1], [3], [0], [1], [1], [1], [3], [3], [1], [0], [3], [0], [0], [1], [0], [1], [0], [0], [0], [0], [1], [1], [0], [3], [3], [1], [0], [1], [1], [1], [1], [1], [3], [0], [1], [0], [1], [3], [3], [1], [0], [1], [1], [3], [3], [3], [1], [0], [0, 3], [0], [1], [1, 3], [0], [0], [1], [1], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch -1 tr: l 2.294 a 0.114 clct[348, 309, 155, 417] 3.269sec\n",
            "200\n",
            "[[1, 3], [0], [0], [0], [0], [0], [1], [0], [1], [0], [2], [0], [2], [0], [2], [0], [3], [1], [0], [2], [0], [0], [0, 2], [0], [0], [0], [0], [3], [0], [0], [1], [3], [0], [0], [0], [0], [0], [0], [0, 3], [0], [0], [3], [0], [1], [0], [3], [3], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [1], [3], [1], [1], [3], [3], [3], [1], [2], [1], [3], [3], [0], [1], [3], [3], [2], [3], [2], [3], [3], [2], [2], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [1], [3], [3], [3], [3], [3], [3], [1], [1], [2], [1], [1], [1], [1], [1], [2], [2], [1, 3], [3], [2], [2], [1], [2], [1], [1], [1], [1], [0, 2], [1], [1], [1], [2], [1], [1], [1], [1, 3], [1], [2], [1], [2], [1], [2], [1], [3], [1], [1], [1], [1], [1], [1], [1], [1], [2], [1], [1], [1], [2], [0], [3], [3], [1], [0], [1], [1], [0], [3], [3], [1], [0], [1], [1], [0], [0], [3], [1], [1], [0], [1], [1], [0], [1], [3], [3], [1], [0], [0], [1], [0], [1], [3], [2], [1], [0], [3], [0], [1], [3], [1], [1], [0], [1], [3], [3], [3], [1], [0, 3], [3]]\n",
            "Epoch -1 tst: l 2.293 a 0.115 clct[51, 68, 25, 63] 0.512sec\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "list indices must be integers or slices, not list",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[132], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m exp \u001b[38;5;241m=\u001b[39m TrainMNISTCluster(config, device)\n\u001b[0;32m     11\u001b[0m exp\u001b[38;5;241m.\u001b[39msetup()\n\u001b[1;32m---> 12\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m duration \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---train cluster Ended in \u001b[39m\u001b[38;5;132;01m%0.2f\u001b[39;00m\u001b[38;5;124m hour (\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m sec) \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (duration\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m3600\u001b[39m), duration))\n",
            "Cell \u001b[1;32mIn[131], line 230\u001b[0m, in \u001b[0;36mTrainMNISTCluster.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    227\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m lr\n\u001b[0;32m    229\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 230\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster_assign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    232\u001b[0m train_time \u001b[38;5;241m=\u001b[39m t1\u001b[38;5;241m-\u001b[39mt0\n",
            "Cell \u001b[1;32mIn[131], line 322\u001b[0m, in \u001b[0;36mTrainMNISTCluster.train\u001b[1;34m(self, cluster_assign, lr)\u001b[0m\n\u001b[0;32m    319\u001b[0m (X, y) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_data(m_i)\n\u001b[0;32m    321\u001b[0m p_i \u001b[38;5;241m=\u001b[39m cluster_assign[m_i]\n\u001b[1;32m--> 322\u001b[0m model \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp_i\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# LOCAL UPDATE PER MACHINE tau times\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(tau):\n",
            "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "config = get_config()\n",
        "\n",
        "config['train_seed'] = config['data_seed']\n",
        "\n",
        "print(\"config:\",config)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "exp = TrainMNISTCluster(config, device)\n",
        "exp.setup()\n",
        "exp.run()\n",
        "duration = (time.time() - start_time)\n",
        "print(\"---train cluster Ended in %0.2f hour (%.3f sec) \" % (duration/float(3600), duration))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deep_learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
